\documentclass[a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage[italian]{babel}
\usepackage{amsmath, amssymb}
\usepackage[makeroom]{cancel}
\usepackage{amsfonts}
\usepackage{mdframed}
\usepackage{xcolor}
\usepackage{float}
\usepackage{tikz}
\usepackage{tikz-cd}
\usepackage{nicematrix}
\usepackage{pgfplots}
\usetikzlibrary{fit}
\usetikzlibrary{pgfplots.fillbetween}
\pgfplotsset{compat=newest, ticks=none}
\usepackage{graphicx}
\graphicspath{{./figures/}}

\pgfdeclarelayer{ft}
\pgfdeclarelayer{bg}
\pgfsetlayers{bg,main,ft}

\usepackage{import}
\usepackage{pdfpages}
\usepackage{transparent}
\usepackage{xcolor}

\usepackage{hyperref}
\hypersetup{
  colorlinks=false,
}

\usepackage{ntheorem}
\newtheorem{theorem}{Teorema}

% Useful definitions frame
\theoremstyle{break}
\theoremheaderfont{\bfseries}
\newmdtheoremenv[%
linecolor=gray,leftmargin=0,%
rightmargin=0,
innertopmargin=8pt,%
ntheorem]{define}{Definizioni utili}[section]

% Example frame
\theoremstyle{break}
\theoremheaderfont{\bfseries}
\newmdtheoremenv[%
linecolor=gray,leftmargin=0,%
rightmargin=0,
innertopmargin=8pt,%
ntheorem]{example}{Esempio}[section]

% Important definition frame
\theoremstyle{break}
\theoremheaderfont{\bfseries}
\newmdtheoremenv[%
linecolor=gray,leftmargin=0,%
rightmargin=0,
backgroundcolor=gray!40,%
innertopmargin=8pt,%
ntheorem]{definition}{Definizione}[section]

% Exercise frame
\theoremstyle{break}
\theoremheaderfont{\bfseries}
\newmdtheoremenv[%
linecolor=gray,leftmargin=0,%
rightmargin=0,
innertopmargin=8pt,%
ntheorem]{exercise}{Esercizio}[section]


% figure support
\usepackage{import}
\usepackage{xifthen}
\pdfminorversion=7
\usepackage{pdfpages}
\usepackage{transparent}
\newcommand{\incfig}[1]{%
  \def\svgwidth{\columnwidth}
  \import{./figures/}{#1.pdf_tex}
}

\pdfsuppresswarningpagegroup=1

% Matrices
\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
\array{#1}}
\makeatother

\begin{document}
\input{title.tex}

\tableofcontents
\pagebreak

% Info sul corso
% Programma su moodle
% Lezioni sia in presenza che su online
% Ogni terza ora del lunedì si discutono gli esercizi votati nel sondaggio su moodle
% Esercizi da fare a gruppi con un punto extra per il voto finale
% Esame esame molto simile agli esercizi assegnati
% Esame di 2 ore con 4 esercizi

\section{Numeri complessi}
\subsection{Insiemi di numeri}
I numeri sono divisi in insiemi in base alle operazioni che si possono fare con essi:
\begin{itemize}
  \item
    I numeri sono stati pensati per contare e per farlo è stato definito l'insieme dei numeri naturali
    che è definito come \[  \mathbb{N} = \{0, 1, 2, 3, \ldots\} \]
  \item
    Per fare operazioni di sottrazione è stato definito l'insieme dei numeri interi che è
    definito come \[ \mathbb{Z} = \{\ldots, -3, -2, -1, 0, 1, 2, 3, \ldots\} \]
  \item
    Per fare operazioni di divisione è stato definito l'insieme dei numeri razionali che è
    definito come \[ \mathbb{Q} = \left\{ \frac{p}{q} \mid p, q \in \mathbb{Z}, q \neq 0 \right\} \]
  \item
    Per fare operazioni di radice quadrata è stato definito l'insieme dei numeri reali che è
    definito come \[ \mathbb{R} = \left\{ x \mid x \in \mathbb{Q} \right\} \]
  \item
    Infine, per fare operazioni di radice quadrata di numeri negativi è stato definito
    l'insieme dei numeri complessi che è definito come
    \[ \mathbb{C} = \left\{ z \mid z = a + bi,\;\;\; a, b \in \mathbb{R}, i^2 = -1 \right\} \]
\end{itemize}

\noindent Ognuno di questi insiemi è un sottoinsieme dell'insieme successivo, ovvero
\[ \mathbb{N} \subset \mathbb{Z} \subset \mathbb{Q} \subset \mathbb{R} \subset \mathbb{C} \]

\noindent Le equazioni non risolvibili in un insieme vengono risolte in un insieme successivo, ad esempio
\[ x^2 + 1 = 0 \] non ha soluzioni in $\mathbb{R}$, ma ha soluzioni in $\mathbb{C}$.

\begin{figure}[H]
  \begin{theorem}
    \textbf{(Teorema fondamentale dell'algebra)}\\
    Qualsiasi equazione di forma:
    \[
      a_nx^n + a_{n-1}x^{n-1} + \ldots + a_1x + a_0 = 0
    \]
    dove
    \[
      n \in \mathbb{N},\;\; a_0, a_1, \ldots, a_n \in \mathbb{C},\;\; a_n \neq 0
    \]
    ed \( x \) è un incognita, ammette \( n \) soluzioni

  \end{theorem}

  \begin{define}
    \[
      a_nx^n + a_{n-1}x^{n-1} + \ldots + a_1x + a_0 \;\; \text{con} \;\; a_n \neq 0
    \]
    è detto \textbf{polinomio di grado \( n \)} con \textbf{coefficienti} \( a_0, \ldots, a_n \in \mathbb{C} \)
  \end{define}
\end{figure}

\subsection{Numeri immaginari}
Aggiungiamo ai numeri reali un "nuovo" numero $i$ che è definito come $i^2 = -1$. Questo numero
è detto: \textbf{unità immaginaria}. Per agevolare le operazioni con i numeri immaginari si
definisce l'insieme dei \textbf{numeri complessi} in modo da poter moltiplicare e sommare un
numero reale con un numero immaginario:
\[
  \mathbb{C} = \{ a + bi \;|\; a,b \in \mathbb{R} \}
\]

\( z = a + bi \) è detta \textbf{forma algebrica} di un numero complesso \( z \in \mathbb{C} \).
\[
  a = \Re(z) \quad \text{è detta parte reale di } z
\]
\[
  b = \Im(z) \quad \text{è detta parte immaginaria di } z
\]

\begin{figure}[H]
  \begin{define}
    Per agevolare la scrittura, al posto di scrivere:
    \[
      a + (-b)i
    \]
    si scrive:
    \[
      a - bi
    \]
  \end{define}
\end{figure}

\subsubsection{Esempi}
\begin{figure}[H]
  \begin{example}
    \begin{itemize}
      \item $3 + 2i$
      \item $-12 + \frac{1}{2}i$
      \item $3-\sqrt{2}i $
      \item $1+0 \cdot i = 1\;\; \in \mathbb{R}$
    \end{itemize}
  \end{example}
\end{figure}

\subsection{Operazioni tra i numeri complessi}
\subsubsection{Somma}
\begin{figure}[H]
  \begin{definition}
    L'addizione tra due numeri complessi è definita come:
    \[
      z_1 = a + bi \quad z_2 = c + di \quad \in \mathbb{C}
    \]
    \[
      z_1 + z_2 = (a + bi) + (c + di) = (a + c) + (b + d)i
    \]
  \end{definition}
\end{figure}

\begin{figure}[H]
  \begin{example}
    \[
      z_1 = 6 + 7i \quad z_2 = -12 + 1732i
    \]
    \[
      z_1 + z_2 = (6 + 7i) + (-12 + 1732i) = -6 + 1739i
    \]
  \end{example}
\end{figure}

\subsubsection{Prodotto}
\begin{figure}[H]
  \begin{definition}
    Il prodotto tra due numeri complessi è definito come:
    \[
      z_1 = a + bi \quad z_2 = c + di \quad \in \mathbb{C}
    \]
    \[
      z_1 \cdot z_2 = (a + bi) \cdot (c + di) = ac + adi + bci + bdi^2
    \]
    visto che $i^2 = -1$ si ha che $bdi^2 = -bd$ quindi
    \[
      z_1 \cdot z_2 = ac + adi + bci - bd = (ac - bd) + (ad + bc)i
    \]
  \end{definition}

  \begin{example}
    \[
      z_1 = 3 + 2i \quad z_2 = 10 - i
    \]
    \[
      z_1 \cdot z_2 = (3 + 2i) \cdot (10 - i) = 30 - 3i + 20i - 2i^2 = 32 + 17i
    \]
  \end{example}
\end{figure}

\subsubsection{Sottrazione}
Notiamo che per ogni numero complesso \( z = a + bi \; \in \mathbb{C} \), il numero complesso
\( -a -bi \) è l'unico numero complesso tale che \( z + (-z) = 0 \). Questo numero complesso è
detto \textbf{opposto} di \( z \) e si indica con \( -z \).

\begin{figure}[H]
  \begin{definition}
    La sottrazione tra due numeri complessi è definita come:
    \[
      z_1 = a + bi \quad z_2 = c + di \quad \in \mathbb{C}
    \]
    \[
      z_1 - z_2 = z_1 + (-z_2) = (a + bi) - (c + di) = (a - c) + (b - d)i
    \]
  \end{definition}

  \begin{example}
    \[
      z_1 = 3 + 2i \quad z_2 = 10 - i
    \]
    \[
      z_1 - z_2 = (3 + 2i) - (10 - i) = -7 + 3i
    \]
  \end{example}
\end{figure}

\subsubsection{Divisione}
\begin{definition}
  La divisione tra due numeri complessi è definita come:
  \[
    z_1, z_2, z_2 \neq 0 \quad \in \mathbb{C}
  \]
  Definiamo \( \frac{1}{z_2} \) come l'unico numero complesso tale che:
  \[
    z_2 \cdot \frac{1}{z_2} = 1
  \]
  \[
    \frac{z_1}{z_2} = z_1 \cdot \frac{1}{z_2}
  \]
  Sia \( z = a + bi\;\; \in \mathbb{C} \) e \( z \neq 0 \). Supponiamo che \( z'= c + di \) sia
  un numero complesso tale che \( z \cdot z' = 1 \), cioè:
  \[
    1 = z \cdot z' = (a + bi) \cdot (c + di) = (ac - bd) + (ad + bc)i
  \]
  Abbiamo \( ac-bd=1 \) e \( ad+bc=0 \).

  \noindent Possiamo trovare \( c \) sostituendo \( d = \frac{-1-ac}{b} \) nella prima equazione:
  \[
    c = -\frac{ad}{b} \quad d = \frac{-(1-ac)}{b} = \frac{1-ac}{b}
  \]
  \[
    c = \frac{-a (\frac{-1+ac}{b})}{b} = \frac{-a (\frac{-1+ac}{b})}{b} \cdot \frac{b}{b} = \frac{-a (-1 + ac)}{b^2}
  \]
  \[
    cb^2 = a - a^2c
  \]
  \[
    c(a^2 + b^2) = a
  \]
  \[
    c = \frac{a}{a^2 + b^2}
  \]
  \noindent Possiamo trovare \( d \) sostituendo \( c = \frac{-ad}{b} \) nella seconda equazione:
  \[
    d = \frac{-bc}{a} \quad c = \frac{-(1-bd)}{a} = \frac{1-bd}{a}
  \]
  \[
    d = \frac{-b (\frac{1-bd}{a})}{a} = \frac{-b (\frac{1-bd}{a})}{a} \cdot \frac{a}{a} = \frac{-b (1 - bd)}{a^2}
  \]
  \[
    ad^2 = b - b^2d
  \]
  \[
    d(a^2 + b^2) = b
  \]
  \[
    d = \frac{b}{a^2 + b^2}
  \]
  Quindi:
  \[
    z' = \frac{a}{a^2 + b^2} - \frac{b}{a^2 + b^2}i = \frac{a - bi}{a^2 + b^2}
  \]
  di conseguenza \[ \frac{1}{z} = \frac{a-bi}{a^2+b^2} \]

  \noindent Siano \( z_1 = a+bi, z_2 = c+di \neq 0 \in \mathbb{C} \). Definiamo:
  \[
    \frac{z_1}{z_2} = z_1 \cdot \frac{1}{z_2} = z_1 \cdot \frac{c-di}{c^2+d^2} = \frac{ac+bd}{c^2+d^2} + \frac{bc-ad}{c^2+d^2}i
  \]
\end{definition}

\begin{figure}[H]
  \begin{example}
    \[
      \frac{1+2i}{2-i} = \left(1+2i\right)\left(\frac{2}{5}+\frac{1}{5}i\right) = \left( \frac{2}{5}-\frac{2}{5} \right) + \left( \frac{1}{5} + \frac{4}{5} \right) i = i
    \]
  \end{example}
\end{figure}

\noindent Un trucco per dividere i numeri complessi è moltiplicare per \( 1 \) la frazione:
\[
  (a+bi)(a-bi) = a^2 \cancel{+abi} \cancel{- abi} + b^2 = a^2 + b^2 \quad \in \mathbb{R}
\]
In questo modo si arriva ad ottenere un numero reale al denominatore facilitando la divisione.
\begin{figure}[H]
  \begin{example}
    \[
      \frac{1+2i}{2-i}
    \]
    \[
      \left( \frac{1+2i}{2-i} \right) \left( \frac{2+i}{2+i} \right) = \frac{(1+2i)(2+i)}{2^2+(-1)^2} =
    \]
    \[
      = \frac{(1+2i)(2+i)}{5} = \frac{2+4i+i+2i^2}{5} = \frac{2+5i-2}{5} = \frac{5i}{5} = i
    \]
  \end{example}
\end{figure}

\subsection{Coniugato e modulo}
\subsubsection{Coniugato}
Sia \( z = a +bi \in \mathbb{C} \). Il numero complesso \( \overline{z} = a - bi \) è detto
\textbf{coniugato} di \( z \).

\subsubsection{Modulo}
Il \textbf{modulo} di \( z \) è definito come:
\[
  |z| = \sqrt{a^2 + b^2} \quad \in \mathbb{R}
\]

\subsubsection{Proprietà}
Siano \( z_1 = a + bi, z_2 = c + di \quad \in \mathbb{C} \)
\begin{enumerate}
  \item \( z_1 \overline{z_1} = a^2 + b^2 = |z_1|^2 \)
  \item \( \overline{z_1 + z_2} = \overline{(a+c) + (b+d)i} = (a-bi) + (c-di) = \overline{z_1} + \overline{z_2} \)
  \item \( \overline{z_1 z_2} = \overline{z_1} \cdot \overline{z_2} \)
  \item Se \[ z_1 \neq 0, \; \overline{\frac{1}{z_1}} = \frac{1}{\overline{z_1}} \]
    Infatti:
    \[
      \overline{z_1} \cdot \left( \overline{\frac{1}{z_1}} \right) = \left(\overline{ z_1 \cdot \frac{1}{z_1}} \right) = \overline{1+0i} = 1 - 0i = 1
    \]
  \item Se \( z_2 \neq 0 \) allora:
    \[
      \left( \overline{\frac{z_1}{z_2}} \right) = \left( \overline{z_1 \cdot \frac{1}{z_2}} \right) = \overline{z_1} \cdot \overline{\frac{1}{z_2}} = \overline{z_1} \cdot \frac{1}{\overline{z_2}} = \frac{\overline{z_1}}{\overline{z_2}}
    \]
  \item Se \( z_1 \neq 0 \), allora
    \[
      \frac{1}{z_1} \stackrel{def}{=} \frac{a-bi}{a^2+b^2}= \frac{\overline{z_1}}{|z_1|^2}
    \]
\end{enumerate}

\begin{example}
  \[
    z = \frac{1 + i}{2 - i} = \left( 1+i \right) \left( \frac{1}{2-i} \right)
  \]
  \[
    \frac{1}{2-i} = \frac{2+i}{5} = \frac{2+i}{5} = \frac{2}{5} + \frac{1}{5}i
  \]
  \[
    z = \left( 1+i \right) \left( \frac{2}{5} + \frac{1}{5}i \right) = \left( \frac{2}{5}-\frac{1}{5} \right) + \left( \frac{2}{5} + \frac{1}{5} \right)i = \frac{1}{5} + \frac{3}{5}i
  \]
  \[
    \overline{z} = \frac{1}{5} - \frac{3}{5}i
  \]
\end{example}

\subsection{Coordinate polari}
Per ogni numero complesso si ha una coppia di coordinate:
\[
  z = a + bi \quad \in \mathbb{C}
\]
\[ (a,b) = (\Re(z), \Im(z)) \in \mathbb{R}^2 \]


\begin{figure}[H]
  \begin{center}
    \begin{tikzpicture}[scale=0.8, domain=0:13]
      \draw[->] (-0.5,0) -- (6,0) node[right] {$\Re$};
      \draw[->] (0,-0.5) -- (0,5) node[above] {$\Im$};

      \draw[fill, blue] (5, 4) circle (4pt) node[above right] {$z = (a,b)$};

      \draw[blue, thick] (0, 0) -- (5, 4);

      \draw[red, thick] (3, 0) arc (0:38:3) node[right] {$\alpha$};
    \end{tikzpicture}
  \end{center}

  \caption{Rappresentazione di un numero complesso}
\end{figure}
\noindent Possiamo esprimere \( z \) in coordinate polari \( (r, \alpha) \) dove \( r \) è la lunghezza del
segmento \( OZ \), detto \textbf{raggio polare}, ed \( \alpha \) è l'angolo compreso tra l'asse delle x
e \( OZ \) in senso antiorario. \( \alpha \) viene misurato in radianti

\begin{figure}[H]
  \begin{example}
    \[
      z_1 = (1,0) \to 1
    \]
    \[
      z_2 = (1, \frac{\pi}{2}) \to i
    \]
    \[
      z_3 = (1, \pi) \to -1
    \]
    \[
      z_4 = (1, \frac{3\pi}{2}) \to -i
    \]

    \begin{center}
      \begin{tikzpicture}[scale=0.5, domain=0:13]
        \draw[->] (-4,0) -- (4,0) node[right] {$\Re$};
        \draw[->] (0,-4) -- (0,4) node[above] {$\Im$};

        \draw[fill, blue] (3, 0) circle (4pt) node[below right] {$z_1 = (1,0)$};
        \draw[fill, blue] (0, 3) circle (4pt) node[above left] {$z_2 = (1, \frac{\pi}{2})$};
        \draw[fill, blue] (-3, 0) circle (4pt) node[below left] {$z_3 = (1, \pi)$};
        \draw[fill, blue] (0, -3) circle (4pt) node[below left] {$z_4 = (1, \frac{3\pi}{2})$};

      \end{tikzpicture}
    \end{center}
    \caption{Esempi di numeri complessi in coordinate polari}
  \end{example}
\end{figure}

\subsection{Forma trigonometrica di un numero complesso}
Dato un \( z = (r, \alpha) \) in coordinate polari, vogliamo ricavare la forma algebrica. Per
fare ciò usiamo il seno e il coseno:
\[
  \cos(\alpha) = \frac{a}{r} \quad \sin(\alpha) = \frac{b}{r}
\]
\begin{figure}[H]
  \begin{center}
    \begin{tikzpicture}[scale=0.8, domain=0:13]
      \draw[->] (-0.5,0) -- (6,0) node[right] {$\Re$};
      \draw[->] (0,-0.5) -- (0,5) node[above] {$\Im$};

      \draw[fill, blue] (5, 4) circle (4pt);

      \draw[blue, thick] (0, 0) -- (5, 4);

      \draw[red, thick] (3, 0) arc (0:38:3) node[right] {$\alpha$};

      \draw[green, dashed] (5, 0) -- (5, 4) node[midway, right] {$b$};
      \draw[green, dashed] (0, 4) -- (5, 4) node[midway, above] {$a$};
    \end{tikzpicture}
  \end{center}

  \caption{Forma trigonometrica di un numero complesso}
\end{figure}
\begin{figure}[H]
  \begin{definition}
    La \textbf{forma trigonometrica} di un numero complesso è definita come:
    \[
      z = (r \cdot \cos(\alpha)) + (r \cdot \sin(\alpha)i) = r \cdot (\cos(\alpha) + i \cdot \sin(\alpha))
    \]
    \[
      r = |z| = \sqrt{a^2 + b^2}
    \]
    \[
      \alpha = \begin{cases}
        \frac{\pi}{2} \quad \text{se} \; a = 0,\; b > 0                                   \\
        \frac{3\pi}{2} \quad \text{se} \; a = 0,\; b < 0                                  \\
        \text{non definito} \quad \text{se} \; a = 0,\; b = 0                             \\
        \arctan\left(\frac{b}{a}\right) \quad \text{se} \; a > 0,\; b \ge 0\\
        \arctan\left(\frac{b}{a}\right) + 2\pi \quad \text{se} \; a > 0,\; b < 0         \\
        \arctan\left(\frac{b}{a}\right) + \pi \quad \text{se} \; a < 0,\; b \;\text{qualsiasi}
      \end{cases}
    \]
  \end{definition}
\end{figure}

\begin{figure}[H]
  \begin{example}
    \[
      1 = \cos(0) + i \cdot \sin(0)
    \]
    \[
      i = \cos(\frac{\pi}{2}) + i \cdot \sin(\frac{\pi}{2})
    \]
    \[
      -1 = \cos(\pi) + i \cdot \sin(\pi)
    \]
    \[
      -i = \cos(\frac{3\pi}{2}) + i \cdot \sin(\frac{3\pi}{2})
    \]

  \end{example}
\end{figure}

\subsection{Prodotto di numeri complessi in forma trigonometrica}
\begin{figure}[H]
  \begin{definition}
    \[
      z_1 = r\left(\cos(\alpha) + i \sin(\alpha)\right), \quad z_2 = s\left(\cos(\beta) + i \sin(\beta)\right) \quad \in \mathbb{C}
    \]
    \vspace{0.1cm}
    \[
      z_1  z_2 = r  s  (\cos(\alpha) + i \sin(\alpha))  (\cos(\beta) + i \sin(\beta)) =
    \]
    \[
      = r  s  \left( (\cos{\alpha} \cos(\beta) - \sin(\alpha) \sin(\beta)) + (\cos(\alpha) \sin(\beta) + \sin(\alpha) \cos(\beta))i \right) =
    \]
    \[
      = r  s  \left( \cos(\alpha + \beta) + i \sin(\alpha + \beta) \right)
    \]
  \end{definition}
\end{figure}
\subsection{Formula di de Moivre}
Dati \( n \in \mathbb{N}, \quad z = r(\cos(\alpha) + i \sin(\alpha)) \in \mathbb{C} \)
\[
  z^n = r^n \cdot (\cos(n\alpha) + i \sin(n\alpha))
\]

\begin{figure}[H]
  \begin{example}
    \[
      z = \sqrt{3} + i = 2 \cdot \left( \cos(\frac{\pi}{6}) + i \sin(\frac{\pi}{6}) \right)
    \]
    \[
      z^6 = 2^6 \cdot \left( \cos(\frac{\pi}{6} \cdot 6) + i \sin(\frac{\pi}{6} \cdot 6) \right) = 64 \cdot \left( \cos(\pi) + i \sin(\pi) \right) = -64
    \]
  \end{example}
\end{figure}

\subsection{Definizione di radice n-esima}
\[
  y \in  \mathbb{C}, \quad n \in \mathbb{N}
\]
Si dicono \textbf{radici n-esime} di \( y \) le soluzioni dell'equazione \( x^n = y \).

\subsection{Teorema delle radici n-esime}
\begin{figure}[H]
  \begin{theorem}
    Siano \( y \in \mathbb{C} \) e \( n \in \mathbb{N} \). Esistono precisamente \( n \) radici n-esime
    complesse distinte \( z_0, z_1, \ldots, z_{n-1} \) di \( y \). Se \( y = r(\cos(\alpha)+i\sin(\alpha)) \),
    allora per \( k = 0, \ldots, n-1 \) :
    \[
      z_k = \sqrt[n]{r} \left( \cos\left(\frac{\alpha + 2k\pi}{n}\right) + i \sin\left(\frac{\alpha + 2k\pi}{n}\right) \right)
    \]
    Si somma \( 2k \pi  \) per ottenere tutte le radici n-esime, siccome \( \sin \) e \( \cos \) sono periodiche.
    \label{th:radici_n-esime}
  \end{theorem}
\end{figure}

\subsubsection{Dimostrazione}
Per la formula di de Moivre sappiamo che:
\[
  z_k^n = \left( \sqrt[n]{r} \right)^n \left( \cos{\alpha + (2 \pi )k} + i \sin{\alpha + (2 \pi )k} \right)  =
\]
\[
  = r \left( \cos{\alpha} + i \sin{\alpha} \right) = y
\]

\begin{figure}[H]
  \centering
  \begin{tikzpicture}
    \draw[->] (-2,0) -- (2,0) node[right] {$\Re$};
    \draw[->] (0,-2) -- (0,2) node[above] {$\Im$};
    \draw (0,0) circle (1.5cm);

    \draw[red, thick] (0,0) -- (1.05,1.05);
    \draw[blue, thick] (0.5,0) arc (0:45:0.5) node[right] {$\alpha$};

    \draw[green, dashed] (1.05,1.05) -- (1.05,0) node[below] {$\cos{x}$};

    \draw[green, dashed] (1.05,1.05) -- (0,1.05) node[left] {$\sin{x}$};

  \end{tikzpicture}
  \caption{Circonferenza goinometrica}
\end{figure}

\begin{figure}[H]
  \centering
  \begin{tikzpicture}
    % Draw the axes
    \draw[->] (-0.5,0) -- (7,0) node[right] {$\alpha$};
    \draw[->] (0,-2) -- (0,2) node[above] {$\sin{x}$};

    % plot sine
    \draw[domain=0:7, smooth, variable=\x, blue] plot ({\x}, {sin(\x r)});

    % radians labels
    \foreach \x/\xtext in {
      1.5708/\frac{\pi}{2},
      3.14159/\pi,
      4.71239/\frac{3\pi}{2},
      6.28319/2\pi,
    }
    \draw (\x,0.1) -- (\x,-0.1) node[below] {$\xtext$};

  \end{tikzpicture}
  \caption{Funzione seno}
\end{figure}

\begin{figure}[H]
  \centering
  \begin{tikzpicture}
    % Draw the axes
    \draw[->] (-0.5,0) -- (7,0) node[right] {$\alpha$};
    \draw[->] (0,-2) -- (0,2) node[above] {$\cos{x}$};

    % plot cosine
    \draw[domain=0:7, smooth, variable=\x, red] plot ({\x}, {cos(\x r)});

    % radians labels
    \foreach \x/\xtext in {
      1.5708/\frac{\pi}{2},
      3.14159/\pi,
      4.71239/\frac{3\pi}{2},
      6.28319/2\pi,
    }
    \draw (\x,0.1) -- (\x,-0.1) node[below] {$\xtext$};
  \end{tikzpicture}
  \caption{Funzione coseno}
\end{figure}

Quindi \( z_0, \ldots, z_{n-1} \) sono soluzioni di \( y = x^n \) , cioè sono radici
n-esime di \( y \).

Siccome il periodo di \( \sin{} \) e \( \cos{} \) è \( 2 \pi \), le radici n-esime sono
tutte distinte.

\subsection{Radici quadrate di numeri reali negativi}
Sia \( a \in \mathbb{R} \; \subseteq \mathbb{C} \) tale che \( a < 0 \). Esistono
precisamente due radici quadrate di \( a \) in \( \mathbb{C} \). Infatti, abbiamo:

\begin{figure}[H]
  \centering
  \begin{tikzpicture}
    \draw[->] (-2,0) -- (2,0) node[right] {$\Re$};
    \draw[->] (0,-2) -- (0,2) node[above] {$\Im$};

    \draw[fill, blue] (-1, 0) circle (2pt) node[below, align=center, yshift=-2] {$-a$\\$\alpha = \pi $};
    \draw[fill, blue] (1, 0) circle (2pt) node[below, align=center, yshift=-2] {$a$\\$\alpha = 0$};

    \draw[red, thick, ->] (1, 0) arc (0:175:1) node[midway, above right] {$\alpha$};

    \draw[fill, green] (0, 1) circle (2pt) node[above left, align=center] {$\frac{\pi }{2}$};
    \draw[fill, green] (0, -1) circle (2pt) node[below left, align=center] {$\frac{3 \pi }{2}$};
  \end{tikzpicture}
  \caption{Radici quadrate di numeri reali negativi}
\end{figure}
\[
  a = (-a) (\cos{\pi } + i \sin{\pi })
\]
Per il teorema \ref{th:radici_n-esime}:
\[
  z_0 = \sqrt{-a} \left( \cos{\frac{\pi}{2}} + i \sin{\frac{\pi}{2}} \right) = i \sqrt{-a}
\]
\[
  z_1 = \sqrt{-a} \left( \cos{\frac{3\pi}{2}} + i \sin{\frac{3\pi}{2}} \right) = -i \sqrt{-a}
\]

\begin{figure}[H]
  \begin{define}
    Se abbiamo un polinomio della forma:
    \[
      ax^2 + bx + c, \quad a,\;b,\;c \in \mathbb{R}
    \]
    Le soluzioni sono:
    \[
      \frac{-b \pm \sqrt{b^2 -4ac}}{2a}
    \]
    \[
      \Delta = b^2 - 4ac
    \]
    In \( \mathbb{C} \) esistono 2 soluzioni anche se \( \Delta < 0 \).
  \end{define}
\end{figure}

\section{Sistemi lineari e matrici}
\subsection{Sistemi lineari}
Un \textbf{sistema lineare} è un insieme di \( m \) equazioni in \( n \) incognite
che può essere scritto nella forma:
\[
  \begin{cases}
    a_{11}x_1 + a_{12}x_2 + \ldots + a_{1n}x_n = b_1 \\
    a_{21}x_1 + a_{22}x_2 + \ldots + a_{2n}x_n = b_2 \\
    \vdots                                           \\
    a_{m1}x_1 + a_{m2}x_2 + \ldots + a_{mn}x_n = b_m \\
  \end{cases}
\]
dove \( b_k,\; a_{ij} \in \mathbb{C} \) oppure \( \mathbb{R} \) per \( 1 \le i \le m,\;
1 \le j \le n,\; 1 \le k \le m\). Se i \textbf{termini noti} sono tutti nulli il sistema è detto
\textbf{omogeneo}. Una n-upla \( (x_1, \ldots, x_n) \) di numeri complessi (o reali) è
una soluzione se soddisfa tutte le \( m \) equazioni.

\begin{example}
  Presa in considerazione la seguente tabella nutrizionale di cereali (per porzione):
  \begin{center}
    \begin{tabular}{c|c|c}
                      & Cheerios & Quakers \\
                      \hline
      Proteine (g)    & 4        & 3       \\
      Carboidrati (g) & 20       & 18      \\
      Grassi (g)      & 2        & 5       \\
    \end{tabular}
  \end{center}
  Quante porzioni di Cheerios e Quakers dobbiamo mangiare per ottenere \( 9g \) di
  proteine, \( 48g \) di carboidrati e \( 8g \) di grassi?
  \[
    \begin{cases}
      4C + 3Q = 9 \quad \text{(P)}    \\
      20C + 18Q = 48 \quad \text{(C)} \\
      2C + 5Q = 8 \quad \text{(G)}
    \end{cases}
  \]

  Per risolvere il sistema lineare:
  \begin{itemize}
    \item
      Moltiplichiamo le per \( \frac{1}{4} \)
      e otteniamo un sistema lineare \textbf{equivalente} (cioè con
      \textbf{esattamente} le stesse soluzioni):
      \[
        (P') \quad C + \frac{3}{4}Q = \frac{9}{4}
      \]
      \[
        (C) \quad 20C + 18Q = 48
      \]
      \[
        (G) \quad 2C + 5Q = 8
      \]
    \item Calcoliamo \( (C)-20(P') \) e \( (G)-2(P') \) e otteniamo:
      \[
        (P') \quad C + \frac{3}{4}Q = \frac{9}{4}
      \]
      \[
        (C') \quad 0C + 15Q = 18
      \]
      \[
        (G') \quad 0C + \frac{7}{2}Q = \frac{7}{2}
      \]
    \item Moltiplichiamo \( (C') \) per \( \frac{1}{3} \) e otteniamo:
      \[
        (P') \quad C + \frac{3}{4}Q = \frac{9}{4}
      \]
      \[
        (C') \quad 0C + Q = 1
      \]
      \[
        (G') \quad 0C + \frac{7}{2}Q = \frac{7}{2}
      \]
    \item Calcoliamo \( (G') - \frac{7}{2} (C") \) e otteniamo:
      \[
        (P') \quad C + \frac{3}{4}Q = \frac{9}{4}
      \]
      \[
        (C') \quad 0C + Q = 1
      \]
      \[
        (G') \quad 0C + 0Q = 0
      \]
  \end{itemize}

  \noindent Otteniamo dunque che \( Q = 1 \) e \( C = \frac{9}{4} - \frac{3}{4} = \frac{7}{4} \)

  \vspace{0.5cm}

  \noindent Per agevolare la risoluzione del sistema lineare si può utilizzare una matrice:
  \begin{itemize}
    \item \textbf{R1} = Riga 1
    \item \textbf{R2} = Riga 2
    \item \textbf{R3} = Riga 3
  \end{itemize}
  \[
    \begin{pmatrix}[cc|c]
      4  & 3  & 9  \\
      20 & 18 & 48 \\
      2  & 5  & 8
    \end{pmatrix}
  \]
  \[\downarrow \frac{1}{4} \cdot R1 \]
  \[
    \begin{pmatrix}[cc|c]
      1  & \frac{3}{4} & \frac{9}{4} \\
      20 & 18          & 48          \\
      2  & 5           & 8
    \end{pmatrix}
  \]
  \[
    \downarrow R2 - 20 \cdot R1
  \]
  \[
    \downarrow R3 - 2 \cdot R1
  \]
  \[
    \begin{pmatrix} [cc|c]
      1 & \frac{3}{4} & \frac{9}{4} \\
      0 & 3           & 3           \\
      0 & \frac{7}{2} & \frac{7}{2}
    \end{pmatrix}
  \]
  \[
    \downarrow \frac{1}{3} \cdot R2
  \]
  \[
    \begin{pmatrix}[cc|c]
      1 & \frac{3}{4} & \frac{9}{4} \\
      0 & 1           & 1           \\
      0 & \frac{7}{2} & \frac{7}{2}
    \end{pmatrix}
  \]
  \[
    \downarrow R3 - \frac{7}{2} \cdot R2
  \]
  \[
    \begin{pmatrix}[cc|c]
      1 & \frac{3}{4} & \frac{9}{4} \\
      0 & 1           & 1           \\
      0 & 0           & 0
    \end{pmatrix}
  \]
  \noindent Otteniamo dunque che \( Q = 1 \) e \( C = \frac{9}{4} - \frac{3}{4} = \frac{7}{4} \)
\end{example}

\subsection{Definizione}
\begin{figure}[H]
  \begin{definition}
    Siano \( m\;,n,;\ < 1 \). Una tabella \( A \) tale che:
    \[
      A = \begin{pmatrix}
        a_{11} & a_{12} & \ldots & a_{1n} \\
        a_{21} & a_{22} & \ldots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & \ldots & a_{mn} \\
      \end{pmatrix}
      = (a_{ij})_{m \times n}
    \]
    di \( m \times n \) elementi di \( \mathbb{C} \) disposti in \( m \) righe e \( n \) colonne
    si chiama una \textbf{matrice} di \textbf{dimensione} \( m \times n \).
    Gli elementi si chiamano \textbf{coefficienti} (o entrate) della matrice e sono
    contrassegnati con un doppio indice \( ij \) dove \( i \) indica la riga e \( j \)
    la colonna di appartenenza.

    \vspace{0.5cm}
    L'insieme di tutte le matrici di dimensione \( m \times n \) con entrate in \( \mathbb{C} \)
    si indica con \( M_{m \times n}(\mathbb{C}) \).

    \vspace{0.5cm}
    L'insieme di tutte le matrici di dimensione \( m \times n \) con entrate in \( \mathbb{R} \)
    si indica con \( M_{m \times n}(\mathbb{R}) \).
  \end{definition}
\end{figure}

\begin{figure}[H]
  \begin{example}
    \[
      \begin{pmatrix} 3 & i & 2+7i \\
        0 & 1 & \pi
      \end{pmatrix} \in M_{2 \times 3}(\mathbb{C})
      \]
      \[
        \begin{pmatrix}
          0 & 1  \\
          1 & -1
        \end{pmatrix} \in M_{2 \times 2}(\mathbb{R}) \subseteq M_{2 \times 2}(\mathbb{C})
      \]
    \end{example}
  \end{figure}

\subsection{Definizione}
Un sistema lineare di \( n \) incognite e \( m \) equazioni:
\[
a_{11}x_1 + a_{12}x_2 + \ldots + a_{1n}x_n = b_1
\]
\[
\vdots
\]
\[
a_{m1}x_1 + a_{m2}x_2 + \ldots + a_{mn}x_n = b_m
\]
può essere rappresentato nella forma \textbf{matriciale}:
\[
Ax = b
\]
\[
\underbrace{
  A = \begin{pmatrix}
    a_{11} & a_{12} & \ldots & a_{1n} \\
    a_{21} & a_{22} & \ldots & a_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{m1} & a_{m2} & \ldots & a_{mn} \\
    \end{pmatrix}}_{\text{Matrice dei coefficienti}} \quad \underbrace{x = \begin{pmatrix}
    x_1    \\
    x_2    \\
    \vdots \\
    x_n
    \end{pmatrix}}_{\text{Vettore delle incognite}} \quad \underbrace{b = \begin{pmatrix}
    b_1    \\
    b_2    \\
    \vdots \\
    b_m
\end{pmatrix}}_{\text{Vettore dei termini noti}}
\]
La matrice \[ (A \;|\; B) = \begin{pmatrix}[cccc|c]
a_{11} & a_{12} & \ldots & a_{1n} & b_1    \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
a_{m1} & a_{m2} & \ldots & a_{mn} & b_n
\end{pmatrix}  \] è detta \textbf{matrice aumentata}.

\begin{example}
\[
  \begin{cases}
    2x_1 + 6x_2 + 3x_3 + 2x_4 = 4                    \\
    x_1 - 2x_2 + \frac{1}{2}x_3 + \frac{9}{4}x_4 = 1 \\
    -x_1 + x_2 - \frac{1}{2}x_3 - x_4 = \frac{2}{5}
  \end{cases}
\]
Scritto come matrice aumentata diventa:
\[
  \begin{pmatrix}[cccc|c]
    2  & 6  & 3            & 2           & 4           \\
    1  & -2 & \frac{1}{2}  & \frac{9}{4} & 1           \\
    -1 & 1  & -\frac{1}{2} & -1          & \frac{2}{5}
  \end{pmatrix}
\]
\[
  \frac{1}{2}R1
\]
\[
  \begin{pmatrix}[cccc|c]
    1  & 3  & \frac{3}{2}  & 1           & 2           \\
    1  & -2 & \frac{1}{2}  & \frac{9}{4} & 1           \\
    -1 & 1  & -\frac{1}{2} & -1          & \frac{2}{5}
  \end{pmatrix}
\]
\[
  R2 - R1 \quad R3 + R1
\]
\[
  \begin{pmatrix}[cccc|c]
    1 & 3  & \frac{3}{2} & 1           & 2            \\
    0 & -5 & -1          & \frac{5}{4} & -1           \\
    0 & 4  & 1           & 0           & \frac{12}{5}
  \end{pmatrix}
\]
\[
  \frac{-1}{5}R2
\]
\[
  \begin{pmatrix}[cccc|c]
    1 & 3 & \frac{3}{2} & 1            & 2            \\
    0 & 1 & \frac{1}{5} & -\frac{1}{4} & \frac{1}{5}  \\
    0 & 4 & 1           & 0            & \frac{12}{5}
  \end{pmatrix}
\]
\[
  R3 - 4R2
\]
\[
  \begin{pmatrix}[cccc|c]
    1 & 3 & \frac{3}{2} & 1            & 2           \\
    0 & 1 & \frac{1}{5} & -\frac{1}{4} & \frac{1}{5} \\
    0 & 0 & \frac{1}{5} & 1            & \frac{4}{5}
  \end{pmatrix}
\]
\[
  5R3
\]
\[
  \begin{pmatrix}[cccc|c]
    1 & 3 & \frac{3}{2} & 1            & 2           \\
    0 & 1 & \frac{1}{5} & -\frac{1}{4} & \frac{1}{5} \\
    0 & 0 & 1           & 5            & 8
  \end{pmatrix}
\]
\end{example}

\noindent Si ottiene il sistema lineare equivalente:
\[
\begin{cases}
  x_1 + 3x_2 + \frac{3}{2}x_3 + x_4 = 2                                  \\
  \quad \quad \;\;\, x_2 + \frac{1}{5}x_3 - \frac{1}{4}x_4 = \frac{1}{5} \\
  \quad \quad \quad \quad \quad x_3 + 5x_4 = 8
\end{cases}
\]
Assegiamo un parametro alla \textbf{variabile libera} \( x_4 \) :
\[
t = x_4 \quad x_4 = t
\]
\[
x_3 = 8 - 5t
\]
\[
x_2 = \frac{1}{5}- \frac{1}{5}(8-5t) + \frac{1}{4}t = \frac{-7}{5}+ t + \frac{1}{4}t = \frac{-7}{5} + \frac{5}{4}t
\]
\[
x_1 = 2 - 3(\frac{-7}{5} + \frac{5}{4}t) \frac{-3}{2}(8-5t)-t = 2 + \frac{21}{5} - 12 - \frac{15}{4}t - \frac{15}{2}t - t =
\]
\[
\frac{10 + 21 - 60}{5} + \frac{15+30}{4}t - t = \frac{-29}{5} + \frac{15}{4}t - \frac{4}{4}t = \frac{-29}{5} + \frac{11}{4}t
\]
Il sistema ha infinite soluzioni, una per ogni \( t \in \mathbb{C} \).

\subsection{Operazioni elementari}
Attraverso le seguenti operazioni sulla matrice aumenta \( (A | b) \), si ottiene un sistema
equivalente di forma più semplice:
\begin{itemize}
\item Moltiplicare una riga \( (R_i) \) per uno scalare \( \alpha \in \mathbb{C} \) \textbf{non nullo}:
  \[
    \alpha R_i
  \]
\item Sommare una riga \( (R_i) \) con un multiplo di un'altra riga \( (R_j) \):
  \[
    R_i + \alpha R_j
  \]
\item Scambiare riga \( R_i \) con riga \( R_j \):
  \[
    R_i \leftrightarrow R_j
  \]
\end{itemize}

\begin{example}
Prendiamo il seguente sistema lineare:
\[
  \begin{cases}
    2x_1 + 6x_2 + 3x_3 = 4          \\
    x_1 - 2x_2 + \frac{1}{2}x_3 = 1 \\
    -x_1 + x_2 - \frac{7}{10}x_3 = \frac{2}{5}
  \end{cases}
\]
Scritto come matrice aumentata diventa:
\[
  \begin{pmatrix}[ccc|c]
    2  & 6  & 3             & 4           \\
    1  & -2 & \frac{1}{2}   & 1           \\
    -1 & 1  & -\frac{7}{10} & \frac{2}{5}
  \end{pmatrix}
  \stackrel{\frac{1}{2}R1}{\sim}
  \begin{pmatrix}[ccc|c]
    1  & 3  & \frac{3}{2}   & 2           \\
    1  & -2 & \frac{1}{2}   & 1           \\
    -1 & 1  & -\frac{7}{10} & \frac{2}{5}
  \end{pmatrix}
\]
\[
  \stackrel{R2 - R1}{\underset{R3+R1}{\sim}}
  \begin{pmatrix} [ccc|c]
    1 & 3  & \frac{3}{2} & 2            \\
    0 & -5 & -1          & -1           \\
    0 & 4  & \frac{4}{5} & \frac{12}{5}
  \end{pmatrix}
  \stackrel{\frac{-1}{5}R2}{\sim}
  \begin{pmatrix}[ccc|c]
    1 & 3 & \frac{3}{2} & 2            \\
    0 & 1 & \frac{1}{5} & \frac{1}{5}  \\
    0 & 4 & \frac{4}{5} & \frac{12}{5}
  \end{pmatrix}
\]
\[
  \stackrel{R3 - 4R2}{\sim}
  \begin{pmatrix}[ccc|c]
    1 & 3 & \frac{3}{2} & 2           \\
    0 & 1 & \frac{1}{5} & \frac{1}{5} \\
    0 & 0 & 0           & \frac{8}{5}
  \end{pmatrix}
  \stackrel{\frac{5}{8}R3}{\sim}
  \begin{pmatrix}[ccc|c]
    1 & 3 & \frac{3}{2} & 2           \\
    0 & 1 & \frac{1}{5} & \frac{1}{5} \\
    0 & 0 & 0           & 1
  \end{pmatrix}
\]
Otteniamo un sistema lineare equivalente:
\[
  \begin{cases}
    x_1 + 3x_2 + \frac{3}{2}x_3 = 2                       \\
    \quad \quad \;\;\, x_2 + \frac{1}{5}x_3 = \frac{1}{5} \\
    \quad \quad \quad \quad \quad \quad  0 = 1
  \end{cases}
\]
Il sistema è impossibile, non ha soluzioni.
\end{example}

\subsection{Linee in \texorpdfstring{\( \mathbb{R}^2 \)}{R²} }
2 equazioni a 2 incognite con coefficienti in \( \mathbb{R} \):
\[
\begin{cases}
  a_{11}x + a_{12}y = b_1 \quad (I) \\
  a_{21}x + a_{22}y = b_2
\end{cases}
\]
\[
a_{11},\;a_{12},\;a_{21},\;a_{22},\;b_1,\;b_2 \in \mathbb{R}
\]
Questo sistema lineare può essere rappresentato come:
\[
y = \frac{-a_{11}}{a_{12}}x + \frac{b_1}{a_{12}} \quad (I)
\]
\[
y = \frac{-a_{21}}{a_{22}}x + \frac{b_2}{a_{22}} \quad (II)
\]

\noindent Il sistema può essere rappresentato come un sistema di rette nel piano cartesiano in cui
la soluzione è l'intersezione delle rette.

\begin{figure}[H]
\centering
\begin{tikzpicture}
  \draw[->] (-2,0) -- (2,0) node[right] {\( x \)};
  \draw[->] (0,-2) -- (0,2) node[above] {\( y \)};
  \draw[red, thick] (-2,1) -- (1.5,-1.5) node[right] {(I)};
  \draw[blue, thick] (-1.5,-1) -- (2,1) node[right] {(II)};
\end{tikzpicture}
\caption{Intersezione di due rette}
\end{figure}

\noindent Può anche succedere che le rette siano parallele, in questo caso il sistema è impossibile:
\begin{figure}[H]
\centering
\begin{tikzpicture}
  \draw[->] (-2,0) -- (2,0) node[right] {\( x \)};
  \draw[->] (0,-2) -- (0,2) node[above] {\( y \)};
  \draw[red, thick] (-2,1) -- (1.5,-1.5) node[right] {(I)};
  \draw[blue, thick] (-2,2) -- (1.5,-0.5) node[right] {(II)};
\end{tikzpicture}
\caption{Retta parallela}
\end{figure}

\noindent Oppure che le rette siano coincidenti, in questo caso il sistema è indeterminato,
cioè con infinite soluzioni:
\begin{figure}[H]
\centering
\begin{tikzpicture}
  \draw[->] (-2,0) -- (2,0) node[right] {\( x \)};
  \draw[->] (0,-2) -- (0,2) node[above] {\( y \)};
  \draw[red, thick] (-2,1) -- (1.5,-1.5) node[right] {\( (I) = (II) \) };
\end{tikzpicture}
\caption{Retta coincidente}
\end{figure}

\subsection{Metodo di eliminazione di Gauss (EG)}
Data una matrice \( M = (a_{ij}) \quad 1 \le i \le m \;\; 1 \le j \le n \) in \( M_{m \times n}(\mathbb{C}) \)
(oppure in \( M_{m \times n}(\mathbb{R}) \)) con righe \( R1, \ldots, Rn \), eseguiamo le
seguenti opreazioni elementari:
\begin{enumerate}
\item Scegliamo la prima colonna non nulla \( j \) di \( M \) (partendo da sinistra).
  Dopo aver eventualmente scambiato 2 righe di \( M \), otteniamo una matrice
  della forma:
  \[
    \begin{pmatrix}
      0      & \ldots & 0      & a_{1j} & \ldots & a_{1n} \\
      \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
      0      & \ldots & 0      & a_{mj} & \ldots & a_{mn} \\
    \end{pmatrix} \quad \text{con \( a_{ij} \neq 0 \) }
  \]
  Moltiplicando \( R1 \) per \( \frac{1}{a_{ij}} \), si ottiene:
  \[
    \begin{pmatrix}
      0      & \ldots & 0      & 1      & *        & \ldots & *      \\
      \vdots & \ddots & \vdots & \vdots & \vdots   & \ddots & \vdots \\
      0      & \ldots & 0      & a_{mj} & a_{mj+1} & \ldots & a_{mn} \\
    \end{pmatrix}
  \]
  Adesso, per ogni \( 2 \le i \le m \), eseguiamo l'operazione elementare \( Ri - a_{ij}R1 \).
  Otteniamo una matrice della forma:
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}
      \node at (0,0) {
          \(
          \begin{pmatrix}
            0      & \ldots & 0      & 1      & *      & \ldots & *      \\
            \vdots & \ddots & \vdots & 0      & *      & \ldots & *      \\
            \vdots & \ddots & \vdots & \vdots & \vdots &        & \vdots \\
            0      & \ldots & 0      & 0      & *      & \ldots & *      \\
          \end{pmatrix}
          \)
        };

      \draw[red, thick] (0.3,-1.1) rectangle (2,0.4) node[midway, xshift=2] {\( M' \) };
      \draw[<-] (0,-1.1) -- (0,-1.5) node[below] {Colonna \( j \) };
    \end{tikzpicture}
  \end{figure}
\item Ripetiamo il procedimento 1. su \( M' \) per ottenere:
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}
      \node at (0,0) {
          \(
          \begin{pmatrix}
            0      & \ldots & 0      & 1      & *      & \ldots & \ldots & \ldots & \ldots & *      \\
            \vdots & \ddots & \vdots & 0      & \ldots & 0      & 1      & *      & \ldots & *      \\
            \vdots & \ddots & \vdots & \vdots & \ddots & \vdots & 0      & *      & \ldots & *      \\
            \vdots & \ddots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots &        & \vdots \\
            0      & \ldots & 0      & 0      & \ldots & 0      & 0      & 0      & \ldots & 0
          \end{pmatrix}
          \)
        };

      \draw[red, thick] (1.45,-1.45) rectangle (3.25,0.1) node[midway, xshift=3] {\( M'' \) };
    \end{tikzpicture}
  \end{figure}
  e così via...
\item Dopo un numero finito di passi, si ottiene una matrice che si chiama
  \textbf{matrice a scala}:
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}
      \node at (0,0) {
          \(
          r
          \left(
            \begin{matrix}
              0      & \ldots & 0      & 1      & *      & \ldots & *      & *      & *      & \ldots \\
              \vdots & \ddots & \vdots & 0      & 0      & \ldots & 0      & 1      & *      & \ldots \\
              \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots & 0      & 0      & \ldots \\
              \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots \\
              \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots \\
              \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots \\
              0      & \ldots & 0      & 0      & 0      & \ldots & 0      & 0      & 0      & \ldots
            \end{matrix}
            \;\;\;
          \begin{matrix}
            *      & \ldots & \ldots & *      & *      & \ldots & *      \\
            *      & \ldots & \ddots & *      & *      & \ldots & *      \\
            0      & 1      & \ddots & \vdots & \vdots & \ddots & \vdots \\
            \vdots & 0      & \ddots & 1      & *      & \ldots & *      \\
            \vdots & \vdots & \ddots & 0      & 0      & \ldots & 0      \\
            \vdots & \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
            0      & 0      & \ldots & 0      & 0      & \ldots & 0
          \end{matrix}
        \right)
        \)
      };
    \draw[green,thick] (-3.2, 2) -- (-3.2,1.6) -- ++(2.35,0) -- ++(0,-0.65) -- ++(2.35,0)
      -- ++(0,-0.65) -- ++(0.6,0) -- ++(0.8,-0.65) -- ++(2.35,0);

    \draw[green, thick] (-2.95,1.87) circle (0.21);
    \draw[<-] (-2.95,2.15) -- (-2.95,2.5) node[above] {Pivot};
    \draw[green, thick] (-0.6,1.24) circle (0.21);
    \draw[green, thick] (1.78,0.59) circle (0.21);
    \draw[green, thick] (3.18,-0.07) circle (0.21);

    \draw (-3.2,-1.9) -- ++(0,-0.2) -- ++(0.5,0) -- ++(0,0.2) node[below] (dom1) {};
    \draw[green, fill, opacity=0.2] (-3.2,-2.1) rectangle (-2.7,2.1);
    \draw (-0.88,-1.9) -- ++(0,-0.2) -- ++(0.5,0) -- ++(0,0.2) node[below left] (dom2) {};
    \draw[green, fill, opacity=0.2] (-0.88,-2.1) rectangle (-0.38,2.1);
    \draw (1.55,-1.9) -- ++(0,-0.2) -- ++(0.5,0) -- ++(0,0.2) node[below left, yshift=-2] (dom3) {};
    \draw[green, fill, opacity=0.2] (1.55,-2.1) rectangle (2.05,2.1);
    \draw (2.95,-1.9) -- ++(0,-0.2) -- ++(0.5,0) -- ++(0,0.2) node[below left, yshift=-3] (dom4) {};
    \draw[green, fill, opacity=0.2] (2.95,-2.1) rectangle (3.45,2.1);

    \node[align=center] at (0,-3.5) (col) {Colonne\\dominanti};

    \draw[->] (col) -- (dom1);
    \draw[->] (col) -- (dom2);
    \draw[->] (col) -- (dom3);
    \draw[->] (col) -- (dom4);

  \end{tikzpicture}
\end{figure}
cioè esiste un numero \( 1 \le r \le m \) tale che:
\begin{enumerate}
  \item Le righe \( 1 \le i \le r \) non sono nulle.
  \item Ogni riga \( 2 \le i \le m \) ha un numero di zeri iniziali superiore alla
    riga precedente.
  \item le righe \( r+1 \le i \le m \) sono tutte nulle.
\end{enumerate}
Inoltre il primo coefficiente non nullo di ogni riga \( i \) è uguale a \( 1 \) e si chiama \textbf{pivot}.
La matrice è detta \textbf{forma ridotta} di \( M \). Le colonne che contengono pivot
sono dette \textbf{dominanti}.
\end{enumerate}

\begin{example}
Prendiamo in considerazione la matrice:
\[
M = \begin{pmatrix}
  0 & 0  & 0 & 5  & 4 \\
  0 & 10 & 0 & 30 & 2 \\
  0 & -i & 0 & 6  & 7
\end{pmatrix} \in M_{3 \times 5}(\mathbb{C})
\]
\[
\stackrel{R1 \leftrightarrow R2}{\leadsto}
\begin{pmatrix}
  0 & 10 & 0 & 30 & 2 \\
  0 & 0  & 0 & 5  & 4 \\
  0 & -i & 0 & 6  & 7
\end{pmatrix}
\stackrel{\frac{1}{10}R1}{\leadsto}
\begin{pmatrix}
  0 & 1  & 0 & 3 & \frac{1}{5} \\
  0 & 0  & 0 & 5 & 4           \\
  0 & -i & 0 & 6 & 7
\end{pmatrix}
\]
\begin{tikzpicture}
\node at (0,0) {
    \(
    \stackrel{R3 + iR1}{\leadsto}
    \begin{pmatrix}
      0 & 1 & 0 & 3      & \frac{1}{5}      \\
      0 & 0 & 0 & 5      & 4                \\
      0 & 0 & 0 & 6 + 3i & 7 + \frac{1}{5}i
    \end{pmatrix}
    \stackrel{\frac{1}{5}R2}{\leadsto}
    \begin{pmatrix}
      0 & 1 & 0 & 3      & \frac{1}{5}      \\
      0 & 0 & 0 & 1      & \frac{4}{5}      \\
      0 & 0 & 0 & 6 + 3i & 7 + \frac{1}{5}i
    \end{pmatrix}
    \)
  };

\draw[green, fill, opacity=0.2] (-3.5,-0.7) rectangle (-3.1,0.7);
\draw[green, thick] (-3.29,0.45) circle (0.2);

\draw[red, thick] (-3,-0.7) rectangle (-0.07,0.25);

\draw[green, fill, opacity=0.2] (1.65,-0.7) rectangle (2.05,0.7);
\draw[green, thick] (1.85,0.45) circle (0.2);

\draw[red, thick] (2.2,-0.7) rectangle (5.05,0.2);

\end{tikzpicture}
\\
\begin{tikzpicture}
\node at (0,0) {
    \(
    \stackrel{R3 - (6 + 3i)R2}{\leadsto}
    \begin{pmatrix}
      0 & 1 & 0 & 3 & \frac{1}{5}                  \\
      0 & 0 & 0 & 1 & \frac{4}{5}                  \\
      0 & 0 & 0 & 0 & \frac{11}{5} - \frac{11}{5}i
    \end{pmatrix}
    \stackrel{\frac{R3}{\frac{11}{5}-\frac{11}{5}i}}{\leadsto}
    \begin{pmatrix}
      0 & 1 & 0 & 3 & \frac{1}{5} \\
      0 & 0 & 0 & 1 & \frac{4}{5} \\
      0 & 0 & 0 & 0 & 1
    \end{pmatrix}
    \)
  };

\draw[green, fill, opacity=0.2] (-1.4,-0.7) rectangle (-1.0,0.7);
\draw[green, thick] (-1.19,0.45) circle (0.2);

\draw[green, fill, opacity=0.2] (-2.45,-0.7) rectangle (-2.05,0.7);
\draw[green, thick] (-2.25,0.45) circle (0.2);

\draw[red, thick] (-0.8,-0.7) rectangle (0.6,-0.19);

\draw[green, fill, opacity=0.2] (2.75,-0.7) rectangle (3.2,0.7);
\draw[green, thick] (2.97,0.45) circle (0.2);

\draw[green, fill, opacity=0.2] (4.35,-0.7) rectangle (4.78,0.7);
\draw[green, thick] (4.57,0.43) circle (0.24);

\draw[green, fill, opacity=0.2] (3.8,-0.7) rectangle (4.24,0.7);
\draw[green, thick] (4.02,0.45) circle (0.2);

\draw[green] (2.65,0.7) -- ++(0,-0.5) -- ++(1.08,0) -- ++(0,-0.35) -- ++(0.6,0) -- ++(0,-0.45) -- ++(0.4,0);
\end{tikzpicture}
\end{example}

\subsection{Risoluzione di un sistema lineare}
Dato un sistema lineare
\[
(*)\quad Ax=b
\]
con \( A \in M_{m \times n}(\mathbb{C}),\; b \in M_{m \times 1}(\mathbb{C}) \) procediamo con
il metodo di eliminazione di Gauss sulla matrice aumentata \( (A | b) \) fino ad ottenere la
forma ridotta \( (U | c) \) e un sistema lineare corrispondente
\[
Ux = c
\]
che è equivalente a \( (*) \). Chiamiamo \textbf{variabili dominanti} le \( r \) variabili
che corrispondono alle colonne dominanti e \textbf{variabili libere} le rimanenti.

\begin{example}
Prendiamo in considerazione il seguente sistema lineare:
\[
\begin{cases}
  10x_1 + 10x_2 + 30x_3 = 2 \\
  \hspace{1.5cm} 5x_3 = 4   \\
  -x_1 - x_2 + 6x_3 = 7
\end{cases}
\]
Scritto come matrice aumentata diventa:
\[
\begin{pmatrix}[ccc|c]
  10 & 10 & 30 & 2 \\
  0  & 0  & 5  & 4 \\
  -1 & -1 & 6  & 7
\end{pmatrix}
\stackrel{EG}{\leadsto}
\begin{pmatrix}[ccc|c]
  1 & 1 & 3 & \frac{1}{5} \\
  0 & 0 & 1 & \frac{4}{5} \\
  0 & 0 & 0 & 0
\end{pmatrix}
\]
\[
\hspace{3.3cm} x_1 \;\, x_2 \;\, x_3
\]
\( x_1 \) e \( x_3 \) sono variabili dominanti e \( x_2 \) è variabile libera.
\end{example}

\noindent Si ha uno dei seguenti casi:
\begin{enumerate}
\item[1)] Tutte le colonne di \( (U|c) \) tranne \( c \) sono dominanti. In questo caso il sistema
ha una soluzione unica. Ad esempio:
\begin{figure}[H]
  \centering
  \begin{tikzpicture}
    \node at (0,0) {
        \(
        \begin{pmatrix}[cc|c]
          1 & \frac{3}{4} & \frac{9}{4} \\
          0 & 1           & 1           \\
          0 & 0           & 0
        \end{pmatrix}
        \)
      };

    \draw[green, fill, opacity=0.2] (-0.76,-0.7) rectangle (-0.37,0.7);
    \draw[green, thick] (-0.57,0.45) circle (0.2);

    \draw[green, fill, opacity=0.2] (-0.22,-0.7) rectangle (0.17,0.7);
    \draw[green, thick] (-0.02,0.02) circle (0.2);
  \end{tikzpicture}
\end{figure}
\item[\( \infty \))] L'ultima colonna e almeno una colonna di \( U \) \textbf{non} sono dominanti. In
tal caso il sistema ha infinite soluzioni che si ottengono assegnando parametri alle
\( n-r \) variabili libere. Ad esempio:
\begin{figure}[H]
  \centering
  \begin{tikzpicture}
    \node at (0,0) {
        \(
        \begin{pmatrix}[cccc|c]
          1 & 3 & \frac{3}{2} & 1            & 2           \\
          0 & 1 & \frac{1}{5} & -\frac{1}{4} & \frac{1}{5} \\
          0 & 0 & 1           & 5            & 8
        \end{pmatrix}
        \)
      };

    \draw[green, fill, opacity=0.2] (-1.45,-0.7) rectangle (-1.05,0.7);
    \draw[green, thick] (-1.25,0.45) circle (0.2);

    \draw[green, fill, opacity=0.2] (-0.94,-0.7) rectangle (-0.54,0.7);
    \draw[green, thick] (-0.74,0) circle (0.2);

    \draw[green, fill, opacity=0.2] (-0.4,-0.7) rectangle (0,0.7);
    \draw[green, thick] (-0.2,-0.42) circle (0.2);
  \end{tikzpicture}
\end{figure}
\item[0)] L'ultima colonna \( c \) è dominante. In questo tal caso il sistema non ammette
soluzioni. Ad esempio:
\begin{figure}[H]
  \centering
  \begin{tikzpicture}
    \node at (0,0) {
        \(
        \begin{pmatrix}[ccc|c]
          1 & 3 & \frac{3}{2} & 2           \\
          0 & 1 & \frac{1}{5} & \frac{1}{5} \\
          0 & 0 & 0           & 1
        \end{pmatrix}
        \)
      };

    % green highlight for first column
    \draw[green, fill, opacity=0.2] (-1.05,-0.7) rectangle (-0.64,0.7);
    \draw[green, thick] (-0.85,0.45) circle (0.2);

    % green highlight for second column
    \draw[green, fill, opacity=0.2] (-0.53,-0.7) rectangle (-0.12,0.7);
    \draw[green, thick] (-0.33,0) circle (0.2);

    % green highlight for last column
    \draw[green, fill, opacity=0.2] (0.6,-0.7) rectangle (1,0.7);
    \draw[green, thick] (0.8,-0.42) circle (0.2);
  \end{tikzpicture}
\end{figure}
\end{enumerate}

\noindent \textbf{Attenzione:} la forma ridotta di una matrice \textbf{non} è unicovamente
determinata, ma le colonne dominanti sono univocamente determinate.

\subsection{Definizione di rango di una matrice}
\begin{definition}
Sia \( A \in M_{m \times n}(\mathbb{C}) \) con forma ridotta \( U \). Il numero \( r \) di righe non
nulle, pari al numero di colonne dominanti, è detto \textbf{rango} di \( U \) e si indica
con \( rk(U) \).
\end{definition}
Verrà dimostrato più avanti che ogni forma ridotta di \( A \) ha lo stesso rango, quindi
definiamo il rango di \( A \) come \( rk(A) = rk(U) \).

\noindent Si ha \( rk(A) \le min(m,n) \).

\subsection{Osservazione}
Possiamo ricavare le condizioni \( [1],\;[\infty],\;[0] \) usando il rango:

\begin{figure}[H]
  \begin{theorem}[Teprema di Rouchè-Capelli]
    Sia \( A \in M_{m \times n}(\mathbb{C}) \), sia \( b \in M_{m \times 1}(\mathbb(C)) \).
    \[
      [1] \Leftrightarrow rk(A) = rk(A|b) = n
    \]
    \[
      \hspace{0.3cm} "rk(U) = rk(U|c)"
    \]
    \vspace{0.05cm}
    \[
      [\infty] \Leftrightarrow rk(A) = rk(A|b) < n
    \]
    \[
      \hspace{1.1cm} "rk(U) = rk(U|c) < n"
    \]
    \vspace{0.05cm}
    \[
      [0] \Leftrightarrow rk(A) < rk(A|b)
    \]
    \[
      \hspace{1cm} "rk(U) < rk(U|c)"
    \]
  \end{theorem}
\end{figure}

\section{Matrici e le loro operazioni}
\subsection{Definizione di somma}
\begin{figure}[H]
\begin{definition}
Siano \( A=(a_{ij}) \quad 1 \le i \le m \; , \; 1 \le j \le n \) e 
\( B=(b_{ij}) \quad 1 \le i \le m \; , \; 1 \le j \le n \) due matrici in \( M_{m \times n}(\mathbb{C}) \).
La \textbf{somma} di \( A \) e \( B \) è la matrice \[
  A + B (a_{ij} + b_{ij}) \quad 1 \le i \le m\;,\;1 \le j \le n  = 
\] 
\[
  = \begin{pmatrix} 
    a_{11} + b_{11} & \ldots & a_{1n} + b_{1n} \\
    \vdots          & \ddots & \vdots          \\
    a_{m1} + b_{m1} & \ldots & a_{mn} + b_{mn}
  \end{pmatrix} 
\] 
in \( M_{m \times n}(\mathbb{C}) \) 
\end{definition}
\end{figure}
\begin{figure}[H]
\begin{example}
\[
  \begin{pmatrix} 
    1 & 0 & i \\
    -3 & 1 & 4
  \end{pmatrix} 
  +
  \begin{pmatrix} 
    2 & 4 & 1 \\
    2 & -i & 1+i 
  \end{pmatrix} 
  =
  \begin{pmatrix} 
    3 & 4 & 1+i \\
    -1 & 1-i & 5+i
  \end{pmatrix}
\]
\end{example}
\end{figure}

\subsubsection{Proprietà}
L'addizione di matrici è:
\begin{itemize}
\item \textbf{Associativa}, cioè:
\[
  A + (B + C) = (A + B) + C
\] 
\item \textbf{Commutativa}, cioè:
\[
  A + B = B + A
\]
\end{itemize}

\subsection{Definizione di prodotto per uno scalare}
\begin{figure}[H]
\begin{definition}
Data una matrice \( A = (a_{ij})_{1 \le i \le m\;,\;1 \le j \le n} \in M_{m \times n}(\mathbb{C}) \) e
\( \alpha \in \mathbb{C} \), il \textbf{prodotto} della matrice \( A \) per lo scalare
\( \alpha \) è la matrice:
\[
  \alpha A = (\alpha a_{ij})_{1 \le i \le m\;,\;1 \le j \le n} \in M_{m \times n}(\mathbb{C})
\] 
\end{definition}
\end{figure}
\begin{figure}[H]
\begin{example}
\[
  \frac{1}{2}\begin{pmatrix} 
    2+i & 5\\
    i & 1-2i
  \end{pmatrix} 
  =
  \begin{pmatrix} 
    1+\frac{1}{2}i & \frac{5}{2} \\
    \frac{1}{2}i & \frac{1}{2}-i
  \end{pmatrix}
\] 
\end{example}
\end{figure}

\subsubsection{Proprietà}
Il prodotto di una matrice per uno scalare gode delle seguenti proprietà:
\begin{itemize}
\item \textbf{Distributiva rispetto all'addizione}, cioè:
\[
  \alpha(A+B) = \alpha A + \alpha B
\] 
\[
  (\alpha + \beta)A = \alpha A + \beta A
\] 
per \( A,b \in M_{m \times n}(\mathbb{C})\;,\; \alpha,\beta \in \mathbb{C} \) 
\end{itemize}

\subsection{Definizione di matrice trasposta}
\begin{figure}[H]
\begin{definition}
Accanto a una matrice \( A = (a_{ij}) \in M_{m \times n}(\mathbb{C}) \), consideriamo
la matrice \( A^T \) ottenuta da \( A \) scambiando le righe con le colonne,
è detta \textbf{trasposta} di \( A \).
\end{definition}
\end{figure}
\begin{figure}[H]
\begin{example}
\[
  A = \begin{pmatrix} 
    1 & i & 7\\
    \pi & \frac{1}{12} & 0
  \end{pmatrix} 
  \quad
  A^T = \begin{pmatrix} 
    1 & \pi \\
    i & \frac{1}{12} \\
    7 & 0
  \end{pmatrix}
\] 
\end{example}
\end{figure}

\subsection{Definizione di prodotto di matrici}
\begin{itemize}
\item Una matrice di dimensione \( m \times 1 \) è detta \textbf{vettore} (colonna) e
si usa la notazione \( v = \begin{pmatrix} v_1 \\ \vdots \\ v_m \end{pmatrix} \in M_{m \times 1}(\mathbb{C})  \).

Una matrice di dimensione \( 1 \times n \) è detta \textbf{vettore riga} e si usa la notazione
\( v^T = \begin{pmatrix} v_1 & \ldots & v_n \end{pmatrix} \in M_{1 \times n}(\mathbb{C}) \).

Sia \( v^T = \begin{pmatrix} v_1 & \ldots & v_n \end{pmatrix} \)  un vettore riga
in \( M_{1 \times n}(\mathbb{C}) \) e \( u = \begin{pmatrix} v_1 \\ \vdots \\ v_n \end{pmatrix} \)
un vettore colonna in \( M_{n \times 1}(\mathbb{C}) \). Il \textbf{prodotto} di \( v^T \) per \( u \) è
il numero complesso: \( v^Tu = v_1u_1 + v_2u_2 + \ldots + v_nu_n \in \mathbb{C} \) 

\begin{example}
  \[
    v^T=\begin{pmatrix} 1 & 2 & 3 \end{pmatrix} \quad u=\begin{pmatrix} 1 \\ 0 \\ 3 \end{pmatrix}
  \] 
  \[
    v^Tu = 1 \cdot 1 + 2 \cdot 0 + 3 \cdot 3 = 1 + 0 + 9 = 10
  \] 
\end{example}
\item Possiamo vedere una matrice \( A=(a_{ij})_{1 \le i \le m\;,\;1 \le j \le n} \)  come \( m \)
vettori riga \( Ri=(a_{i1} \ldots a_{in})_{1 \le i \le m} \) detti \textbf{righe di \( A \)}
oppure \( n \) vettori colonna \( Cj=\begin{pmatrix} a_{1j} \\ \vdots \\ a_{mj} \end{pmatrix} _{1 \le j \le n} \) detti \textbf{colonne di \( A \)}.

Siano \[ A = (a_{ij})_{1 \le i \le m\;,\;1 \le j \le n} \in M_{m \times n}(\mathbb{C}) \]
\[ B = (b_{ij})_{1 \le i \le s\;,\;1 \le j \le t} \in M_{n \times t}(\mathbb{C}) \]
Se \( n=s \), allora possiamo formare il prodotto di \( A \) e \( B \):
\[
  AB = (c_{ij})_{1 \le i \le m\;,\;1 \le j \le t}
\] 
dove
\[
  c_{ij} = RiCj = \begin{pmatrix} a_{i1} & \ldots & a_{in} \end{pmatrix} 
  \begin{pmatrix} b_{1j} \\ \vdots \\ b_{nj} \end{pmatrix} = a_{i1}b_{1j} + \ldots + a_{in}b_{nj}
\] 
è il prodotto della riga \( i \) di \( A \) e la colonna \( j \) di \( B \).

\begin{example}
  \[
    \begin{pmatrix} 1 & 2 & 3\\
    0 & 1 & 4\end{pmatrix} 
      \begin{pmatrix} 1 & 4 & 0\\
        0 & 1 & 5\\
      1 & 2 & 4\end{pmatrix} 
      =
        \begin{pmatrix} R1C1 & R1C2 & R1C3\\
        R2C1 & R2C2 & R2C3\end{pmatrix} =
        \] 
        \[
          =
          \begin{pmatrix} 
            4 & 12 & 22\\
            4 & 9 & 21
          \end{pmatrix}
        \] 
      \end{example}
  \end{itemize}

\subsubsection{Proprietà}
Il prodotto di matrici gode delle seguenti proprietà:
\begin{itemize}
  \item \textbf{Associativa}, cioè:
    \[
      A(BC) = (AB)C
    \] 
  \item \textbf{Distributiva rispetto all'addizione}, cioè:
    \[
      (A+B)C = AC + BC
    \] 
    Con \( A,B \in M_{m \times n}(\mathbb{C}) \) e \( C \in M_{n \times t}(\mathbb{C}) \)
    \[
      A(B+C) = AB + AC
    \] 
    In sostanza le matrici devono avere il numero di colonne uguale al numero di righe.
  \item Scriviamo \( I_n \in M_{m \times n}(\mathbb{C}) \) per la matrice:
    \[
      \begin{pmatrix} 
        1 & 0 & \ldots & 0\\
        0 & 1 & \ldots & 0\\
        \vdots & \vdots & \ddots & \vdots\\
        0 & 0 & \ldots & 1
      \end{pmatrix}
    \] 
    Questa matrice viene detta \textbf{matrice identità}.

    Per ogni matrice \( M \in M_{m \times n}(\mathbb{C}) \), abbiamo che:
    \[ M \cdot I_m = I_m \cdot M =  M \]
    \begin{figure}[H]
      \begin{example}
        \[
          I_3 = \begin{pmatrix} 
            1 & 0 & 0\\
            0 & 1 & 0\\
            0 & 0 & 1
          \end{pmatrix}
          \quad
          I_2 = \begin{pmatrix} 
            1 & 0\\
            0 & 1
          \end{pmatrix}
        \] 
      \end{example}
    \end{figure}
    \begin{figure}[H]
      \begin{example}
        \[
          M = \begin{pmatrix} 
            1 & 2\\
            3 & 4
          \end{pmatrix}
          \quad
          M \cdot I_2 = \begin{pmatrix} 
            1 & 2\\
            3 & 4
          \end{pmatrix}
          \begin{pmatrix} 
            1 & 0 \\
            0 & 1
          \end{pmatrix} 
          =
          \begin{pmatrix} 
            1 & 2\\
            3 & 4
          \end{pmatrix}
          = M
        \] 
      \end{example}
    \end{figure}
  \item \( (AB)^T = B^T A^T \) con \[ A \in M_{m \times n}(\mathbb{C}) \quad
    B \in M_{n \times t}(\mathbb{C}) \]
    \[
      A^T \in M_{n \times m}(\mathbb{C}) \quad
      B^T \in M_{t \times n}(\mathbb{C})
    \] 
    \begin{figure}[H]
      \begin{example}
        \[
          A = \begin{pmatrix}
            1 & 0\\
            2 & 1
          \end{pmatrix} 
          \quad
          B = \begin{pmatrix} 
            1 & 2 & 4\\
            0 & 1 & 5
          \end{pmatrix} 
        \] 
        \[
          AB = \begin{pmatrix} 
            1 & 2 & 4\\
            2 & 5 & 13
          \end{pmatrix} 
        \] 
        \[
          A^T = \begin{pmatrix}
            1 & 2\\
            0 & 1
          \end{pmatrix} 
          \quad
          B^T = \begin{pmatrix} 
            1 & 0\\
            2 & 1\\
            4 & 5
          \end{pmatrix} 
        \] 
        \[
          (AB)^T = \begin{pmatrix} 
            1 & 2\\
            2 & 5\\
            4 & 13
          \end{pmatrix} 
        \] 
        \[
          B^T A^T = \begin{pmatrix} 
            1 & 0\\
            2 & 1\\
            4 & 5 
          \end{pmatrix}
          \begin{pmatrix} 
            1 & 2\\
            0 & 1
          \end{pmatrix}
          =
          \begin{pmatrix} 
            1 & 2\\
            2 & 5\\
            4 & 13
          \end{pmatrix}
        \] 
      \end{example}
    \end{figure}

  \item Il prodotto di matrici \textbf{non} è commutativo:
    \[
      AB \neq BA
    \] 
    Infatti:
    \[
      AB = \begin{pmatrix} 
        0 & 1\\
        0 & 0
      \end{pmatrix}
      \begin{pmatrix} 
        0 & 2\\
        0 & 4
      \end{pmatrix} 
      =
      \begin{pmatrix} 
        0 & 4\\
        0 & 0
      \end{pmatrix} 
    \] 
    \[
      BA = \begin{pmatrix} 
        0 & 2\\
        0 & 4
      \end{pmatrix} 
      \begin{pmatrix} 
        0 & 1\\
        0 & 0
      \end{pmatrix} 
      =
      \begin{pmatrix} 
        0 & 0\\
        0 & 0
      \end{pmatrix} 
    \] 
\end{itemize}

\subsection{Osservazione}
Siano \( A = (a_{ij})_{1 \le i \le m\;,\;1 \le j \le n} \in M_{m \times n}(\mathbb{C}) \) e
\( b = \begin{pmatrix} b_1 \\ \vdots \\ b_m \end{pmatrix} \in M_{m \times i}(\mathbb{C}) \),
\( x = \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix}  \).
Consideriamo \( Ax=b \) in forma matriciale. Abbiamo
\[
  Ax = \underbrace{\begin{pmatrix} 
      a_{11} & \ldots & a_{1n}\\
      \vdots & \ddots & \vdots\\
      a_{m1} & \ldots & a_{mn}
  \end{pmatrix}}_{\in M_{m \times n}(\mathbb{C})}
  \underbrace{\begin{pmatrix} 
      x_1\\
      \vdots\\
      x_n
  \end{pmatrix}}_{\in M_{n \times 1}(\mathbb{C})}
  =
  \underbrace{\begin{pmatrix} 
      a_{11}x_1 + \ldots + a_{1n}x_n\\
      \vdots\\
      a_{m1}x_1 + \ldots + a_{mn}x_n
  \end{pmatrix}}_{\in M_{m \times 1}(\mathbb{C})}
\]
che è uguale a \( b = \begin{pmatrix} b_1 \\ \vdots \\ b_m \end{pmatrix}  \) 
\[
  \begin{pmatrix} 
    b_1\\
    \vdots\\
    b_m
  \end{pmatrix} 
  =
  \begin{pmatrix} 
    a_{11}x_1 + \ldots + a_{1n}x_n\\
    \vdots\\
    a_{m1}x_1 + \ldots + a_{mn}x_n
  \end{pmatrix} 
\] 
\[
  \leadsto \begin{cases}
    a_{11}x_1 + \ldots + a_{1n}x_n = b_1\\
    \vdots\\
    a_{m1}x_1 + \ldots + a_{mn}x_n = b_m
  \end{cases}
\] 
\begin{figure}[H]
  \begin{example}
    \[
      \begin{cases}
        2x_1 + 6x_2 = 4\\
        x_1 - 2x_2 = 1\\
        -x_1 + x_2 = \frac{2}{5}
      \end{cases}
    \] 
    \[
      A = 
      \begin{pmatrix} 
        2 & 6 \\
        1 & -2 \\
        -1 & 1
      \end{pmatrix} 
      \quad
      x = \begin{pmatrix} 
        x_1\\
        x_2
      \end{pmatrix} 
      \quad
      b = \begin{pmatrix} 
        4\\
        1\\
        \frac{2}{5}
      \end{pmatrix} 
    \] 
    \[
      Ax = \begin{pmatrix} 
        2 & 6\\
        1 & -2\\
        -1 & 1
      \end{pmatrix} 
      \begin{pmatrix} 
        x_1\\
        x_2
      \end{pmatrix} 
      =
      \begin{pmatrix} 
        2x_1 + 6x_2\\
        x_1 - 2x_2\\
        -x_1 + x_2
      \end{pmatrix}
      =
      \begin{pmatrix} 
        4\\
        1\\
        \frac{2}{5}
      \end{pmatrix} 
    \] 
  \end{example}
\end{figure}

\subsection{Definizione}
Una matrice \( A = (a_{ij})_{1 \le i,j \le n} \in M_{n \times n}(\mathbb{C}) \) di dimensione
\( n \times n \) si dice \textbf{matrice quadrata} di ordine \( n \).
Gli elementi di \( A \): \( a_{ii}\quad 1 \le i \le n \) formano la \textbf{diagonale} di \( A \).
\begin{figure}[H]
  \begin{example}
    \[
      \begin{pNiceArray}{>{\strut}ccc}[margin,extra-margin = 1pt]
        0 & -10 & i\\
        7 & 8 & 0\\
        100 & \frac{1}{2} & -i
        \CodeAfter
        \begin{tikzpicture}
          \node [draw=green, fill=green, opacity=0.3, rounded corners=2pt, inner ysep = 0pt,
          rotate fit=-22, fit = (1-1) (3-3) ] {} ;
        \end{tikzpicture}
      \end{pNiceArray}
    \] 
  \end{example}
\end{figure}
\noindent Se tutti gli elementi fuorri dalla diagonale sono nulli, la matrice è detta
\textbf{matrice diagonale}.
\begin{figure}[H]
  \begin{example}
    \[
      \begin{pNiceArray}{>{\strut}ccc}[margin,extra-margin = 1pt]
        0 & 0 & 0\\
        0 & 8 & 0\\
        0 & 0 & -i
        \CodeAfter
        \begin{tikzpicture}
          \node [draw=green, fill=green, opacity=0.3, rounded corners=2pt, inner ysep = 0pt,
          rotate fit=-33, fit = (1-1) (3-3) ] {} ;
        \end{tikzpicture}
      \end{pNiceArray}
    \] 
  \end{example}
\end{figure}

\noindent Se tutti i coefficienti al di sotto della diagonale sono nulli, allora la
matrice è detta \textbf{matrice triangolare superiore}.
\begin{figure}[H]
  \begin{example}
    \[
      \begin{pmatrix} 
        0 & -10 & i\\
        0 & 8 & 0\\
        0 & 0 & -i 
      \end{pmatrix} 
    \] 
    \[
      \begin{pNiceArray}{>{\strut}ccc}[margin,extra-margin = 1pt]
        0 & -10 & i\\
        0 & 8 & 0\\
        0 & 0 & -i
        \CodeAfter
        \begin{tikzpicture}
          \node [draw=green, fill=green, opacity=0.3, rounded corners=2pt, inner ysep = 0pt,
          rotate fit=-28, fit = (2-1) (3-2) ] {} ;
          \node [draw=green, fill=green, opacity=0.3, rounded corners=2pt, inner ysep = 0pt,
          rotate fit=-28, fit = (3-1) ] {} ;
        \end{tikzpicture}
      \end{pNiceArray}
    \] 
  \end{example}
\end{figure}
\noindent Se tutti i coefficienti al di sopra della diagonale sono nulli, allora la matrice è detta
\textbf{matrice triangolare inferiore}.
\begin{figure}[H]
  \begin{example}
    \[
      \begin{pmatrix} 
        0 & 0 & 0\\
        7 & 8 & 0\\
        100 & \frac{1}{2} &-i 
      \end{pmatrix} 
    \] 
    \[
      \begin{pNiceArray}{>{\strut}ccc}[margin,extra-margin = 1pt]
        0 & 0 & 0\\
        7 & 8 & 0\\
        100 & \frac{1}{2} & -i
        \CodeAfter
        \begin{tikzpicture}
          \node [draw=green, fill=green, opacity=0.3, rounded corners=2pt, inner ysep = 2pt,
          rotate fit=-33, fit = (1-2) (2-3) ] {} ;
          \node [draw=green, fill=green, opacity=0.3, rounded corners=2pt, inner ysep = 2pt,
          rotate fit=-33, fit = (1-3) ] {} ;
        \end{tikzpicture}
      \end{pNiceArray}
    \]
  \end{example}
\end{figure}

\subsection{Matrici elementari}
Prendiamo la matrice identità:
\[
  I_n = \begin{pmatrix} 
    1 & 0 & \ldots & 0\\
    0 & 1 & \ldots & 0\\
    \vdots & \vdots & \ddots & \vdots\\
    0 & 0 & \ldots & 1
  \end{pmatrix}
\] 
Applichiamo le operazioni elementari alla matrice identità \( I_n \) per ottenere le
matrici elementari che denotiamo come segue:
\begin{itemize}
  \item 
    \(E_{ij}\) la matrice ottenuta da \( I_n \) scambiando la riga \( i \)
    con la riga \( j \) 
    \begin{example}
      \[
        n = 3 \quad I_3 = \begin{pmatrix} 
          1 & 0 & 0\\
          0 & 1 & 0\\
          0 & 0 & 1
        \end{pmatrix}
      \] 
      \[
        E_{12} = \begin{pmatrix} 
          0 & 1 & 0\\
          1 & 0 & 0\\
          0 & 0 & 1
        \end{pmatrix}
      \] 
    \end{example}
  \item \( E_i(\alpha) \) ottenuta da \( I_n \) moltiplicando la riga \( i \) per lo 
    scalare \( 0 \neq \alpha \in \mathbb{C} \) 
    \begin{example}
      \[
        n = 3 \quad \alpha = i + 5 \in \mathbb{C}
      \] 
      \[
        E_3(i+5) = \begin{pmatrix} 
          1 & 0 & 0\\
          0 & 1 & 0\\
          0 & 0 & i+5 
        \end{pmatrix}
      \] 
    \end{example}

  \item \( E_{ij}(\alpha) \) ottenuta da \( I_n \) sommando la riga \( i \) con la 
    riga \( j \) moltiplicata per lo scalare \( \alpha \in \mathbb{C} \) 
    \begin{example}
      \[
        n = 3 \quad \alpha = \frac{-5}{6} \in \mathbb{C}
      \] 
      \[
        E_{13} = \begin{pmatrix} 
          1 & 0 & \frac{-5}{6}\\
          0 & 1 & 0\\
          0 & 0 & 1
        \end{pmatrix}
      \] 
    \end{example}
\end{itemize}

\subsection{Moltiplicazione con matrici elementari}
\begin{figure}[H]
  \begin{example}
    \[
      A = \begin{pmatrix} 
        1 & 0\\
        0 & 3\\
        -1 & 5
      \end{pmatrix} 
    \] 
    \[
      E_{23}A = \begin{pmatrix} 
        1 & 0 & 0\\
        0 & 0 & 1\\
        0 & 1 & 0
      \end{pmatrix}
      \begin{pmatrix} 
        1 & 0\\
        0 & 3\\
        -1 & 5
      \end{pmatrix}
      =
      \begin{pmatrix} 
        1 & 0\\
        -1 & 5\\
        0 & 3
      \end{pmatrix}
    \] 
    \[
      E_3(i+5)A = \begin{pmatrix} 
        1 & 0 & 0\\
        0 & 1 & 0\\
        0 & 0 & i+5
      \end{pmatrix}
      \begin{pmatrix} 
        1 & 0\\
        0 & 3\\
        -1 & 5
      \end{pmatrix}
      =
      \begin{pmatrix} 
        1 & 0\\
        0 & 3\\
        -i-5 & 5(i+5)
      \end{pmatrix}
    \] 
    \[
      E_{13}A = \begin{pmatrix} 
        1 & 0 & \frac{-5}{6}\\
        0 & 1 & 0\\
        0 & 0 & 1
      \end{pmatrix}
      \begin{pmatrix} 
        1 & 0\\
        0 & 3\\
        -1 & 5
      \end{pmatrix}
      =
      \begin{pmatrix} 
        \frac{11}{6} & \frac{-25}{6}\\
        0 & 3\\
        -1 & 5
      \end{pmatrix}
    \] 
  \end{example}
\end{figure}
\noindent Osserviamo che ogni operazione elementare su una matrice \( A \in M_{m \times n} (\mathbb{C}) \)
corrisponde alla (pre)moltiplicazione di \( A \) con la matrice elementare ottenuta da
\( I_m \) effettuando la medesima operazione elementare.
\begin{figure}[H]
  \begin{define}
    \[
      AE_1(-\pi ) = \begin{pmatrix} 
        1 & 0\\
        0 & 3\\
        -1 & 5
      \end{pmatrix}
      \begin{pmatrix} 
        -\pi & 0\\
        0 & 1\\
      \end{pmatrix} 
      =
      \begin{pmatrix} 
        -\pi & 0\\
        0 & 3\\
        \pi & 5
      \end{pmatrix}
    \] 
  \end{define}
\end{figure}
\begin{figure}[H]
  \begin{example}
    \[
      A = \begin{pmatrix} 
        1 & -1 & 0\\
        3 & 2 & 15
      \end{pmatrix} 
      \stackrel{EG}{\leadsto}
    \] 
    \[
      \stackrel{R2-3R1}{\underset{E_{21}(-3)}{\leadsto}}
      \underbrace{
        \begin{pmatrix} 
          1 & -1 & 0\\
          0 & 5 & 15
      \end{pmatrix}}_{\equiv E_{21}A}
      \stackrel{\frac{1}{5}R2}{\underset{E_2(\frac{1}{5})}{\leadsto}}
      \underbrace{
        \begin{pmatrix} 
          1 & -1 & 0\\
          0 & 1 & 3
      \end{pmatrix}}_{\equiv E_2(\frac{1}{5})(E_{21}(-3)A)}
      =U
    \] 
    Otteniamo una matrice con 2 pivot e 2 colonne dominanti. Questa matrice viene chiamata
    \textbf{forma ridotta di \( A \) }. Quindi il calcolo può essere anche fatto in questo modo:
    \[
      U = E_2\left(\frac{1}{5}\right)(E_{21}(-3)A)=
    \] 
    \[
      = \begin{pmatrix}
        1 & 0\\
        0 & \frac{1}{5}
      \end{pmatrix} 
      \begin{pmatrix} 
        1 & 0\\
        -3 & 1
      \end{pmatrix} 
      \begin{pmatrix} 
        1 & -1 & 0\\
        3 & 2 & 15
      \end{pmatrix} =
    \] 
    \[
      = \underbrace{\begin{pmatrix} 
          1 & 0\\
          -\frac{3}{5} & \frac{1}{5}
      \end{pmatrix} }_{E}
      \begin{pmatrix} 
        1 & -1 & 0\\
        3 & 2 & 15
      \end{pmatrix} 
    \] 
    \vspace{0.5cm}
    \[
      \begin{pmatrix}[ccc|cc]
        1 & -1 & 0 & 1 & 0\\
        3 & 2 & 15 & 0 & 1
      \end{pmatrix} 
      \stackrel{R2-3R1}{\leadsto}
      \begin{pmatrix}[ccc|cc] 
        1 & -1 & 0 & 1 & 0\\
        0 & 5 & 15 & -3 & 1
      \end{pmatrix} 
    \] 
    \[
      \stackrel{\frac{1}{5}R2}{\leadsto}
      \begin{pmatrix}[ccc|cc] 
        1 & -1 & 0 & 1 & 0\\
        0 & 1 & 3 & -\frac{3}{5} & \frac{1}{5}
      \end{pmatrix}
    \] 
    A sinistra della barra abbiamo la matrice \( U \) e a destra la matrice \( E \).
  \end{example}
\end{figure}

\subsection{Definizione di matrice invertibile}
Una matrice \( A \in M_{n \times n}(\mathbb{C}) \) si dice \textbf{invertibile} se esiste
\( C \in M_{n \times n}(\mathbb{C}) \) tale che:
\[
  CA = I_n \quad \text{e} \quad AC = I_n
\]
In tal caso, \( C \) è detta \textbf{inversa} di \( A \). L'inversa di \( A \), quando
esiste, è univocamente determinata e si denota con \( A^{-1} \). Infatti, se \( C \) e \( C' \) 
sono due matrici inverse di \( A \) , allora:
\[
  C = I_nC = (C'A)C = C'(AC) = C'I_n = C'
\] 
\begin{figure}[H]
  \begin{example}
    \[
    A = \begin{pmatrix}
      2 & 5\\
      -3 & -7
    \end{pmatrix} 
    \quad
    C = \begin{pmatrix} 
      -7 & -5\\
      3 & 2
    \end{pmatrix} 
    \] 
    \[
    AC = \begin{pmatrix} 
      2 & 5\\
      -3 & -7
    \end{pmatrix}
    \begin{pmatrix} 
      -7 & -5\\
      3 & 2
    \end{pmatrix}
    =
    \begin{pmatrix} 
      1 & 0\\
      0 & 1
    \end{pmatrix}
    \] 
    \[
    CA = \begin{pmatrix} 
      -7 & -5\\
      3 & 2
    \end{pmatrix}
    \begin{pmatrix} 
      2 & 5\\
      -3 & -7
    \end{pmatrix}
    =
    \begin{pmatrix} 
      1 & 0\\
      0 & 1
    \end{pmatrix}
    \] 
    \[
      \leadsto C = A^{-1}
    \] 
  \end{example}
\end{figure}

\noindent Se \( A,B \in M_{m \times n}(\mathbb{C}) \) sono invertibili, allora lo è anche
il loro prodotto \( AB \). Infatti l'inversa di \( AB \) è \( B^{-1}A^{-1} \).
Infatti:
\[
  (AB)(B^{-1}A^{-1}) = A(BB^{-1})A^{-1} = A I_n A^{-1} = AA^{-1} = I_n
\] 
\begin{center}
  oppure
\end{center}
\[
  (B^{-1}A^{-1})(AB) = B^{-1}(A^{-1}A)B = (B^{-1}I_n)B = B^{-1}B = I_n
\] 
Quindi \( (AB)^{-1} = B^{-1}A^{-1} \).

\subsection{Inverse di matrici elementari}
Le matrici elementari sono tutte invertibili con inverse:
\[
  E_{ij}^{-1} = E_{ij}
\] 
\begin{figure}[H]
  \begin{example}
    \[
      E_{23} = \begin{pmatrix} 
        1 & 0 & 0\\
        0 & 0 & 1\\
        0 & 1 & 0
      \end{pmatrix}
    \] 
    \[
      \begin{pmatrix} 
        1 & 0 & 0\\
        0 & 0 & 1\\
        0 & 1 & 0
      \end{pmatrix} 
      \begin{pmatrix} 
        1 & 0 & 0\\
        0 & 0 & 1\\
        0 & 1 & 0
      \end{pmatrix}
      =
      \begin{pmatrix} 
        1 & 0 & 0\\
        0 & 1 & 0\\
        0 & 0 & 1
      \end{pmatrix}
    \] 
  \end{example}
\end{figure}
\[
  E_i(\alpha)^{-1} = E_i(\frac{1}{\alpha})
\] 
\begin{figure}[H]
  \begin{example}
    \[
      E_3(i+5) = \begin{pmatrix} 
        1 & 0 & 0\\
        0 & 1 & 0\\
        0 & 0 & i+5
      \end{pmatrix}
    \] 
    \[
      E_3(\frac{1}{i+5})E_3(i+5) = I_3
    \]
    \[
      \begin{pmatrix} 
        1 & 0 & 0\\
        0 & 1 & 0\\
        0 & 0 & i+5
      \end{pmatrix}
      \begin{pmatrix} 
        1 & 0 & 0\\
        0 & 1 & 0\\
        0 & 0 & \frac{1}{1+5} 
      \end{pmatrix}
      =
      \begin{pmatrix} 
        1 & 0 & 0\\
        0 & 1 & 0\\
        0 & 0 & 1
      \end{pmatrix}
    \] 
  \end{example}
\end{figure}
\[
  E_{ij}(\alpha)^{-1} = E_{ij}(-\alpha)
\] 
\begin{figure}[H]
  \begin{example}
    \[
      E_{23}(-\frac{5}{6}) = \begin{pmatrix} 
        1 & 0 & \frac{-5}{6}\\
        0 & 1 & 0\\
        0 & 0 & 1
      \end{pmatrix}
    \] 
    \[
      E_{23}(\frac{5}{6})E_{23}(-\frac{5}{6}) = I_3
    \] 
    \[
      \begin{pmatrix} 
        1 & 0 & 0\\
        0 & 1 & 0\\
        0 & 0 & \frac{5}{6}
      \end{pmatrix}
      \begin{pmatrix} 
        1 & 0 & \frac{-5}{6}\\
        0 & 1 & 0\\
        0 & 0 & 1
      \end{pmatrix}
      =
      \begin{pmatrix} 
        1 & 0 & 0\\
        0 & 1 & 0\\
        0 & 0 & 1
      \end{pmatrix}
    \] 
  \end{example}
\end{figure}

\subsection{Proposizione}
Sia \( Ax = b \) un sistema lineare in forma matriciale, cioè \( A \in M_{m \times n}(\mathbb{C}) \) 
e \( b \in M_{m \times 1}(\mathbb{C}) \). Se \( (U|c) \) è una forma ridotta della matrice
aumentata \( (A|b) \), allora i sistemi lineari \( Ax = b \) e \( Ux = c \) hanno le
stesse soluzioni, cioè sono equivalenti.

\subsubsection{Dimostrazione}
Siano \( E_1, \ldots, E_s \) le matrici elementari che trasformano \( (A|b) \) nella forma
ridotta \( (U|c) \). Allora:
\[
  (A|b) \underset{E_1}{\sim} (A'|b') \underset{E_2}{\sim} \ldots \underset{E_s}{\sim} (U|c)
\] 
Allora abbiamo:
\[
  (U|c) = E_s \ldots \underbrace{E_1(A|b)}_{(A'|b')}
\] 
Per 3.10, le matrici elementari \( E_1, \ldots, E_s \) sono invertibili. Dunque anche il
prodotto \( E = E_s \ldots E_1 \) è invertibile con \( E^{-1} = E_1^{-1} \ldots E_s^{-1} \).
Abbiamo che \( E(A|b) = (U|c) \), ovvero \( EA = U \) e \( Eb = c \).
Pertanto, se \( v \in M_{n \times 1}(\mathbb{C}) \) è una soluzione di \( Ax = b \),
cioè \( Av = b \), allora:
\[
  Uv = (EA)v = E(Av) = Eb = c
\] 
Quindi \( v \) è soluzione di \( Ux = c \).

Se \( v \in M_{a \times  1})\mathbb{C} \) è soluzione di \( Ux = c \), cioè \( Uv = c \),
allora:
\[
  Av = \underbrace{(E^{-1}E)}_{I_m}Av = E^{-1}(EA)v = E^{-1}(Uv) = E^{-1}c =
\] 
\[
  = E^{-1}(Eb) = \underbrace{(E^{-1}E)}_{I_m}b = b
\] 
Quindi \( v \) è soluzione di \( Ax = b \quad \square \). 

\subsection{Proposizione}
Sono equivalenti i seguenti enunciati per \( A \in  M_{m \times n}(\mathbb{C}) \):
\begin{enumerate}
  \item Il sistema lineare \( Ax = b \) ammette soluzione per qualsiasi \( b \in M_{m \times 1}(\mathbb{C}) \).
  \item Il rango \( rk(A) \) di \( A \) è pari al numero di righe di \( A \).
\end{enumerate}

\subsubsection{Dimostrazione}
Dimostriamo che 1. implica 2. Supponiamo (1.)

\vspace{1em}
\noindent Sia \( U \) una forma ridotta di \( A \):
\begin{itemize}
  \item[] 
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}
      \node at (0,0) {
          \(
          r
          \left(
            \begin{matrix}
              0      & \ldots & 0      & 1      & *      & \ldots & *      & *      & *      & \ldots \\
              \vdots & \ddots & \vdots & 0      & 0      & \ldots & 0      & 1      & *      & \ldots \\
              \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots & 0      & 0      & \ldots \\
              \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots \\
              \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots \\
              \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots \\
              0      & \ldots & 0      & 0      & 0      & \ldots & 0      & 0      & 0      & \ldots
            \end{matrix}
            \;\;\;
          \begin{matrix}
            *      & \ldots & \ldots & *      & *      & \ldots & *      \\
            *      & \ldots & \ddots & *      & *      & \ldots & *      \\
            0      & 1      & \ddots & \vdots & \vdots & \ddots & \vdots \\
            \vdots & 0      & \ddots & 1      & *      & \ldots & *      \\
            \vdots & \vdots & \ddots & 0      & 0      & \ldots & 0      \\
            \vdots & \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
            0      & 0      & \ldots & 0      & 0      & \ldots & 0
          \end{matrix}
        \right)
        \)
      };
    \draw[green,thick] (-3.2, 2) -- (-3.2,1.6) -- ++(2.35,0) -- ++(0,-0.65) -- ++(2.35,0)
      -- ++(0,-0.65) -- ++(0.6,0) -- ++(0.8,-0.65) -- ++(2.35,0);

    \draw[green, thick] (-2.95,1.87) circle (0.21);
    \draw[<-] (-2.95,2.15) -- (-2.95,2.5) node[above] {Pivot};
    \draw[green, thick] (-0.6,1.24) circle (0.21);
    \draw[green, thick] (1.78,0.59) circle (0.21);
    \draw[green, thick] (3.18,-0.07) circle (0.21);

    \draw (-3.2,-1.9) -- ++(0,-0.2) -- ++(0.5,0) -- ++(0,0.2) node[below] (dom1) {};
    \draw[green, fill, opacity=0.2] (-3.2,-2.1) rectangle (-2.7,2.1);
    \draw (-0.88,-1.9) -- ++(0,-0.2) -- ++(0.5,0) -- ++(0,0.2) node[below left] (dom2) {};
    \draw[green, fill, opacity=0.2] (-0.88,-2.1) rectangle (-0.38,2.1);
    \draw (1.55,-1.9) -- ++(0,-0.2) -- ++(0.5,0) -- ++(0,0.2) node[below left, yshift=-2] (dom3) {};
    \draw[green, fill, opacity=0.2] (1.55,-2.1) rectangle (2.05,2.1);
    \draw (2.95,-1.9) -- ++(0,-0.2) -- ++(0.5,0) -- ++(0,0.2) node[below left, yshift=-3] (dom4) {};
    \draw[green, fill, opacity=0.2] (2.95,-2.1) rectangle (3.45,2.1);

    \node[align=center] at (0,-3.5) (col) {Colonne\\dominanti};

    \draw[->] (col) -- (dom1);
    \draw[->] (col) -- (dom2);
    \draw[->] (col) -- (dom3);
    \draw[->] (col) -- (dom4);

  \end{tikzpicture}
\end{figure}
\end{itemize}
Queste righe esistono se e solo se \( rk(U) < \) numero di righe di \( U \). 

\vspace{1em}
\noindent Esiste una matrice invertibile \( E \) tale che \( U = EA \) (\( E = \) prodotto
delle matrici elementari dell'Eliminaizone di Gauss). Consideriamo il vettore
\( C = \begin{pmatrix} 0 \\ \vdots \\ 0 \\ 1 \end{pmatrix} \) e mettiamo \( b = E^{-1}C \).
Allora il sistema lineare \( Ax=b \) ammette una soluzione \( v \) per (1.), cioè
\( Av = b \). Allora \( Uv = Eb = E(E^{-1}C) = C \) per (3.11). Per il teorema di
\textbf{Rouché-Capelli}, \( rk(U) = rk(U|c) \), cioè:
\[
  (U|c) = \begin{pmatrix}[ccccccc|c]
    1 & * & \ldots & * & * & \ldots & * & 0\\
    0 & 0 & \ldots & 0 & 1 & \ldots & * & 0\\
    \vdots & \vdots & \ddots & \vdots & \vdots & \ddots & \vdots & \vdots\\
    * & * & \ldots & * & * & \ldots & * & 1
  \end{pmatrix} 
\] 
L'ultima riga non può essere nulla, altrimenti l'ultima colonna di \( (U|c) \) sarebbe una
colonna dominante.

\vspace{1em}
\noindent Dunque \( rk(A) = rk(U) = \) numero di righe di \( U \) \( = \) numero di righe di \( A \).

\vspace{2em}
\noindent Dimostriamo che 2. implica 1. Supponiamo (2.)

\vspace{1em}
\noindent Sia \( b \in  M_{m \times 1}(\mathbb{C}) \) e consideriamo \( Ax = b \). Eseguendo l'Eliminazione
di Gauss sulla matrice \( (A|b) \), otteniamo una forma ridotta \( (U|c) \).
Siccome \( rk(U) = \) numero di righe di \( U \), ogni riga di \( U \) contiene un pivot.
Perciò \( rk(U) = rk(U|c) \) e quindi \( rk(A) = rk(A|b) \). Quindi siamo nel caso di
una soluzione unica, oppure nel caso di infinite soluzioni del teorema di \textbf{Rouché-Capelli}.
\( \quad \square \) 

\section{Matrici inverse e determinante}
\begin{figure}[H]
  \begin{example}
    \[
      A = \begin{pmatrix} 
        1 & 2 & 0\\
        5 & 11 & -1\\
        -4 & -10 & -2
      \end{pmatrix} 
    \] 
    Eseguiamo l'Eliminazione di Gauss e calcoliamo il prodotto delle matrici elementari
    contemporaneamente:
    \[
      \begin{pmatrix}[ccc|ccc] 
        1 & 2 & 0 & 1 & 0 & 0\\
      5 & 11 & -1 & 0 & 1 & 0\\
      -4 & -10 & -2 & 0 & 0 & 1
    \end{pmatrix} 
    \stackrel{E_{21}(-5)}{\underset{E_{31}4}{\leadsto}}
    \begin{pmatrix}[ccc|ccc] 
      1 & 2 & 0 & 1 & 0 & 0\\
      0 & 1 & -1 & -5 & 1 & 0\\
      0 & -2 & -2 & 4 & 0 & 1
    \end{pmatrix}
   \]
   \[
     \stackrel{E_{32}(2)}{\leadsto}
     \begin{pmatrix}[ccc|ccc]
       1 & 2 & 0 & 1 & 0 & 0\\
       0 & 1 & -1 & -5 & 1 & 0\\
       0 & 0 & -4 & -6 & 2 & 1
     \end{pmatrix}
     \stackrel{E_3(-\frac{1}{4})}{\leadsto}
     \begin{pmatrix}[ccc|ccc]
       1 & 2 & 0 & 1 & 0 & 0\\
       0 & 1 & -1 & -5 & 1 & 0\\
       0 & 0 & 1 & \frac{3}{2} & -\frac{1}{2} & -\frac{1}{4}
      \end{pmatrix}
   \] 
   A sinistra della barra abbiamo la matrice ridotta \( U \) , a destra abbiamo il prodotto delle
   matrici elementari. Cioè: 
   \[
     E_3(-\frac{1}{4})E_{32}(2)E_{31}(4)E_{21}(-5)
   \] 
   Siccome \( rk(U) = 3 \), possiamo continuare per ottenere la matrice identità:
   \[
     (U|E) \stackrel{E_{23}(1)}{\leadsto} \begin{pmatrix}[ccc|ccc]
       1 & 2 & 0 & 1 & 0 & 0\\
       0 & 1 & 0 & -\frac{7}{2} & \frac{1}{2} & -\frac{1}{4}\\
       0 & 0 & 1 & \frac{3}{2} & -\frac{1}{2} & -\frac{1}{4}
     \end{pmatrix} 
   \] 
   \[
     \stackrel{E_{12}(-2)}{\leadsto}
     \begin{pmatrix}[ccc|ccc]
       1 & 0 & 0 & 8 & -1 & \frac{1}{2}\\
       0 & 1 & 0 & -\frac{7}{2} & \frac{1}{2} & -\frac{1}{4}\\
       0 & 0 & 1 & \frac{3}{2} & -\frac{1}{2} & -\frac{1}{4}
     \end{pmatrix} 
   \] 
   A sinistra della barra abbiamo la matrice identità, a destra abbiamo la matrice 
   \( E' = E_{12}(-2)E_{23}(1)E \). Allora:
   \[
     I_3 = E_{12}(-2)E_{23}(1)U = E_{12}(-2)E_{23}(1)E \cdot A =
   \] 
   \[
     = E'A
   \] 
   Osserviamo che:
   \[
   AE' = \begin{pmatrix} 
     1 & 2 & 0\\
     5 & 11 & -1\\
     -4 & -10 & -2
   \end{pmatrix} 
   \begin{pmatrix} 
     8 & -1 & \frac{1}{2}\\
     -\frac{7}{2} & \frac{1}{2} & -\frac{1}{4}\\
     \frac{3}{2} & -\frac{1}{2} & -\frac{1}{4}
   \end{pmatrix} 
   =
   \begin{pmatrix} 
     1 & 0 & 0\\
     0 & 1 & 0\\
     0 & 0 & 1
   \end{pmatrix} 
   \] 
   Dunque \( A^{-1}=E' \) 
  \end{example}
\end{figure}

\subsection{Proposizione}
Sia \( A \in M_{n \times n}(\mathbb{C}) \). Allora \( A \) è invertibile se e solo se
esiste una sequenza di matrici elementari \( E_1, \ldots, E_t \) tale che \( I_n = 
(E_t \ldots E_1 A) \).

\subsubsection{Dimostrazione}
Supponiamo che \( A \) sia invertibile. Per ogni \( b \in M_{n \times 1}(\mathbb{C}) \),
il vettore \( A^{-1}b =: v \) è soluzione del sistema lineare \( Ax = b \). Infatti:
\[
  Av = b = A(A^{-1}b) = (AA^{-1})b = I_nb = b
\] 
Per (3.12), abbiamo che \( rk(A) = n \). Esiste una forma ridotta \( U \) di \( A \) tale
che \( rk(U) = n \) e
\[
  U = \begin{pmatrix} 
    1 & * & \ldots & \ldots & * & *\\
    0 & 1 & * & \ldots & * & *\\
    \vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
    0 & 0 & 0 & \ldots & 0 & 1
  \end{pmatrix} 
\] 
con 1 sulla diagnoale e matrici elementari \( E_1, \ldots, E_t \) tali che \( U = E_t
\ldots E_1A \). Proseguendo come nell'esempio precedente, otteniamo le matrici elementari
\( E_{t+1}, \ldots, E_s \) tali che:
\[ I_n = E_s \ldots E_{t+1}U = E_s \ldots E_{t+1}E_t
\ldots E_1A \]
Ora supponiamo che esistano le matrici elementari \( E_1, \ldots, E_s \) tali che:
\[
 I_n = E_s \ldots E_1A
\] 
Per 3.10, le matrici elementari sono invertibili. Dunque:
\[
  E_i^{-1} \ldots E_s^{-1} = E_i^{-1} \ldots E_s^{-1}I_n =
  \overbrace{\underbrace{E_i^{-1} \ldots E_s^{-1}}_{(E_s \ldots E_1)^{-1}}E_s \ldots E_1}^{I_n}
  A = A
\]
\( A \) è un prodotto di matrici invertibili, quindi è invertibile con \( A^{-1} = Es \ldots E_1 \) 
\( \quad \square \) 

\subsection{Calcolo della matrice inversa}
\label{th:4.2}
Data una matrice invertibile \( A \in M_{m \times n}(\mathbb{C}) \). Usiamo le operazioni
elementari per trasformare \( A \) nella matrice identità, e eseguiamo le stesse
operazioni elementari su \( I_n \) per ottenere \( A^{-1} \):
\[
  (A|I_n) \stackrel{E_1}{\leadsto} (A'|E') \stackrel{E_2}{\leadsto} \ldots \stackrel{E_s}{\leadsto}
  (I_n|A^{-1})
\] 
\begin{figure}[H]
  \begin{example}
    \[
    A = \begin{pmatrix} 
      1 & 2 & 3\\
      0 & 1 & 4\\
      5 & 6 & 0
    \end{pmatrix} 
    \] 
    \[
      \begin{pmatrix}[ccc|ccc]
        1 & 2 & 3 & 1 & 0 & 0\\
        0 & 1 & 4 & 0 & 1 & 0\\
        5 & 6 & 0 & 0 & 0 & 1
    \end{pmatrix} 
    \stackrel{E_{31}(-5)}{\underset{R3-5R1}{\leadsto}}
    \begin{pmatrix}[ccc|ccc] 
      1 & 2 & 3 & 1 & 0 & 0\\
      0 & 1 & 4 & 0 & 1 & 0\\
      0 & -4 & -15 & -5 & 0 & 1
    \end{pmatrix}
    \] 
    \[
      \stackrel{E_{32}(4)}{\underset{R3+4R2}{\leadsto}}
      \begin{pmatrix}[ccc|ccc] 
        1 & 2 & 3 & 1 & 0 & 0\\
        0 & 1 & 4 & 0 & 1 & 0\\
        0 & 0 & 1 & -5 & 4 & 1
    \end{pmatrix} 
    \stackrel{E_{23}(-4)}{\underset{R2-4R3}{\leadsto}}
    \begin{pmatrix}[ccc|ccc] 
      1 & 2 & 3 & 1 & 0 & 0\\
      0 & 1 & 0 & 20 & -15 & -4\\
      0 & 0 & 1 & -5 & 4 & 1
    \end{pmatrix}
    \] 
    \[
      \stackrel{E_{13}(-3)}{\underset{R1-3R3}{\leadsto}}
      \begin{pmatrix}[ccc|ccc] 
        1 & 2 & 0 & 16 & -12 & -3\\
        0 & 1 & 0 & 20 & -15 & -4\\
        0 & 0 & 1 & -5 & 4 & 1
      \end{pmatrix}
      \stackrel{E_{12}(-2)}{\underset{R1-2R2}{\leadsto}}
      \begin{pmatrix}[ccc|ccc] 
        1 & 0 & 0 & 24 & 18 & 5\\
        0 & 1 & 0 & 20 & -15 & -4\\
        0 & 0 & 1 & -5 & 4 & 1
      \end{pmatrix}
    \] 
    A sinistra della barra abbiamo la matrice identità \( I_3 \) , a destra abbiamo 
    la matrice inversa \( A^{-1} \) 
  \end{example}
\end{figure}

\subsection{Teorema delle matrici invertibili}
Sono equivalenti i seguenti enunciati per \( A \in M_{n \times n}(\mathbb{C}) \):
\begin{enumerate}
  \item[(a)] \( A \) è invertibile.
  \item[(b)] Esiste una sequenza di matrici elementari \( E_1, \ldots, E_t \) tale che:
    \[
      I_n = E_t \ldots E_1 A
    \]
  \item[(c)] \( rk(a) = n \)
  \item[(d)] Il sistema lineare \( Ax = b \) ammette una soluzione per qualsiasi vettore \( b \in 
    M_{n \times 1}(\mathbb{C}) \).
  \item[(e)] Il sistema lineare \( Ax = \underbrace{o}_{\text{vettore nullo}} \) ha una
    sola soluzione, cioè: \[ x = 0 = \begin{pmatrix} 0 \\ \vdots \\ 0 \end{pmatrix}  \]
  \item[(f)] Esiste una matrice \( C \in M_{n \times n}(\mathbb{C}) \) tale che:
    \[
      CA = I_n
    \]
  \item[(g)] Esiste una matrice \( D \in M_{n \times n}(\mathbb{C}) \) tale che:
    \[
      AD = I_n
    \]
\end{enumerate}

\subsubsection{Dimostrazione}
\begin{figure}[H]
  \centering
  \begin{tikzcd}
    & (b) \arrow[d, Leftrightarrow, "\surd\;(4.2)"] \\
    (g) \arrow[r, Leftarrow, "\surd"] & (a) \arrow[r, Rightarrow, "\surd"] & (f) \arrow[d,Rightarrow, "\surd"]\\
    (d) \arrow[u, Leftarrow, "dim\;(4.1)"]& \arrow[l, Leftarrow, "dim\;(3.12)"] (c) \arrow[u, Rightarrow, "dim\;(4.1)"] 
                                          & \arrow[l, Rightarrow, "1 soluz."] (e)\\
  \end{tikzcd}
  \caption{Diagramma delle implicazioni}
\end{figure}
\begin{itemize}
  \item[\( (f) \Rightarrow (e) \) ] Supponiamo che \( \exists C \in M_{n \times n}(\mathbb{C}) \)
    tale che \( CA = I_n \). Sia \( v \in  M_{n \times 1}(\mathbb{C}) \) una soluzione
    del sistema \( Ax = 0 \). Allora: 
    \[ v = I_nv = (CA)v = C(Av) = Co = 0 \]
    Ad esempio:
    \[
    \begin{pmatrix} 
      1 & 2\\
      3 & 4
    \end{pmatrix} 
    \begin{pmatrix} 0\\0 \end{pmatrix} 
    =
    \begin{pmatrix} 0\\0 \end{pmatrix}
    \] 
    \( \square \) 
\end{itemize}

\begin{figure}[H]
  \begin{define}
    Sia \( D \) la matrice inversa destra:
    \[
    D = I_nD = (CA)D = C(AD) = CI_n = C
    \] 
    Osserviamo che \( D = C \). Quindi:
    \[
      C = D = A^{-1}
    \] 
  \end{define}
\end{figure}

\subsection{Proposizione (Determinante di una matrice)}
Sia \( A \in M_{2 \times 2}(\mathbb{C}) \) tale che:
\[
A = \begin{pmatrix} 
  a & b\\
  c & d
\end{pmatrix} 
\] 
Se \( ad-bc \neq 0 \), allora \( A \) è invertibile e \( A^{-1} = \frac{1}{ad-bc} \begin{pmatrix} 
  d & -b\\
  -c & a
\end{pmatrix}  \) 
Se \( ad-bc = 0 \), allora \( A \) non è invertibile.

\vspace{1em}
\noindent \( ad-bc \) è detto \textbf{determinante} di \( A \) e si indica con \( \det(A) \).
\subsubsection{Dimostrazione}
\[
  \begin{pmatrix} 
    a & b\\
    c & d
  \end{pmatrix} 
  \left( 
    \frac{1}{ad-bc}
    \begin{pmatrix} 
      d & -b\\
      -c & a
    \end{pmatrix} 
  \right) =
\] 
\[
  = \frac{1}{ad-bc}
  \begin{pmatrix} 
    a & b\\
    c & d
  \end{pmatrix} 
  \begin{pmatrix} 
    d & -b\\
    -c & a
  \end{pmatrix} 
  =
\] 
\[
  = \frac{1}{ad-bc}
  \begin{pmatrix} 
    ad-bc & 0\\
    0 & ad-bc
  \end{pmatrix} 
  =
  \begin{pmatrix} 
    1 & 0\\
    0 & 1
  \end{pmatrix} 
  = I_2
\] 
Quindi \( A \) è invertibile e \( A^{-1} = \frac{1}{ad-bc} \begin{pmatrix} 
  d & -b\\
  -c & a
\end{pmatrix}  \).

\vspace{1em}
\noindent Se \( ad-bc = 0 \), allora:
\[
  \begin{pmatrix} 
    a & b\\
    c & d
  \end{pmatrix} 
  \begin{pmatrix} 
    d\\-c
  \end{pmatrix} 
  =
  \begin{pmatrix} 
    ad-bc\\
    cd-cd
  \end{pmatrix} 
  =
  \begin{pmatrix} 
    0\\0
  \end{pmatrix}
\] 
Quindi \( \begin{pmatrix} d\\-c \end{pmatrix}  \) è soluzione al sistema \( Ax=0 \).
Se \( \begin{pmatrix} d\\-c \end{pmatrix} \neq \begin{pmatrix} 0\\0 \end{pmatrix}  \),
allora \( A \) non è invertibile per (4.3(e)).

\vspace{1em}
\noindent Se \( \begin{pmatrix} d\\-c \end{pmatrix} = \begin{pmatrix} 0\\0 \end{pmatrix}  \),
allora \( d = c = 0 \) e:
\[
A = \begin{pmatrix} 
  a & b\\
  0 & 0
\end{pmatrix} 
\] 
che ha rango \( <2 \), quindi \( A \) non è invertibile per (4.3(c)). \( \quad \square \) 

\subsection{Definizione di determinante}
\begin{figure}[H]
  \begin{definition}
    Definiamo una funzione \( \det : M_{n \times n}(\mathbb{C}) \to \mathbb{C} \) detta
    \textbf{determinante} per ricorrenza:
    \begin{itemize}
      \item \( n = 1 \):
        \[
        A = (a) \quad \det(A) = a
        \] 
      \item \( n = 2 \) (4.3):
        \[
        A = \begin{pmatrix} 
          a & b\\
          c & d
        \end{pmatrix} 
        \quad \det(A) = ad-bc
        \]
      \item \( n \ge 3 \):
        \[
          A = (A_{ij})_{1 \le j \le n}= \begin{pmatrix} 
          a_{11} & a_{12} & \ldots & a_{1n}\\
          a_{21} & a_{22} & \ldots & a_{2n}\\
          \vdots & \vdots & \ddots & \vdots\\
          a_{n1} & a_{n2} & \ldots & a_{nn}
        \end{pmatrix} 
        \]
        \[
        \det(A) = \sum_{j=1}^{n}(-1)^{1+j}a_{1j}\det(A_{1j})
        \]
        dove \( A_{1j} \) è la matrice ottenuta da \( A \) cancellando la prima riga e la
        colonna \( j \).
    \end{itemize}
  \end{definition}
\end{figure}
\begin{figure}[H]
  \begin{example}
    \[
      A = \begin{pmatrix} 
        1 & 2 & 3\\
        0 & 1 & 3\\
        1 & 2 & 0
      \end{pmatrix} 
      =
      \begin{pmatrix} 
        a_{11} & a_{12} & a_{13}\\
        a_{21} & a_{22} & a_{23}\\
        a_{31} & a_{32} & a_{33}
      \end{pmatrix} 
    \] 
    \[
      \det(A) = (-1)^{2}1\det\begin{pmatrix} 
        \cancel{1} & \cancel{2} & \cancel{3} \\
        \cancel{0} & 1 & 3\\
        \cancel{1} & 2 & 0
      \end{pmatrix} +
    \] 
    \[
      (-1)^3 2 \det\begin{pmatrix} 
        \cancel{1} & \cancel{2} & \cancel{3} \\
        0 & \cancel{1} & 3\\
        1 & \cancel{2} & 0
      \end{pmatrix} +
    \] 
    \[
      (-1)^4 3 \det\begin{pmatrix} 
        \cancel{1} & \cancel{2} & \cancel{3} \\
        0 & 1 & \cancel{3}\\
        1 & 2 & \cancel{0}
      \end{pmatrix} =
    \] 
    \[
     = -6 + 6 -3 = -3
    \] 
  \end{example}
\end{figure}

\begin{figure}[H]
  \begin{example}
    \[
      \det\begin{pmatrix} 
        1 & 2 & 3\\
        3 & 2 & 1\\
        0 & 1 & -1
      \end{pmatrix}
      = (-1)^{1+1} \cdot 1 \cdot \det\begin{pmatrix} 
        2 & 1\\
        1 & -1
      \end{pmatrix} +
    \] 
    \[
      (-1)^{1+2} \cdot 2 \cdot \det\begin{pmatrix} 
        3 & 1\\
        0 & -1
      \end{pmatrix} +
    \] 
    \[
      (-1)^{1+3} \cdot 3 \cdot \det\begin{pmatrix} 
        3 & 2\\
        0 & 1
      \end{pmatrix} =
    \] 
    \[
      = 1(-2-1) - 2(-3-0) + 3(3-0) = -3 + 6 + 9 = 12
    \] 
  \end{example}
\end{figure}

\subsection{Regola di Sarrus}
\begin{figure}[H]
  \begin{definition}
    Per una matrice di dimensione \( 3 \times 3 \) si può usare la regola di Sarrus:
    \[
      \begin{NiceArray}{>{\strut}ccccc}[margin,extra-margin = 1pt]
        a_{11} & a_{12} & a_{13} & a_{11} & a_{12}\\
        a_{21} & a_{22} & a_{23} & a_{21} & a_{22}\\
        a_{31} & a_{32} & a_{33} & a_{31} & a_{32}
        \CodeAfter
        \begin{tikzpicture}
          \node[scale=4.8] at (-3,0.1) {$\left(\quad\right)$};
          \node [fill=red, opacity=0.3, rounded corners=2pt, inner ysep = 0.2pt,
          rotate fit=-28, fit = (1-1) (3-3) ] {} ;
          \node [fill=red, opacity=0.3, rounded corners=2pt, inner ysep = 0.2pt,
          rotate fit=-28, fit = (1-2) (3-4) ] {} ;
          \node [fill=red, opacity=0.3, rounded corners=2pt, inner ysep = 1.8pt,
          rotate fit=-28, fit = (1-3) (3-5) ] {} ;


          \node [fill=blue, opacity=0.3, rounded corners=2pt, inner ysep = 0pt,
          rotate fit=25, fit = (1-3) (3-1) ] {} ;
          \node [fill=blue, opacity=0.3, rounded corners=2pt, inner ysep = 0.2pt,
          rotate fit=25, fit = (1-4) (3-2) ] {} ;
          \node [fill=blue, opacity=0.3, rounded corners=2pt, inner ysep = 1pt,
          rotate fit=25, fit = (1-5) (3-3) ] {} ;
        \end{tikzpicture}
      \end{NiceArray}
    \] 
    \[
      \det A = \color{red} a_{11}a_{22}a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32} 
\color{blue}- a_{13}a_{22}a_{31} - a_{12}a_{21}a_{33} - a_{11}a_{23}a_{32}
    \] 
  \end{definition}
\end{figure}

\begin{figure}[H]
  \begin{example}
    \[
    A = \begin{pmatrix} 
      1 & 2 & 3\\
      0 & 1 & 3\\
      1 & 2 & 0
    \end{pmatrix} 
    \] 
    Per la regola di Sarrus:
    \[
      A = \begin{pmatrix} 
        1 & 2 & 3\\
        0 & 1 & 3\\
        1 & 2 & 0
      \end{pmatrix} 
      \begin{matrix} 
        1 & 2\\
        0 & 1\\
        1 & 2
      \end{matrix} 
    \] 
    \[
    \det(A) = 1 \cdot 1 \cdot 0 + 2 \cdot 3 \cdot 1 + 3 \cdot 0 \cdot 2 
    \] 
    \[
    - 3 \cdot 1 \cdot 1 - 2 \cdot 0 \cdot 1 - 1 \cdot 3 \cdot 2 = -3
    \] 
    \[
    = 6 - 3 - 6 = -3
    \] 
  \end{example}
\end{figure}

\subsection{Teorema di Laplace}
\begin{figure}[H]
  \begin{definition}
    Il determinante di una matrice \( A = (a_{ij}) \) può essere sviluppato per qualsiasi
    riga o colonna come segue:
    \begin{itemize}
      \item Sviluppo per la riga \( i \) 
        \[
        \det(A) = \sum_{j=1}^{n}(-1)^{i+j}a_{ij}\det(A_{ij})
        \] 
        dove \( A_{ij} \) è la matrice ottenuta da \( A \) cancellando la riga \( i \) e
        la colonna \( j \).
      \item Sviluppo per la colonna \( j \) 
        \[
        \det(A) = \sum_{i=1}^{n}(-1)^{i+j}a_{ij}\det(A_{ij})
        \] 
        dove \( A_{ij} \) è la matrice ottenuta da \( A \) cancellando la riga \( i \) e
        la colonna \( j \).
    \end{itemize}
    Il valore \( (-1)^{i+j} \det(A_{ij}) \) è detto \textbf{complemento algebrico} di
    \( a_{ij} \). Il segno si determina secondo:
    \[
      \begin{pmatrix} 
        + & - & + & \ldots\\
        - & + & - & \ldots\\
        + & - & + & \ldots\\
        \vdots & \vdots & \vdots & \ddots
      \end{pmatrix} 
    \] 
  \end{definition}
\end{figure}

\begin{figure}[H]
  \begin{example}
    \[
    A = \begin{pmatrix} 
      1^+ & 2^- & 3^+\\
      0^- & 1^+ & 3^-\\
      1^+ & 2^- & 0^+
    \end{pmatrix} 
    \] 
    \begin{itemize}
      \item \textbf{Riga \( 3 \)}:
        \[
        \det(A) = 1 \cdot \det\begin{pmatrix} 
          \cancel{1} & 2 & 3\\
          \cancel{0} & 1 & 3\\
          \cancel{1} & \cancel{2} & \cancel{0}
        \end{pmatrix}
        \] 
        \[
        - 2 \cdot \det\begin{pmatrix} 
          0 & \cancel{1} & 3\\
          1 & \cancel{1} & 3\\
          \cancel{1} & \cancel{2} & \cancel{0}
        \end{pmatrix}
        \] 
        \[
        = (6 - 3) - 2(3-0) = 3-6 = -3
        \] 
      \item \textbf{Colonna \( 3 \)}:
        \[
        \det(A) = 3 \cdot \det\begin{pmatrix} 
          \cancel{1} & \cancel{2} & \cancel{3}\\
          0 & 1 & \cancel{3}\\
          1 & 2 & \cancel{0}
        \end{pmatrix}
        \] 
        \[
        - 3 \cdot \det\begin{pmatrix} 
          1 & 2 & \cancel{3}\\
          \cancel{0} & \cancel{1} & \cancel{3}\\
          1 & 2 & \cancel{0}
        \end{pmatrix}
        \] 
        \[
        = 3(1-6) - 3(1-6) = -15 + 15 = 0
        \]
    \end{itemize}
  \end{example}
\end{figure}

\subsection{Determinante e trasposta}
\[
A = \begin{pmatrix} 
  a & b\\
  c & d
\end{pmatrix} 
\quad
A^T = \begin{pmatrix} 
  a & c\\
  b & d
\end{pmatrix}
\] 
\[
\det(A) = ad-bc \quad \det(A^T) = ad-cb
\] 
\[
\Downarrow
\] 
\[
\det(A) = \det(A^T)
\] 
\begin{figure}[H]
  \begin{example}
    \[
    A = \begin{pmatrix} 
      1 & 2 & 3\\
      0 & 1 & 3\\
      1 & 2 & 0
    \end{pmatrix} 
    \quad \det(A) = -3
    \] 
    \[
    A^T = \begin{pmatrix} 
      1 & 0 & 1\\
      2 & 1 & 2\\
      3 & 3 & 0
    \end{pmatrix}
    \] 
    \[
      \det(A) \stackrel{R1}{=} 1 \cdot \det\underbrace{\begin{pmatrix} 
        1 & 3\\
        2 & 0
    \end{pmatrix}}_{A_{11}}
    - 2 \cdot \det\underbrace{\begin{pmatrix} 
        0 & 3\\
        1 & 0
    \end{pmatrix}}_{A_{12}}
    + 3 \cdot \det\underbrace{\begin{pmatrix} 
        0 & 1\\
        1 & 2
    \end{pmatrix}}_{A_{13}} = -3
    \] 
    \[
    \det(A^T) \stackrel{C1}{=} 1 \cdot \det\underbrace{\begin{pmatrix} 
      1 & 2\\
      3 & 0
    \end{pmatrix}}_{A^T_{11}}
    - 2 \cdot \det\underbrace{\begin{pmatrix} 
      0 & 1\\
      3 & 0
    \end{pmatrix}}_{A^T_{21}}
    + 3 \cdot \det\underbrace{\begin{pmatrix} 
      0 & 1\\
      1 & 2
    \end{pmatrix}}_{A^T_{31}} = -3
    \] 
  \end{example}
\end{figure}
Se \( A = (a_{ij})_{1 \le i,j \le n} \in M_{n \times n}(\mathbb{C}) \), allora:
\[
\det(A) = \det(A^T)
\] 

\subsection{Il principio di induzione}
Il principio di induzione serve a dimostrare che per ogni \( n \ge 1 \) vale una 
proprietà \( P(n) \). Nel nostro caso che ogni matrice \( A \) di dimensione \( n \times n \) 
, \( \det(A) = \det(A^T) \). Si procede in due passi:
\begin{itemize}
  \item \textbf{Base dell'induzione}:

    \( P(n) \) è vera per \( n = 1 \), ovvero \( P(1) \) è vera.
  \item \textbf{Passo induttivo}:

    Supponendo che \( p(n) \) sia vera; ne consegue che \( p(n+1) \) è vera.
\end{itemize}
Allora \( p(n) \) è vera per tutti gli \( n \in \mathbb{N} \).

\begin{figure}[H]
  \begin{example}
    \[
    A = \begin{pmatrix} 
      1 & 2 & 3 & 4\\
      0 & 5 & 6 & 7\\
      0 & 0 & 8 & 9\\
      0 & 0 & 0 & 10
    \end{pmatrix} 
    \] 
    Sviluppo per la riga 4:
    \[
    \det(A) = 10 \cdot \det\begin{pmatrix} 
      1 & 2 & 3 & \cancel{4}\\
      0 & 5 & 6 & \cancel{7}\\
      0 & 0 & 8 & \cancel{9}\\
      \cancel{0} & \cancel{0} & \cancel{0} & \cancel{10}
    \end{pmatrix}
    \] 
    Si utilizza di nuovo il teorema di Laplace per la matrice \( 3 \times 3 \) ottenuta:
    \[
    \stackrel{R3}{=} 10 \left( 8 \cdot \det\begin{pmatrix} 
      1 & 2 \\
      0 & 5
    \end{pmatrix}
    = 10 \cdot 8 \cdot (1 \cdot 5 - 0 \cdot 2) = 10 \cdot 8 \cdot 5 \cdot 1 = 400
    \right) 
    \] 
  \end{example}
\end{figure}

\subsection{Proposizione}
Sia \( A = (a_{ij})_{1 \le i,j \le n} \in M_{n \times n}(\mathbb{C}) \) una matrice
triangolare superiore o inferiore. Allora:
\[
\det(A) = a_{11}a_{22} \ldots a_{nn}
\] 
\textbf{Dimostrazione} (superiore):

\noindent Per induzione su \( n \):
\begin{itemize}
  \item \textbf{Proprietà \( P(n) \)}:

    Per \( A \in M_{n \times n}(\mathbb{C}) \), \( \det(A) = a_{11} \ldots a_{nn} \) 

  \item \textbf{Base dell'induzione}:
    \[
    A = (a_{11}) \in M_{1 \times 1}(\mathbb{C})
    \] 
    \[
    \det(A) = a_{11} \quad \text{Per definizione}
    \] 
  \item \textbf{Passo induttivo}:

    Supponiamo \( P(n) \) 
    \[
      A = (a_{ij}) \in M_{n+1 \times n+1}(\mathbb{C})
    \] 
    \[
      \det(A) \stackrel{Rn+1}{=} a_{n+1n+1} \cdot \underbrace{\det(A_{n+1n+1})}_{
        \text{mat. triang. sup. di dim. \( n \times n \) }
      } = a_{n+1n+1} (a_{nn}\ldots a_{11})
    \] 
    Quindi \( P(n+1) \) è vera.
\end{itemize}
Per il principio di induzione, abbiamo dimostrato che \( P(n) \) vale per ogni \( n \in \mathbb{N} \).

\noindent La dimostrazione per \( A \) triangolare inferiore è simile. \( \quad \square \) 

\begin{figure}[H]
  \begin{example}
    \[
      U = \begin{pmatrix} 
        1 & 8 & 0 & i\\
        0 & 1 & 2 & 0\\
        0 & 0 & 1 & 5-i\\
        0 & 0 & 0 & 1
      \end{pmatrix} 
    \] 
    La matrice è una matrice ridotta, cioè una matrice triangolare superiore con 1 sulla
    diagonale.
    \[
    \det(U) = 1
    \] 
    \[
    U' = \begin{pmatrix} 
      1 & 8 & 0 & i\\
      0 & 0 & 1 & 7\\
      0 & 0 & 0 & 1\\
      0 & 0 & 0 & 0
    \end{pmatrix} 
    \] 
    \[
    \det(U') = 0
    \] 
  \end{example}
\end{figure}

  \begin{example}
    \[
    A = \begin{pmatrix} 
      1 & 2 & 3\\
      0 & 1 & 3\\
      1 & 2 & 0
    \end{pmatrix} 
    \quad
    \det(A) = -3
    \] 
    \begin{itemize}
      \item 
        \[
          \det(E_{23} A) = \det \begin{pmatrix} 
            1 & 2 & 3\\
            1 & 2 & 0\\
            0 & 1 & 3
          \end{pmatrix} 
        \] 
        \[
          \stackrel{C1}{=} 1 \cdot \det \begin{pmatrix} 
            2 & 0\\
            1 & 3
          \end{pmatrix}
          - 1 \cdot \det \begin{pmatrix} 
            2 & 3\\
            1 & 3
          \end{pmatrix}
        \] 
        \[
          = (6-0)-(6-3) = 6-6+3 = 3 = -\det(A)
        \] 
      \item 
        \[
        \det(E_2(2)A) = det \begin{pmatrix} 
          1 & 2 & 3\\
          0 & 2 & 6\\
          1 & 2 & 0
        \end{pmatrix}
        \] 
        \[
        \stackrel{C2}{=} 1 \cdot \det \begin{pmatrix} 
          2 & 6\\
          2 & 0
        \end{pmatrix}
        + 1 \cdot \det \begin{pmatrix} 
          2 & 3\\
          2 & 6
        \end{pmatrix}
        \] 
        \[
        = -12 + 6 = -6 = 2\det(A)
        \] 
      \item
        \[
          \det(E_{13}(2)A) = \det \begin{pmatrix} 
            3 & 6 & 3\\
            0 & 1 & 3\\
            1 & 2 & 0
          \end{pmatrix}
        \] 
        \[
        \stackrel{C1}{=} 3 \cdot \det \begin{pmatrix} 
          1 & 3\\
          2 & 0
        \end{pmatrix}
        + 1 \cdot \det \begin{pmatrix} 
          6 & 3\\
          1 & 3
        \end{pmatrix}
        \] 
        \[
        = 3(-6) + 1(18-3) = -3 = \det(A)
        \] 
    \end{itemize}
  \end{example}

\subsection{Teorema}
Siano \( A \in M_{n \times n}(\mathbb{C}),\; 1 \le i,j \le n,\; 0 \neq \alpha \in \mathbb{C} \).
Allora:
\[
\det(EA) = \begin{cases}
  -\det(A) & \text{se } E = E_{ij}\\
  \alpha \det(A) & \text{se } E = E_i(\alpha)\\
  \det(A) & \text{se } E = E_{ij}(\alpha)
\end{cases}
\] 
\textbf{Dimostrazione} (\( n=2 \)):
\[
A = \begin{pmatrix} 
  a & b\\
  c & d
\end{pmatrix} 
\in  M_{2 \times 2}(\mathbb{C})
\] 
\[
\det(A) = ad-bc
\] 
\begin{itemize}
  \item 
    \[
      \det(E_{12}A) = \det \begin{pmatrix} 
        c & d\\
        a & b
      \end{pmatrix}
      = cb - ad = -\det(A)
    \] 
  \item 
    \[
      \det(E_{12}A) = \det(E_{21}A)
    \] 
    \item
      \[
        \det(E_1(\alpha)A) = \det \begin{pmatrix} 
          \alpha a & \alpha b\\
          c & d
        \end{pmatrix}
        = \alpha ad - \alpha bc = \alpha(ad-bc) = \alpha \det(A)
      \] 
      \item
        \[
          \det(E_2(\alpha)A) = \det \begin{pmatrix} 
            a & b\\
            \alpha c & \alpha d
          \end{pmatrix}
          = a(\alpha d) - b(\alpha c) = \alpha(ad-bc) = \alpha \det(A)
        \] 
      \item 
        \[
          \det(E_{21}(\alpha)A) = \det \begin{pmatrix} 
            a & b\\
            c+\alpha a & d+\alpha b
          \end{pmatrix} 
          = a(d+\alpha b) - b(c+\alpha a) = 
        \] 
        \[
          ad + \alpha ab - bc - \alpha ab = ad-bc = \det(A)
        \] 
      \item
        \[
          \det(E_{12}(\alpha)A) = \det \begin{pmatrix} 
            a+\alpha c & b+\alpha d\\
            c & d
          \end{pmatrix}
          = (a+\alpha c)d - (b+\alpha d)c = 
        \] 
        \[
          ad + \alpha cd - bc - \alpha cd =  ad-bc = \det(A)
        \] 
\end{itemize}

\begin{figure}[H]
  \begin{example}
    \[
    A = \begin{pmatrix}
      1 & 2 & 3\\
      0 & 1 & 3\\
      1 & 2 & 0
    \end{pmatrix} 
    \] 
    Troviamo la forma ridotta della matrice:
    \[
    \begin{pmatrix} 
      1 & 2 & 3\\
      0 & 1 & 3\\
      1 & 2 & 0
    \end{pmatrix} 
    \stackrel{E_{31}(-1)}{\leadsto}
    \begin{pmatrix} 
      1 & 2 & 3\\
      0 & 1 & 3\\
      0 & 0 & -3
    \end{pmatrix}
    \stackrel{E_3(-\frac{1}{3})}{\leadsto}
    \begin{pmatrix} 
      1 & 2 & 3\\
      0 & 1 & 3\\
      0 & 0 & 1
    \end{pmatrix} = U
    \] 
    \[
    \det(U) = 1
    \] 
    \[
      U = E_3(-\frac{1}{3})E_{31}(-1)A
    \] 
    \[
      A = E_{31}(-1)^{-1}E_3(-\frac{1}{3})^{-1}U = E_{31}(1)E_3(-3)U
    \] 
    \[
    \det(A) = \det(E_{31}(1)E_3(-3)U) = 
    \] 
    \[
    \det(E_3(-3)U) = -3\det(U) = -3
    \] 
  \end{example}
\end{figure}

\subsection{Corollario}
Se \( A \in M_{n \times n} \), allora \( \det(A) \neq 0 \) se e solo se \( A \) è
invertibile.

\vspace{1em}
\noindent \textbf{Dimostrazione}:

Sia \( U \) una forma ridotta di \( A \):
\[
  \det(A) \neq 0 \underset{4.3}{\Leftrightarrow} \det(U) \neq 0 \underset{4.10}{\Leftrightarrow} rk(U) = n
  \underset{4.3}{\Leftrightarrow} A \text{ è invertibile}
\] 
\[
\square
\] 
\subsection{Corollario}
Siano \( A,B \in M_{n \times n}(\mathbb{C}) \). Allora \( \det(AB) = \det(A) \det(B) \) 

\vspace{1em}
\noindent \textbf{Dimostrazione}:
\begin{itemize}
  \item \textbf{Caso 1}:

    \( A \) non è invertibile, ovvero \( \det(A) = 0 \). Se \( AB \) è invertibile, allora
    \[
      A(B(AB)^{-1}) = AB(AB)^{-1} = I_n \quad \text{e} \quad B(AB)^{-1}
    \] 
    sarebbe l'inversa di \( A \). Quindi \( AB \) \textbf{non} è invertibile.
    Allora \( \det(AB) = 0 = \det(A) \det(B) \) 

  \item \textbf{Caso 2}:

    \( A \) è invertibile. Per (4.1), esiste una sequenza \( E_1, \ldots, E_t \) di
    matrici elementari tali che:
    \[
    E_t \ldots E_1 A = I_n
    \] 
    Siccome \( E_1, \ldots, E_t \) sono invertibili, possiamo considerare:
    \[
    A = (E_1^{-1} \ldots E_t^{-1})E_t \ldots E_1 A = E_1^{-1} \ldots E_t^{-1}
    I_n = E_1^{-1} \ldots E_t^{-1}
    \] 
    Dunque:
    \[
    \det(AB) = \det(E_1^{-1} \ldots E_t^{-1}B)
    \] 
    \[
      \stackrel{teo}{=} \det(E_1^{-1}) \ldots \det(E_t^{-1}) \det(B)
    \] 
    \[
      \stackrel{teo}{=} \det(E_1^{-1} \ldots \det(E_t^{-1}) \det(B)
    \] 
    \[
    = \det(A) \det(B)
    \] 
    \[
    \square
    \] 
\end{itemize}

\subsection{Formula per \texorpdfstring{\( A^{-1} \)}{A inversa}}
Se \( \det(A) \neq 0 \) allora:
\[
  A^{-1} = \frac{1}{\det(A)} A^*
\] 
dove \( A^* \) è la matrice i cui coefficienti sono i complementi algebrici di \( A^T \)
e \( \det(A^{-1}) = \frac{1}{\det(A)} \) 

\begin{figure}[H]
  \begin{example}
    \[
    A = \begin{pmatrix} 
      1 & 2 & 3\\
      0 & 1 & 3\\
      1 & 2 & 0
    \end{pmatrix} 
    \quad
    A^T = \begin{pmatrix} 
      1 & 0 & 1\\
      2 & 1 & 2\\
      3 & 3 & 0
    \end{pmatrix} 
    \] 
    \[
    A^* = \begin{pmatrix} 
      \det\begin{pmatrix} 
        \cancel{1} & \cancel{0} & \cancel{1} \\
        \cancel{2} & 1 & 2\\
        \cancel{3} & 3 & 0
      \end{pmatrix}  & -\det\begin{pmatrix} 
      \cancel{1} & \cancel{0} & \cancel{1}\\
      2 & \cancel{1} & 2\\
      3 & \cancel{3} & 0
    \end{pmatrix} & \det\begin{pmatrix} 
    \cancel{1} & \cancel{0} & \cancel{1}\\
    2 & 1 & \cancel{2} \\
    3 & 3 & \cancel{0}
    \end{pmatrix}\\
      -\det \begin{pmatrix} 
        \cancel{1} & 0 & 1 \\
        \cancel{2} & \cancel{1} & \cancel{2}\\
        \cancel{3} & 3 & 0
      \end{pmatrix} & 
      \det \begin{pmatrix} 
        1 & \cancel{0} & 1 \\
        \cancel{2} & \cancel{1} & \cancel{2}\\
        3 & \cancel{3} & 0
      \end{pmatrix} &
      -\det \begin{pmatrix} 
        1 & 0 & \cancel{1} \\
        \cancel{2} & \cancel{1} & \cancel{2}\\
        3 & 3 & \cancel{0}
      \end{pmatrix}\\
      \det\begin{pmatrix} 
        \cancel{1} & 0 & 1 \\
        \cancel{2} & 1 & 2\\
        \cancel{3} & \cancel{3} & \cancel{0}
      \end{pmatrix} &
      -\det\begin{pmatrix} 
        1 & \cancel{0} & 1 \\
        2 & \cancel{1} & 2\\
        \cancel{3} & \cancel{3} & \cancel{0}
      \end{pmatrix} &
      \det\begin{pmatrix} 
        1 & 0 & \cancel{1} \\
        2 & 1 & \cancel{2}\\
        \cancel{3} & \cancel{3} & \cancel{0}
        \end{pmatrix}\\
    \end{pmatrix} 
    \] 
    \[
    = \begin{pmatrix} 
      -6 & 6 & 3\\
      3 & -3 & -3\\
      -1 & 0 & 1
    \end{pmatrix}
    \] 
    \[
      A^{-1} = \frac{1}{-3} \begin{pmatrix} 
        -6 & 6 & 3\\
        3 & -3 & -3\\
        -1 & 0 & 1
      \end{pmatrix}
      =
      \begin{pmatrix} 
        2 & -2 & -1\\
        -1 & 1 & 1\\
        \frac{1}{3} & 0 & -\frac{1}{3}
      \end{pmatrix}
    \] 
    \[
    \det(A^{-1}) = -\frac{1}{3}
    \] 
  \end{example}
\end{figure}

\subsection{Teorema di Cramer}
Sia \( A \in M_{n \times n}(\mathbb{C}) \) con \( \det(A) \neq 0 \), sia \( b \in M_{n \times n}(\mathbb{C}) \).
Allora il sistema lineare \( Ax = b \) possiede l'unica soluzione \( p = \begin{pmatrix} 
  p_1\\
  \vdots\\
  p_n
  \end{pmatrix}  \) dove \[ p_i = \frac{\det(A_i)}{\det(A)} \] e \( A_i \) è la matrice
  ottenuta da \( A \) sostituendo la colonna \( i \) con il vettore \( b \).

  \begin{figure}[H]
    \begin{example}
      \[
      A = \begin{pmatrix} 
        1 & 2 & 3\\
        0 & 1 & 3\\
        1 & 2 & 0
      \end{pmatrix}
      \] 
      \[
      \det(A) = -3 \quad b = \begin{pmatrix} 
        1\\
        0\\
        0
      \end{pmatrix}
      \] 
      \[
        A_1=
      \begin{pNiceArray}{>{\strut}ccc}[margin,extra-margin = 1pt]
        +1 & 2 & 3\\
        0 & 1 & 3\\
        0 & 2 & 0
        \CodeAfter
        \begin{tikzpicture}
          \node [fill=green, opacity=0.3, rounded corners=2pt, inner ysep = 0pt,
          rotate fit=0, fit = (1-1) (3-1) ] {} ;
        \end{tikzpicture}
      \end{pNiceArray}
      \quad
      \det(A_1) = 1 \cdot \det\begin{pmatrix} 
        1 & 3\\
        2 & 0
      \end{pmatrix}
      = -6
      \] 

      \[
        A_2=
      \begin{pNiceArray}{>{\strut}ccc}[margin,extra-margin = 1pt]
        +1 & -1 & 3\\
        0 & 0 & 3\\
        1 & 0 & 0
        \CodeAfter
        \begin{tikzpicture}
          \node [fill=green, opacity=0.3, rounded corners=2pt, inner ysep = 0pt,
          rotate fit=0, fit = (1-2) (3-2) ] {} ;
        \end{tikzpicture}
      \end{pNiceArray}
      \quad
      \det(A_2) = -1 \cdot \det\begin{pmatrix} 
        0 & 3\\
        1 & 0
      \end{pmatrix} = 3
      \] 

      \[
        A_3=
      \begin{pNiceArray}{>{\strut}ccc}[margin,extra-margin = 1pt]
        1 & 2 & 1\\
        0 & 1 & 0\\
        1 & 2 & 0
        \CodeAfter
        \begin{tikzpicture}
          \node [fill=green, opacity=0.3, rounded corners=2pt, inner ysep = 0pt,
          rotate fit=0, fit = (1-3) (3-3) ] {} ;
        \end{tikzpicture}
      \end{pNiceArray}
      \quad
      \det(A_3) = 2 \cdot \det\begin{pmatrix} 
        0 & 1\\
        1 & 2
      \end{pmatrix} = -1
      \]
      \[
      p_1 = \frac{\det(A_1)}{\det(A)} = \frac{-6}{-3} = 2
      \] 
      \[
      p_2 = \frac{\det(A_2)}{\det(A)} = \frac{3}{-3} = -1
      \] 
      \[
      p_3 = \frac{\det(A_3)}{\det(A)} = \frac{-1}{-3} = \frac{1}{3}
      \] 
      Dunque \( p = \begin{pmatrix} 2\\-1\\\frac{1}{3} \end{pmatrix}  \) è l'unica
      soluzione del sistema lineare \( Ax = b \) 
    \end{example}
  \end{figure}

  \section{Spazi vettoriali e sottospazi}
  \begin{figure}[H]
    \begin{example}
      Prendiamo in considerazione il piano cartesiano: \( \mathbb{R}^2 \) 
      \begin{center}
        \begin{tikzpicture}
          \draw[->] (-3,0) -- (3,0) node[right] {\( x \)};
          \draw[->] (0,-3) -- (0,3) node[above] {\( y \)};

          \draw[fill=red] (1,2) circle (0.05) node[above right] {\( (a,b) \)};
          
          \draw[dashed] (1,2) -- (1,0) node[below] {\( a \)};
          \draw[dashed] (1,2) -- (0,2) node[left] {\( b \)};

        \end{tikzpicture}
      \end{center}

      \noindent Ogni punto nel piano cartesiano può essere rappresentato con una coppia
      di valori \( (a,b) \). Possiamo identificare \( \mathbb{R}^2 \) con l'insieme
      \[ M_{2 \times 1}(\mathbb{R}) = \left\{ \begin{pmatrix} a\\b \end{pmatrix} \Big|\; a,b \in \mathbb{R} \right\} \]
      Possiamo:
      \begin{itemize}
        \item Sommare i vettori:
          \[
          \begin{pmatrix} 
            a\\b
          \end{pmatrix} 
          +
          \begin{pmatrix} 
            a'\\b'
          \end{pmatrix} 
          =
          \begin{pmatrix} 
            a+a'\\b+b'
          \end{pmatrix}
          \] 
        \item Moltiplicare per uno scalare \( \alpha \in \mathbb{R} \) 
          \[
          \alpha \begin{pmatrix} 
            a\\b
          \end{pmatrix}
          =
          \begin{pmatrix} 
            \alpha a\\ \alpha b
          \end{pmatrix}
          \] 
      \end{itemize}
    \end{example}
  \end{figure}

  \subsection{Definizione di spazio vettoriale}
  \begin{definition}
    Sia \( \mathbb{K} = \mathbb{R} \) oppure \( \mathbb{K} = \mathbb{C} \). Uno \textbf{
      spazio vettoriale} su \( \mathbb{K} \) è un insieme non vuoto \( V \) i cui
      elementi sono detti \textbf{vettori} sul quale sono definite due operazioni:
      \begin{enumerate}
        \item \textbf{Addizione}: per \( v,w \in V \) abbiamo:
          \[
          v+w \in V
          \] 
        \item \textbf{Moltiplicazione per uno scalare}:
          per \( \alpha \in \mathbb{K},\; v \in V \) abbiamo:
          \[
          \alpha v \in V
          \] 
      \end{enumerate}
      che godono delle seguenti proprietà:
      \begin{enumerate}
        \item Valgono le proprietà:
          \begin{enumerate}
            \item \textbf{Associatività}: \[ (v+u)+w = v+(u+w) \] per ogni \( v,u,w \in V \)
            \item \textbf{Elemento neutro}: esiste \( 0_v \in V \) tale che:
              \[
              v + 0_v = v = 0_v + v
              \] 
              per ogni \( v \in V \)
            \item \textbf{Elemento inverso}: per ogni \( v \in V \) esiste \( w \in V \) tale che:
              \[
              v + w = 0_v = w + v
              \] 
              Scriviamo \( w = -v \)
            \item \textbf{Commutatività}: \[ v+w = w+v \] per ogni \( v,w \in V \)

          \end{enumerate}
        \item Per ogni \( v \in V \):
          \[
          1 \cdot v = v
          \] 
        \item Per ogni \( v \in V \) e \( a,b \in \mathbb{K} \)   \[
        (\alpha \beta)v = \alpha(\beta v)
        \] 
        \item Per ogni \( v,w \in V \) e \( \alpha \in \mathbb{K} \) valgono le seguenti
          \textbf{leggi distributive}:
          \[
          \alpha(v+w) = \alpha v + \alpha w
          \] 
          \[
          (\alpha + \beta)v = \alpha v + \beta v
          \] 
      \end{enumerate}
  \end{definition}

  \subsubsection{Esempi}
  \begin{enumerate}
    \item 
      \( V = M_{m \times n}(\mathbb{K}) \) è uno spazio vettoriale su \( \mathbb{K} \) con
      addizione di matrici e moltiplicazione per scalari usuale.
      \[
        0_v = \begin{pmatrix} 
          0 & \ldots & 0\\
          \vdots & \ddots & \vdots\\
          0 & \ldots & 0
        \end{pmatrix}
      \] 
      In particolare scriviamo:
      \[
        \mathbb{K}^m := M_{m \times 1}(\mathbb{K})
      \] 
      \[
        0_{\mathbb{K}^m}
        = \begin{pmatrix}
          0\\
          \vdots\\
          0
        \end{pmatrix}
        = \mathbb{O}
      \] 
    \item \( \mathbb{K}[x] \) l'insieme dei polinomi a coefficienti in \( \mathbb{K} \).
      \[
      f = a_0 + a_1x + a_2x^2 + \ldots + a_nx^n
      \] 
      \[
      g = b_0 + b_1x + b_2x^2 + \ldots + b_nx^n
      \] 
      \vspace{1em}
      \[
      f+g = (a_0+b_0) + (a_1+b_1)x + \ldots + (a_n+b_n)x^n
      \] 
      \[
      \alpha f = (\alpha a_0) + (\alpha a_1)x + \ldots + (\alpha a_n)x^n
      \] 
      \begin{itemize}
        \item 
          \( \mathbb{K}[x] \) è uno spazio vettoriale. L'elemento neutro è:
          \[
            0_{\mathbb{K}[x]} = 0 + 0x + 0x^2 + \ldots + 0x^n
          \] 
        \item 
          \( \mathbb{K}[x] \) è l'insieme di polinomi di grado \( \le n \) a coefficienti
          in \( \mathbb{K} \). È uno spazio vettoriale
      \end{itemize}
    \item Le successioni sono delle liste di numeri \( (a_n)_{n \in \mathbb{N}} \in \mathbb{K} \). Ad
      esempio:
      \[
        (1,-1,2,3,6,i, \ldots) \in \mathbb{C}
      \] 
      formano uno spazio vettoriale \( \mathcal{S} \) su \( \mathbb{K} \):
      \[
        (a_n)_{n \in \mathbb{N}} + (b_n)_{n \in \mathbb{N}} = (a_n+b_n)_{n \in \mathbb{N}}
      \] 
      Ad esempio:
      \[
        (1,-1,2,3,6,i, \ldots) + (1,0,1,0,1,0, \ldots) = (2,-1,3,3,7,i, \ldots)
      \] 
      La molitplicazione per uno scalare è:
      \[
        \alpha(a_n)_{n \in \mathbb{N}} = (\alpha a_n)_{n \in \mathbb{N}}
      \] 
      Ad esempio:
      \[
        2(1,-1,2,3,6,i, \ldots) = (2,-2,4,6,12,2i, \ldots)
      \] 
      L'insieme di successioni che soddisfano la relazione:
      \[
        a_{k+2} -5a_{k+1} + 3a_k = 0 \quad \forall k \in \mathbb{N}
      \] 
      Ad esempio:
      \[
        (1,0,-3,-15,-66, \ldots)
      \] 
      è uno spazio vettoriale. L'elemento neutro è:
      \[
        0_{\mathcal{S}} = (0,0,0,0,0, \ldots)
      \] 
    \item L'insieme di funzioni \( f: \mathbb{R} \to \mathbb{R} \) è uno spazio vettoriale:
      \[
      f,g \in \mathbb{R}^\mathbb{R}
      \] 
      \[
      f+g: \mathbb{R} \to \mathbb{R}
      \] 
      \[
        (f+g)(x) = f(x) + g(x)
      \] 

      \[
      \alpha \in \mathbb{R},
      \] 
      \[
      \alpha f: \mathbb{R} \to \mathbb{R}
      \] 
      \[
      (\alpha f)(x) = \alpha f(x)
      \] 

      \( 0_{\mathbb{R}^\mathbb{R}} \) è la funzione: \( 0_{\mathbb{R}^\mathbb{R}}(x) =0 \) 

    \item \( V = \{0_v\}  \) è uno spazio vettoriale. Scriviamo \( V = \{0\} \).
  \end{enumerate}

  \subsection{Osservazioni} \label{oss:5.2}
  Sia \( V \) uno spazio vettoriale. Sia \( v \in V,\; \alpha \in \mathbb{K} \) 
  \begin{enumerate}
    \item[a.] \( \alpha 0_v = 0_v \), infatti:
      \[
      \alpha 0_v = \alpha (0_v + 0_v) = \alpha 0_v + \alpha 0_v
      \] 
      Sommando \( -\alpha 0_v \) ad entrambi i membri otteniamo:
      \[
      \alpha 0_v + (-\alpha 0_v) = (\alpha 0_v + \alpha 0_v) + (-\alpha 0_v)
      \] 
      \[
      0_v = \alpha 0_v + (\alpha 0_v - \alpha 0_v)
      \] 
      \[
      0_v  = \alpha 0_v + 0_v
      \] 
      \[
      0_v = \alpha 0_v
      \] 
    \item[b.] \( 0 \cdot v = 0_v \) 
      \[
      v = 1 \cdot v = (1+0)v = 1 \cdot v + 0 \cdot v = v + 0 \cdot v
      \] 
      Sommando \( -v \) ad entrambi i membri otteniamo:
      \[
      v + (-v) = v + 0 \cdot v + (-v)
      \] 
      \[
      0_v = 0 \cdot v + (v + (-v))
      \] 
      \[
      0_v = 0 \cdot v + 0_v
      \] 
      \[
      0_v = 0 \cdot v
      \] 

    \item[c.] Se \( \alpha v = 0_v \), allora \( \alpha = 0 \) oppure \( v = 0_v \) 
      \[
      \alpha v = 0_v
      \] 
      \[
      \alpha v = \alpha 0_v
      \] 
      \[
      \alpha v = \alpha (v - v)
      \] 
      \[
      \alpha v = \alpha v - \alpha v
      \] 
      \[
      0_v = 0_v
      \]

    \item[d.] \( (-\alpha)v = -(\alpha v) = \alpha (-v) \) 
  \end{enumerate}

  \subsection{Definizione combinazione lineare}
  Sia \( V \) uno spazio vettoriale su \( \mathbb{K} \) e siano \( v_1, \ldots, v_n \in V \),
  \( \alpha_1, \ldots, \alpha_n \in \mathbb{K} \). Il vettore:
  \[
  v = \alpha_1 v_1 + \alpha_2 v_2 + \ldots + \alpha_n v_n
  \] 
  è detto \textbf{combinazione lineare} di \( v_1, \ldots, v_n \) con coefficienti
  \( \alpha_1, \ldots, \alpha_n \).

  \begin{figure}[H]
    \begin{example}
      Il vettore \[ \begin{pmatrix} 1\\2\\3 \end{pmatrix} \in \mathbb{C}^3 \] 
      è combinazione lineare di:
      \[
      e_1 = \begin{pmatrix} 1\\0\\0 \end{pmatrix},\; e_2 = \begin{pmatrix} 0\\1\\0 \end{pmatrix},\; e_3 = \begin{pmatrix} 0\\0\\1 \end{pmatrix}
      \] 
      con coefficienti \( 1,2,3 \) rispettivamente. Infatti:
      \[
      \begin{pmatrix} 1\\2\\3 \end{pmatrix} = 1 \begin{pmatrix} 1\\0\\0 \end{pmatrix} + 2 \begin{pmatrix} 0\\1\\0 \end{pmatrix} + 3 \begin{pmatrix} 0\\0\\1 \end{pmatrix}
      \] 
      Un'altra combinazione lineare è:
      \[
      \begin{pmatrix} 1\\2\\3 \end{pmatrix} = 2 \begin{pmatrix} 1\\i\\0 \end{pmatrix} - i \begin{pmatrix} 0\\1\\0 \end{pmatrix} + 3 \begin{pmatrix} 0\\0\\1 \end{pmatrix}
      \] 
    \end{example}
  \end{figure}
  \begin{figure}[H]
    \begin{example}
      Il polinomio
      \[
        f = 2x^2 + 4x + 3 \in \mathbb{R}[x]
      \] 
      è combinazione lineare di:
      \[
      g_1 = x^2 + 2x, \quad g_2 = x-1, \quad g_3 = \frac{1}{2}x - 1
      \] 
      Infatti:
      \[
      2g_1+3g_2+(-6)g_3 = 2(x^2+2x) + 3(x-1) - 6\left( \frac{1}{2}x-1 \right) = 2x^2 + 4x + 3 = f
      \] 
    \end{example}
  \end{figure}

  \subsection{Definizione di insieme di generatori}
  Sia \( V \) uno spazio vettoriale e siano \( v_1, \ldots, v_n \in V \). Se ogni \( v \in V \) 
  è combinazione lineare di \( v_1, \ldots, v_n \) si dice che \( \{v_1, \ldots, v_n\}  \) 
  è un \textbf{insieme di generatori} e \( V \) è detto \textbf{finitamente generato}.

  \subsubsection{Esempi}
  \begin{figure}[H]
    \begin{example}
      \[
      \left\{e_1=\begin{pmatrix} 
          1\\0\\0
      \end{pmatrix},\;
      e_2=\begin{pmatrix} 
        0\\1\\0
      \end{pmatrix},\;
      e_3=\begin{pmatrix} 
        0\\0\\1
      \end{pmatrix} \right\}
      \] 
      è un insieme di generatori di \( \mathbb{K}^3 = M_{3 \times 1}(\mathbb{K}) \) 
      (per \( \mathbb{K} = \mathbb{R} \) e \( \mathbb{K} = \mathbb{C} \) ).

      \noindent Infatti, se \( v = \begin{pmatrix} v_1\\v_2\\v_3 \end{pmatrix} \in \mathbb{K}^3 \), allora:
      \[
      v = v_1 \begin{pmatrix} 1\\0\\0 \end{pmatrix} + v_2 \begin{pmatrix} 0\\1\\0 \end{pmatrix} + v_3 \begin{pmatrix} 0\\0\\1 \end{pmatrix}
      \] 
      Scrivendo:
      \[
      e_i = \begin{pmatrix} 
        0\\
        \vdots\\
        0\\
        1\\
        0\\
        \vdots\\
        0
      \end{pmatrix} 
      \] 
      con \( 1 \) nella \( i \)-esima posizione, otteniamo l'insime di generatori di
      \( \mathbb{K}^n \): \( \{e_1, \ldots, e_n\}  \)  Dunque \( \mathbb{K}^n \) è finitamente generato.
    \end{example}
  \end{figure}
  \begin{figure}[H]
    \begin{example}
      \[
        \left\{ \begin{pmatrix} 1\\3 \end{pmatrix},\;
          \begin{pmatrix} 1\\1 \end{pmatrix},\;
          \begin{pmatrix} 0\\1 \end{pmatrix} 
        \right\} 
      \] 
      è un insieme di generatori di \( \mathbb{R}^2 \).
      Infatti, se \( v = \begin{pmatrix} v_1\\v_2 \end{pmatrix} \in \mathbb{R}^2 \) allora
      \[
      v = (v_1,-v_2)\begin{pmatrix} 1\\3 \end{pmatrix} + v_2 \begin{pmatrix} 1\\1 \end{pmatrix} + 3(v_2-v_1)
      \begin{pmatrix} 0\\1 \end{pmatrix}
      \] 
      \[
      = \begin{pmatrix} 
        (v_1-v_2) + v_2\\
        3(v_1-v_2) + v_2 + 3(v_2-v_1)
      \end{pmatrix} 
      \] 
      I coefficienti della combinazione lineare non sono univocamente determinati:
      \[
      \begin{pmatrix} 2\\3 \end{pmatrix}  = -1 \begin{pmatrix} 1\\3 \end{pmatrix} 
      + 3 \begin{pmatrix} 1\\1 \end{pmatrix} + 3 \begin{pmatrix} 0\\1 \end{pmatrix} 
      \] 
      \[
      = 0 \begin{pmatrix} 1\\3 \end{pmatrix} + 2 \begin{pmatrix} 1\\1 \end{pmatrix} + 1 \begin{pmatrix} 0\\1 \end{pmatrix}
      \] 
    \end{example}
  \end{figure}
  \begin{figure}[H]
    \begin{example}
      Le successioni:
      \[
      u_1 = (1,0,-3,-15,-66, \ldots)
      \] 
      \[
      u_2 = (0,1,5,22,95, \ldots)
      \] 
      formano un insieme di generatori di \( \mathcal{S}' \). Infatti, se:
      \[
        (a_n)_{n \in \mathbb{N}} = (a,b,5b-3a,5(5b-3a)-3b, \ldots) \in \mathcal{S}'
      \] 
      allora
      \[
        (a_n)_{n \in \mathbb{N}} = au_1 + bu_2
      \] 
    \end{example}
  \end{figure}
  \begin{figure}[H]
    \begin{example}
      Gli spazi vettoriali \( \mathcal{S},\; \mathbb{K}[X],\; \mathbb{R}^\mathbb{R} \) 
      (successioni, polinomi, funzioni) \textbf{non} sono finitamente generati.
    \end{example}
  \end{figure}

  \subsection{Definizione di sottospazio}
  Sia \( V \) uno spazio vettoriale su \( \mathbb{K} \). Un sottoinsieme \( \emptyset \neq U \subseteq V \) 
  è detto \textbf{sottospazio} di \( V \) se soddisfa le proprietà:
  \begin{enumerate}
    \item per ogni \( u,u' \in U \):
      \[
      u + u' \in U
      \] 
    \item per ogni \( u \in U \), \( \alpha \in \mathbb{K} \):
      \[
        \alpha u \in U
      \] 
  \end{enumerate}
  \noindent \textbf{Osservazione:}
  
  \noindent In tal caso \( U \) è uno spazio vettoriale rispetto alle stesse operazioni
  \( +,\; \cdot  \) di \( V \).

  \subsubsection{Esempi}
  \begin{figure}[H]
    \begin{example}
      \[
        \mathbb{K}_n[x] \subseteq \mathbb{K}[x] \quad \text{sottospazi}
      \] 
      \[
        \mathcal{S'} \subseteq \mathcal{S} \quad \text{sottospazi}
      \] 
    \end{example}
  \end{figure}

  \begin{example}
    Il sottoinsieme
    \[
    u = \left\{ \begin{pmatrix} v_1\\v_2 \end{pmatrix} \in \mathbb{R}^2 \Big | v_2 = mv_1 \right\} 
    \] 
    è un sottospazio di \( \mathbb{R}^2 \) per qualsiasi \( m \in \mathbb{R} \).
    Infatti:
    \begin{enumerate}
      \item \[
      \begin{pmatrix} v\\mv \end{pmatrix}
      +
      \begin{pmatrix} u\\mu \end{pmatrix} =
      \begin{pmatrix} v+u\\mv+mu \end{pmatrix}
      \] 
      \item
        \[
        \alpha \begin{pmatrix} v\\mv \end{pmatrix} =
        \begin{pmatrix} \alpha v\\\alpha mv \end{pmatrix} =
        \begin{pmatrix} \alpha v\\m(\alpha v) \end{pmatrix} \in U
        \] 
      \begin{center}
        \begin{tikzpicture}[scale=0.7]
          \draw[->] (-3,0) -- (3,0) node[right] {\( x \)};
          \draw[->] (0,-3) -- (0,3) node[above] {\( y \)};

          % draw line passing through origin
          \draw[red, thick] (-2.5,-2.5) -- (2.5,2.5) node[right] {\( U \)};
        \end{tikzpicture}
      \end{center}
    \end{enumerate}
    Il sottoinsieme \( \mathcal{L} = \left\{ \begin{pmatrix} v_1\\v_2 \end{pmatrix} \in \mathbb{R} \Big | v_2 = mv_1 + c \right\}  \) non
    è un sottospazio se \( c \neq 0 \).
    \begin{center}
      \begin{tikzpicture}[scale=0.7]
        \draw[->] (-3,0) -- (3,0) node[right] {\( x \)};
        \draw[->] (0,-3) -- (0,3) node[above] {\( y \)};

        % draw line passing through origin
        \draw[red, thick] (-2.5,-2.0) -- (2.5,3) node[right] {\( \mathcal{L} \)};
      \end{tikzpicture}
    \end{center}
    Infatti:
    \[
    \begin{pmatrix} v\\mv+c \end{pmatrix} +
    \begin{pmatrix} u\\mu+c\end{pmatrix} 
    =
    \begin{pmatrix} v+u\\mv+mu+2c \end{pmatrix} \notin U
    \] 
    \[
    mv+mu+2c \neq m(v+u)+c
    \] 
  \end{example}

  \begin{figure}[H]
    \begin{example}
      \( O = \{0_v\} \subseteq V \) è un sottospazio per ogni spazio vettoriale \( V \).
      Infatti 
      \begin{enumerate}
        \item \( 0_v + 0_v = 0_v \in O \) 
        \item \( \alpha 0_v = 0_v \in O \) 
      \end{enumerate}
      Ogni sottospazio \( U \) di \( V \) contiene \( 0_v \). infatti \( \forall u \in U \)
      abbiamo che \( (-1)u = -u \in U \). Quindi \( 0_v = u + (-u) \in U \) 
    \end{example}
  \end{figure}
  \begin{figure}[H]
    \begin{example}
      Il sottoinsieme 
      \[
        T = \left\{ \begin{pmatrix} a & b\\
            0 & c
        \end{pmatrix} \in M_{2 \times 2}(\mathbb{C}) \Big | \;
        a,b,c \in \mathbb{C} \right\}
      \] 
      è un sottospazio di \( M_{2 \times 2}(\mathbb{C}) \). Infatti
      \begin{enumerate}
        \item \[
        \begin{pmatrix} 
          a & b\\
          0 & c
        \end{pmatrix} 
        +
        \begin{pmatrix} 
          d & e\\
          0 & f
        \end{pmatrix} 
        =
        \begin{pmatrix} 
          a+d & b+e\\
          0 & c+f
        \end{pmatrix} \in T
        \] 
        \item
          \[
          \alpha \begin{pmatrix} 
            a & b\\
            0 & c
          \end{pmatrix}
          =
          \begin{pmatrix} 
            \alpha a & \alpha b\\
            0 & \alpha c
          \end{pmatrix} \in T
          \] 
      \end{enumerate}
    \end{example}
  \end{figure}

  \begin{figure}[H]
    \begin{example}
      \[
      \overbrace{\mathbb{K}_n[x]}^{\text{polinomi di grado \( \le n \) }}
      \subseteq
      \overbrace{\mathbb{K}[x]}^{\text{polinomi}}
      \]  
      \[
        \underbrace{\mathcal{S'}}_{\text{successioni che soddisfano una relazione}}
        \subseteq
        \underbrace{\mathcal{S}}_{\text{successioni}}
      \] 
      sono sottospazi
    \end{example}
  \end{figure}

\subsection{Definizione di sottospazio generato}
\begin{definition}
  Dati \( v_1, \ldots, v_n  \in V\), l'insieme
  \[ \left< v_1, \ldots, v_n \right> = \left\{ \alpha_1 v_1 + \ldots + \alpha_n v_n | \alpha_1, \ldots, \alpha_n \in \mathbb{K} \right\}  \]
  di tutte le combinazioni lineari di \( v_1, \ldots, v_n \) è un sottospazio di \( V \).
  Infatti:
  \begin{enumerate}
    \item \[
        \left( \sum_{i=1}^n \alpha_i v_i \right) 
        +
        \left( \sum_{i=1}^n \beta_i v_i \right) 
      \] 
      \[
        = \alpha_1 v_1 + \ldots + \alpha_n v_n + \beta_1 v_1 + \ldots + \beta_n v_n
      \] 
      \[
        = (\alpha_1 v_1 + \beta_1 v_1) + \ldots + (\alpha_n v_n + \beta_n v_n)
      \] 
      \[
        = (\alpha_1 + \beta_1)v_1 + \ldots + (\alpha_n + \beta_n)v_n
      \] 
      \[
        = \sum_{i=1}^n (\alpha_i + \beta_i)v_i \quad \in \left< v_1, \ldots, v_n \right>
      \] 
    \item
      \[
        \beta \left( \sum_{i=1}^n \alpha_i v_i \right) = \sum_{i=1}^n (\beta \alpha_i)v_i \quad \in \left< v_1, \ldots, v_n \right>
      \] 
  \end{enumerate}
  Diciamo che \( \left< v_1, \ldots, v_n \right> \) è il sottospazio generato da \( v_1, \ldots, v_n \).
\end{definition}

\begin{figure}[H]
  \begin{example}
    \[
      V = \mathbb{R}^2 \quad \mathcal{L} \subseteq \mathbb{R}^2
    \] 
    \begin{center}
      \begin{tikzpicture}[scale=0.7]
        \draw[->] (-3,0) -- (3,0) node[right] {\( x \)};
        \draw[->] (0,-3) -- (0,3) node[above] {\( y \)};

        % draw line passing through origin
        \draw[red, thick] (-3,-3) -- (3,3) node[left, xshift=-3, yshift=3] (line) {\( \mathcal{L} \)};
        \node[red, below right, xshift=5, yshift=-5] at (line) {\( y = mx \)};
      \end{tikzpicture}
    \end{center}
    \noindent è il sottospazio generato da \(  \left\{ \begin{pmatrix} 1\\m \end{pmatrix}  \right\} \)
    \[
    \left< \begin{pmatrix} 1\\m \end{pmatrix}  \right> = \left\{ \alpha \begin{pmatrix} 1\\m \end{pmatrix} \Big | \; \alpha \in \mathbb{R}  \right\} 
    \] 
    \( \mathcal{S'} \) è il sottospazio di \( \mathcal{S} \) generato da \( u_1 \) e \( u_2 \).
  \end{example}
\end{figure}

\subsection{Definitione}
Se \( U,W \) sono sottospazi di \( V \), allora l'intersezione
\[
  U \cap W = \left\{ v \in V \;|\; v \in U \land v \in W \right\}
\] 
è un sottospazio di \( V \).

\noindent In generale, l'unione
\[
  U \cup W = \left\{ v \in V \;|\; v \in U \lor v \in W \right\}
\] 
non è un sottospazio di \( V \).

\begin{figure}[H]
  \begin{example}
    \[
      V = \mathbb{R}^2, \quad U = \left< \begin{pmatrix} 1\\0 \end{pmatrix}  \right> = \left\{ \alpha \begin{pmatrix} 1\\0 \end{pmatrix} \Big |\; \alpha \in \mathbb{R} \right\}
    \] 
    \[
    W = \left< \begin{pmatrix} 0\\1 \end{pmatrix}  \right> = \left\{  \alpha \begin{pmatrix} 0\\1 \end{pmatrix} \Big |\; \alpha \in \mathbb{R} \right\}
    \] 
      \begin{center}
        \begin{tikzpicture}
          \draw[->] (-3,0) -- (3,0) node[right] {\( x \)};
          \draw[->] (0,-3) -- (0,3) node[above] {\( y \)};

          \draw[fill=red] (1,2) circle (0.05) node[above right] {\( \begin{pmatrix} a\\b \end{pmatrix}  \)};
          
          \draw[dashed] (1,2) -- (1,0) node[below] {\( a \)};
          \draw[dashed] (1,2) -- (0,2) node[left] {\( b \)};

          \draw[red, thick] (-3,0) -- (3,0) node[below] {\( U \)};
          \draw[blue, thick] (0,-3) -- (0,3) node[left] {\( W \)};

        \end{tikzpicture}
      \end{center}
      \[
      \begin{pmatrix} a\\0 \end{pmatrix} 
      +
      \begin{pmatrix} 0\\b \end{pmatrix} 
      =
      \begin{pmatrix} a\\b \end{pmatrix} 
      \notin U \cup W
      \] 
      Quindi \( U \cup W \subseteq V \) non soddisfa la prima proprietà dei sottospazi.
  \end{example}
\end{figure}

\noindent L'insieme \( U + W = \left\{ u + w \; |\; u \in U,\; w \in W \right\}  \) è
un sottospazio di \( V \), detto la \textbf{somma di \( U \) e \( W \)}.

\vspace{1em}
\noindent \textbf{NB:}

\noindent \( U \cup U \subseteq U + W \) perchè
\[
U = \left\{ u = u + 0_v \;|\; u \in U \right\}
\] 
\[
W = \left\{ w = 0_v + w \;|\; w \in W \right\}
\] 
\subsection{Definizione}
Consideriamo lo spazio vettoriale \( \mathbb{K}^m \). Sia \( A = (a_{ij})_{1 \le i \le m,\; 1 \le j \le n} \in M_{m \times n}(\mathbb{K}) \) 
Il sottospazio di \( \mathbb{K}^m \) 
\[
  C(A) = \left< \begin{pmatrix} a_{11} \\ \vdots \\ a_{m1} \end{pmatrix}, \ldots, \begin{pmatrix} a_{1n} \\ \vdots \\ a_{mn} \end{pmatrix} \right>
\] 
generato dalle colonne di \( A \) è detto lo \textbf{spazio delle colonne} di \( A \).

\begin{figure}[H]
  \begin{example}
    \( \mathbb{K} = \mathbb{R} \) 
    \[
      A = \begin{pmatrix} 
        1 & 2 & 0\\
        0 & 6 & 3
      \end{pmatrix}  \in M_{2 \times 3}(\mathbb{R})
    \] 
    \[
    C(A) \subseteq \mathbb{R}^2
    \] 
    \[
      C(A) = \left\{ x_1 \begin{pmatrix} 1\\0 \end{pmatrix} 
        + x_2 \begin{pmatrix} 2\\6 \end{pmatrix}
        + x_3 \begin{pmatrix} 0\\3 \end{pmatrix} \Big |\; x_1,x_2,x_3 \in \mathbb{R} \right\}
    \] 
    Ad esempio:
    \[
    2 \begin{pmatrix} 1\\0 \end{pmatrix}  + (-1) \begin{pmatrix} 2\\6 \end{pmatrix} + 3 \begin{pmatrix} 0\\3 \end{pmatrix} = \begin{pmatrix} 0\\3 \end{pmatrix} \in C(A)
    \] 
    \textbf{NB:}
    \[
    x_1 \begin{pmatrix} 1\\0 \end{pmatrix} + x_2 \begin{pmatrix} 2\\6 \end{pmatrix} + x_3 \begin{pmatrix} 0\\3 \end{pmatrix} = \begin{pmatrix} x_1 + 2x_2\\6x_2 + 3x_3 \end{pmatrix}
    \] 
    \[
     = \begin{pmatrix} 
       1 & 2 & 0\\
       0 & 6 & 3
     \end{pmatrix} 
     \begin{pmatrix} 
       x_1\\
       x_2\\
       x_3
     \end{pmatrix} 
    \] 
    \[
      \begin{pmatrix} a\\b \end{pmatrix} \in C(A) \Leftrightarrow \exists x_1,x_2,x_3 \in \mathbb{R}
    \] 
    tali che
    \[
    \begin{pmatrix} a\\b \end{pmatrix} = \begin{pmatrix}
    1 & 2 & 0\\
    0 & 6 & 3
  \end{pmatrix} 
  \begin{pmatrix} 
    x_1\\
    x_2\\
    x_3
  \end{pmatrix} 
    \] 
    \[
      \Leftrightarrow
    \] 
    Il sistema lineare \( Ax = \begin{pmatrix} a\\b \end{pmatrix}  \) possiede soluzione

  \end{example}
\end{figure}

\subsection{Proposizione}
Sia \( A = (a_{ij})_{1 \le i \le m,\; 1 \le j \le n} \in M_{m \times n}(\mathbb{K}) \) 
Lo spazio delle colonne \( C(A) \) consiste di tutti i vettori \( b \in \mathbb{K}^m \)
per i quali il sistema lineare \( Ax = b \) possiede soluzione.

\subsubsection{Dimostrazione}
\[
  C(A) \stackrel{def}{=} \left\{ \begin{pmatrix} 
      b_1\\
      \vdots\\
      b_m
  \end{pmatrix}
  =
  v_1 \begin{pmatrix} 
    a_{11}\\
    \vdots\\
    a_{m1}
  \end{pmatrix} 
  +
  \ldots
  +
  v_n \begin{pmatrix} 
    a_{1n}\\
    \vdots\\
    a_{mn}
  \end{pmatrix}
  \Big |\; v_1, \ldots, v_n \in \mathbb{K} 
\right\}
\] 
\[
 = \left\{
   b \in \mathbb{K}^m \Big |\; \exists v = \begin{pmatrix}
     v_1\\
     \vdots\\
     v_n
   \end{pmatrix} 
   \; \text{tale che }
   Av = b
\right\}
\] 
\[
\square
\] 

\subsection{Definizione}
Sia \( A \in M_{m \times n}(\mathbb{K}) \) l'insieme 
\[
  N(A) = \left\{ v \in \mathbb{K}^n \Big | \; Av = \mathbb{O} \right\} \subseteq \mathbb{K}^n 
\] 
(dove \( \mathbb{O} = \begin{pmatrix} 0\\ \vdots\\ 0 \end{pmatrix}  \) ) è detto
\textbf{spazio nullo di \( A \) }.

\subsection{Proposizione}
Lo spazio nullo \( N(A) \) di una matrice \( A \in M_{m \times n}(\mathbb{K}) \) è un
sottospazio di \( \mathbb{K}^n \).

\subsubsection{Dimostrazione}
Siano \( v,u \in N(A) \), cioè \( Av = \mathbb{O} \) e \( Au = \mathbb{O} \) e sia
\( \alpha \in \mathbb{K} \). Allora
\begin{itemize}
  \item Per la legge distributiva del prodotto di matrici:
  \[
  A(v+u) = Av + Au = \mathbb{O} + \mathbb{O} = \mathbb{O}
  \] 
  Quindi \( v+u \in N(A) \)
\item Per la proprietà di molitplicazione per uno scalare:
  \[
  A(\alpha v) = \alpha Av = \alpha \mathbb{O} = \mathbb{O}
  \] 
  Quindi \( \alpha v \in N(A) \)
\end{itemize}
Dunque \( N(A) \) è un sottospazio. \( \quad \square \) 

\begin{figure}[H]
  \begin{example}
    \[
    \mathbb{K} = \mathbb{C}, \quad A = \begin{pmatrix} 
      i & 0 \\
      0 & 1 \\
      i & -1
    \end{pmatrix} 
    , \quad
    N(A) \subseteq \mathbb{C}^2
    \] 
    \[
    N(A) = \{\text{Soluzioni del sistema lineare \( Ax=0 \) }\} 
    \] 
    Risolviamo il sistema lineare:
    \[
      \begin{pmatrix}[cc|c]
        i & 0 & 0\\
        0 & 1 & 0\\
        i & -1 & 0
    \end{pmatrix}
    \stackrel{E_1(-i)}{\leadsto}
    \begin{pmatrix}[cc|c]
      1 & 0 & 0\\
      0 & 1 & 0\\
      i & -1 & 0
    \end{pmatrix} 
    \] 
    \[
      \stackrel{E_{31}(-i)}{\leadsto}
      \begin{pmatrix}[cc|c]
        1 & 0 & 0\\
        0 & 1 & 0\\
        0 & -1 & 0
      \end{pmatrix} 
      \stackrel{E_{23}(1)}{\leadsto}
      \begin{pmatrix}[cc|c]
        1 & 0 & 0\\
        0 & 1 & 0\\
        0 & 0 & 0
      \end{pmatrix} 
    \] 
    
    \[
    \leadsto \begin{cases}
      x_1 = 0 \\
      x_2 = 0
    \end{cases}
    \quad \text{Quindi } N(A) = \left\{ \mathbb{O}=\begin{pmatrix} 0\\0 \end{pmatrix}  \right\} 
    \] 
  \end{example}
\end{figure}
\begin{figure}[H]
  \begin{example}
    \[
    \mathbb{K} = \mathbb{R}, \quad A = \begin{pmatrix} 
      1 & 2 & 0 \\
      0 & 6 & 3
    \end{pmatrix} 
    , \quad
    N(A) \subseteq \mathbb{R}^3
    \] 
    Risolviamo il sistema lineare \( Ax = 0 \) 
    \[
      \begin{pmatrix}[ccc|c]
        1 & 2 & 0 & 0\\
        0 & 6 & 3 & 0
    \end{pmatrix} 
    \stackrel{E_2(\frac{1}{6})}{\leadsto}
    \begin{pmatrix}[ccc|c]
      1 & 2 & 0 & 0\\
      0 & 1 & \frac{1}{2} & 0
    \end{pmatrix} 
    \] 
    \[
    \leadsto \begin{cases}
      _1 + 2x_2 = 0 \\
      x_2 + \frac{1}{2}x_3 = 0
    \end{cases}
    \] 
    Siccome la matrice ha soltanto 2 colonne dominanti bisogna introdurre un parametro
    per la variabile libera \( x_3 \).
    \[
    x_3 = t, \quad t \in \mathbb{R}
    \] 
    \[
    \begin{cases}
      x_1 = -2 \left( -\frac{1}{2}t \right) = t \\
      x_2 = -\frac{1}{2}t \\
      x_3 = t
    \end{cases}
    \] 
    Quindi \( N(A) = \left\{ \begin{pmatrix} t\\ -\frac{1}{2} \\ t \end{pmatrix} =
    t \begin{pmatrix} 1\\-\frac{1}{2}\\1 \end{pmatrix} \Big |\; t \in \mathbb{R} \right\}
    = \left< \begin{pmatrix} 1 \\ -\frac{1}{2} \\ 1 \end{pmatrix}  \right> 
    \underset{\text{sottospazio}}{\subseteq} \mathbb{R}^3 \) 
  \end{example}
\end{figure}

\section{Dipendenza e indipendenza lineare}
\begin{figure}[H]
  \begin{example}
    \[
      \mathbb{K} = \mathbb{R}, \quad V = \mathbb{R}^2 = \left\{ \begin{pmatrix} 
          x_1 \\
          x_2
      \end{pmatrix} \in M_{2 \times 1}(\mathbb{R})  \right\}
    \] 
    \[
      \mathcal{C} = \left\{ \begin{pmatrix} 
          0\\1
      \end{pmatrix},
      \begin{pmatrix} 
        1\\0
      \end{pmatrix},
      \begin{pmatrix} 
        3\\2
      \end{pmatrix} 
    \right\}\; \text{insieme di generatori}
    \] 
    Infatti, per ogni \( v = \begin{pmatrix} v_1\\v_2 \end{pmatrix} \in V \) abbiamo che:
    \[
    \begin{pmatrix} 
      v_1\\
      v_2
    \end{pmatrix} 
    =
    (v_2 - 2) \begin{pmatrix} 0\\1 \end{pmatrix} 
    +
    (v_1 - 3) \begin{pmatrix} 1\\0 \end{pmatrix}
    +
    1 \begin{pmatrix} 3\\2 \end{pmatrix}
    \] 
    \[
    = v_2 \begin{pmatrix} 0\\1 \end{pmatrix}
    +
    v_1 \begin{pmatrix} 1\\0 \end{pmatrix}
    +
    0 \begin{pmatrix} 3\\2 \end{pmatrix}
    \]
    \[
    = 0 \begin{pmatrix} 0\\1 \end{pmatrix} 
    +
    (v_1-\frac{3}{2}v_2) \begin{pmatrix} 1\\0 \end{pmatrix}
    +
    \frac{v_2}{2} \begin{pmatrix} 3\\2 \end{pmatrix}
    \] 
    Non è efficiente usare l'insieme di generatori \( \mathcal{C} \) perchè esistono
    almeno 2 sottoinsiemi di generatori più piccoli:
    \[
    \left\{ \begin{pmatrix} 0\\1 \end{pmatrix}, \begin{pmatrix} 1\\0 \end{pmatrix}  \right\},
    \left\{ \begin{pmatrix} 1\\0 \end{pmatrix}, \begin{pmatrix} 3\\2 \end{pmatrix}  \right\}
    \subseteq \mathcal{C}
    \] 
    In particolare:
    \[
    \begin{pmatrix} 3\\2 \end{pmatrix} = 2 \begin{pmatrix} 0\\1 \end{pmatrix} +
    3 \begin{pmatrix} 1\\0 \end{pmatrix}
    \] 
    \[
    \begin{pmatrix} 0\\1 \end{pmatrix} = -\frac{3}{2} \begin{pmatrix} 1\\0 \end{pmatrix} +
    \frac{1}{2} \begin{pmatrix} 3\\2 \end{pmatrix}
    \] 
  \end{example}
\end{figure}

\subsection{Proposizione}
\label{prop:generatori}
Se \( \{v_1, \ldots, v_n\}  \) è un insieme di generatori di uno spazio vettoriale \( V \) 
su \( \mathbb{K} \) e \( v_n \) è una combinazione lineare di \( v_1, \ldots, v_{n-1} \),
allora \( \{v_1, \ldots, v_{n-1}\}  \) è un insieme di generatori.


\subsubsection{Dimostrazione}
Siano \( \alpha_1, \ldots, \alpha_{n-1} \in \mathbb{K} \) tali che:
\[
v_n = \sum_{i=1}^{n-1} \alpha_i v_i
\] 
Per ogni \( v \in V \) esistono \( \beta_1, \ldots, \beta_n \in \mathbb{K} \) tali che:
\[
  v = \beta_1 v_1 + \ldots + \beta_{n-1} v_{n-1} + \beta_n v_n
\] 
\[
  = \beta_1 v_1 + \ldots + \beta_{n-1} v_{n-1} + \beta_n \left(\sum_{i=1}^{n-1} \alpha_i v_i\right)
\] 
\[
 = (\beta_1 + \beta_n \alpha_1) v_1 + \ldots + (\beta_{n-1} + \beta_n \alpha_{n-1}) v_{n-1}
\] 
Quindi \( \{v_1, \ldots, v_{n-1}\}  \) è un insieme di generatori. \( \quad \square \)

\subsection{Definizione}
Siano \( v_1, \ldots, v_n \in V \) vettori in uno spazio vettoriale \( V \). 
Un insieme
\( \{v_1, \ldots, v_n\}  \) è detto \textbf{linearmente dipendente} se almeno uno dei
vettori \( v_1, \ldots, v_n \) è combinazione lineare dei rimanenti.

\subsection{Teorema}
Siano \( v_1, \ldots, v_n \in V \). Sono equivalenti i seguenti enunciati:
\begin{enumerate}
  \item \( \{v_1, \ldots, v_n\}  \) \textbf{non} è linearmente dipendente
  \item Se \[ \sum_{i=1}^n \alpha_i v_i = \sum_{i=1}^n \beta_i v_i \] allora 
    \( \alpha_i = \beta_i \) per ogni \( 1 \le i \le n \)
  \item Se \( \alpha_1, \ldots, \alpha_n \in \mathbb{K} \) sono coefficienti tali che
    \[
    \alpha_1 v_1 + \ldots + \alpha_n v_n = 0_v
    \] 
    allora \( \alpha_1 = \alpha_2 = \ldots = 0 \) 
\end{enumerate}
Se valgono le condizioni (1), (2) + (3), allora \( \{v_1, \ldots, v_n\}  \) è detto
\textbf{linearmente indipendente}.

\subsubsection{Dimostrazione}
Dimostriamo che \( (1) \Rightarrow (2) \Rightarrow (3) \Rightarrow (1) \), quindi:
\[
\neg (1) \Rightarrow \neg (2) \Rightarrow \neg (3) \land (2) \Rightarrow (3)
\] 
\begin{itemize}
  \item \( [(2) \Rightarrow (3)] \) Supponiamo che:
    \[
    \left( \sum_{i=1}^n \alpha_i v_i \right) = \left( \sum_{i=1}^n \beta_i v_i \right) \Rightarrow
    \alpha_i = \beta_i \quad \forall i \in \{1, \ldots, n\}
    \] 
    Supponiamo che:
    \[
    \alpha_1 v_1 + \alpha_2 v_2 + \ldots + \alpha_n v_n = 0_v
    \] 
    \[
    \alpha_1 v_1 + \alpha_2 v_2 + \ldots + \alpha_n v_n = 0 \cdot v_1 + 0 \cdot v_2 + \ldots + 0 \cdot v_n
    \] 
    Quindi \( \alpha_1 = \ldots = \alpha_n = 0 \) per (2).

  \item \( [\neg (2) \Rightarrow \neg (3)] \) Supponiamo che:
    \[
    \sum_{i=1}^n \alpha_i v_i = \sum_{i=1}^n \beta_i v_i
    \] 
    e \( a_j \neq \beta_j \) per qualche \( 1 \le j \le n \). Quindi:
    \[
    0_v = \sum_{i=1}^n \alpha_i v_i - \sum_{i=1}^n \beta_i v_i = \sum_{i=1}^n (\alpha_i - \beta_i) v_i
    \] 
    e allora:
    \[ v_j = \sum_{i=1}^{j-1} \frac{\beta_i - \alpha_i}{\alpha_j - \beta_j}v_i + 
    \sum_{i=j+1}^n \frac{\beta_i - \alpha_i}{\alpha_j - \beta_j}v_i \]
    Dunque \( \{v_1, \ldots, v_n\}  \) è linearmente dipendente

  \item \( [\neg (1) \Rightarrow \neg (3)] \) Supponiamo che \( \{v_1, \ldots, v_n\}  \) 
    sia linearmente dipendente, cioè esistono \( \alpha_1, \ldots, \alpha_{j+1}, \ldots, \alpha_n \in \mathbb{K}\) 
    tali che:
    \[
    v_j = \sum_{i=1}^{j-1} \alpha_i v_i + \sum_{i=j+1}^n \alpha_i v_i
    \] 
    Allora:
    \[
      0_v = \alpha_1 v_1 + \ldots + \alpha_{j-1} v_{j-1} + (-1) v_j + \alpha_{j+1} v_{j+1} + \ldots + \alpha_n v_n
    \] 
    Dunque (3) non è verificata. \( \quad \square \)
\end{itemize}

\subsubsection{Esempi}
\begin{figure}[H]
  \begin{example}
    \[
    \mathbb{K} = \mathbb{R}, \quad V = \mathbb{R}^2
    \] 
    L'insieme \( \left\{ \begin{pmatrix} 
        1\\0
    \end{pmatrix},
    \begin{pmatrix} 
      3\\2
    \end{pmatrix} 
  \right\}  \) è linearmente indipendente. Infatti se:
  \[
  \mathbb{O} = \begin{pmatrix} 0\\0 \end{pmatrix} = \alpha_1 + \begin{pmatrix} 1\\0 \end{pmatrix} 
  + \alpha_2 \begin{pmatrix} 3\\2 \end{pmatrix}
  = \begin{pmatrix} \alpha_1\\0 \end{pmatrix} 
  +
  \begin{pmatrix} 3 \alpha_2\\ 2 \alpha_2 \end{pmatrix} 
  \] 
  \[
  = \begin{pmatrix} 
    \alpha_1 + 3 \alpha_2\\
    2 \alpha_2
  \end{pmatrix} 
  \] 
  Quindi \( \alpha_1 + 3 \alpha_2 = 0 \) e \( 2 \alpha_2 = 0 \). Abbiamo che \( \alpha_2 = 0 \) e
  \( \alpha_1 = 0 \). Quindi l'insieme è linearmente indipendente.
  \end{example}
\end{figure}
\begin{figure}[H]
  \begin{example}
    Un insieme \( \{v_1, v_2\} \subseteq V \) è linearmente dipendente se e solo se esiste
    \( \alpha \in \mathbb{K} \) tale che \( \alpha v_1 = v_2 \) oppure
    \( v_1 = \alpha v_2 \) 
  \end{example}
\end{figure}
\begin{figure}[H]
  \begin{example}
    Un insieme \( \{v\} \subseteq V \) è linearmente dipendente se e solo se \( v = 0_v \).
    Inoltre, per ogni \( \{v_1, \ldots, v_n\}  \), se \( v_j = 0_v \) per qualche \( j \),
    allora \( \{v_1, \ldots, v_n\}  \) è linearmente dipendente perchè:
    \[
    0_v = \underbrace{0 \cdot v_1}_{0_v} + \ldots + \underbrace{0 \cdot v_{j-1}}_{0_v} +
    \underbrace{v_j}_{0_v} + \underbrace{0 \cdot v_{j+1}}_{0_v} + \ldots +
    \underbrace{0 \cdot v_n}_{0_v}
    \] 
    e quindi abbiamo \( \neg (3) \) 
  \end{example}
\end{figure}

\subsection{Definizione}
\label{def:base}
Sia \( V \) uno spazio vettoriale su \( \mathbb{K} \) e siano \( v_1, \ldots, v_n \in V \).
L'insieme \( \mathcal{U} = \{v_1, \ldots, v_n\}  \) è detto \textbf{base} di \( V \) 
se \( \mathcal{U} \) è un insieme di generatori di \( V \) e \( \mathcal{U} \) è linearmente
indipendente.

\subsection{Osservazione}
Per il Teorema \ref{def:base} un sottoinsieme \( \mathcal{U} \subseteq V \) è una base
se e solo se possiamo ricostruire in un modo unico tutti i vettori di \( V \) mediante
combinazioni lineari. Possiamo pensare ad una base \( \mathcal{U} = \{b_1, \ldots, b_n\}  \) 
di \( V \) come ad un sistema di coordinate:

\noindent Sia \( v \in V \). Esiste un unico vettore \( \begin{pmatrix} 
  \alpha_1 \\
  \vdots \\
  \alpha_n
\end{pmatrix} \in \mathbb{K}^n  \) tale che \( v = \alpha_1 b_1 + \ldots + \alpha_n b_n \).

\noindent Scriviamo \( [v]_\mathcal{U} \) per il vettore \( \begin{pmatrix} 
  \alpha_1\\
  \vdots\\
  \alpha_n
\end{pmatrix}  \) 

\subsubsection{Esempi}
\begin{figure}[H]
  \begin{example}
    \[
      V = \mathbb{K}^n \quad \mathcal{C} = \left\{ \underbrace{\begin{pmatrix} 
          1\\
          0\\
          \vdots\\
          0
      \end{pmatrix}}_{e_1},
      \underbrace{\begin{pmatrix} 
        0\\
        1\\
        \vdots\\
        0
    \end{pmatrix}}_{e_2},
    \ldots,
    \underbrace{\begin{pmatrix} 
      0\\
      0\\
      \vdots\\
      1
  \end{pmatrix}}_{e_n}
\right\}
    \] 

    \begin{center}
      \begin{tikzpicture}
        \draw[->] (-1,0) -- (3,0) node[right] {\( x_1 \)};
        \draw[->] (0,-1) -- (0,3) node[above] {\( x_2 \)};

        \node at (3,3) {\( \mathbb{R}^2 \)};

        \draw[->, thick, red] (0,0) -- (1,0) node[below, scale=0.8, yshift=-2] {\( \begin{pmatrix} 0\\1 \end{pmatrix}  \)};
        \draw[->, thick, blue] (0,0) -- (0,1) node[left, scale=0.8] {\( \begin{pmatrix} 1\\0 \end{pmatrix}  \)};
      \end{tikzpicture}
      \caption{Base canonica di \( \mathbb{K}^n \) }
    \end{center}
    Infatti, per ogni \( v = \begin{pmatrix} 
        v_1\\
        \vdots\\
        v_n
    \end{pmatrix} \in \mathbb{K}^n \) abbiamo che:
    \[
    v = v_1 e_1 + \ldots + v_n e_n
    \] 
    Supponiamo \( \mathbb{O} = v_1 e_1 + \ldots + v_n e_n = \begin{pmatrix} 
    v_1\\
    \vdots\\
    v_n
    \end{pmatrix}  \), quindi: \[ v_1 = 0,\; v_2 = 0, \ldots, v_n = 0 \]
  \end{example}
\end{figure}
\begin{figure}[H]
  \begin{example}
    \[
      V = \mathbb{R}^2, \quad \mathbb{K} = \mathbb{R} \quad \mathcal{B} = \left\{ \begin{pmatrix} 
          1\\0
      \end{pmatrix},
      \begin{pmatrix} 
        3\\2
      \end{pmatrix}
    \right\}
    \] 
    è una base di \( \mathbb{R}^2 \), quindi non esiste un unica base di \( \mathbb{R}^2 \).
  \end{example}
\end{figure}

\subsection{Base di C(U) per una matrice U in forma ridotta }
\begin{example}
  \[
  U = \begin{pmatrix} 
    1 & 2 & 7 & 3 & 0 \\
    0 & 0 & 1 & -1 & 1 \\
    0 & 0 & 0 & 0 & 1 \\
    0 & 0 & 0 & 0 & 0
  \end{pmatrix} 
  \] 
  \[
  C(U) = \left< \begin{pmatrix} 1\\0\\0\\0 \end{pmatrix},
  \begin{pmatrix} 2\\0\\0\\0 \end{pmatrix},
  \begin{pmatrix} 7\\1\\0\\0 \end{pmatrix},
  \begin{pmatrix} 3\\-1\\0\\0 \end{pmatrix},
  \begin{pmatrix} 0\\1\\1\\0 \end{pmatrix}
\right> \subseteq \mathbb{R}^4
  \] 
  Le colonne dominanti formano una base di \( C(U) \), infatti:
  \begin{itemize}
    \item \textbf{Linearmente indipendente:} Siano \( \alpha_1, \alpha_2, \alpha_3 \in \mathbb{R} \) 
      tali che:
      \[
      \begin{pmatrix} 
        0\\
        0\\
        0\\
        0
      \end{pmatrix} 
      =
      \alpha_1 \begin{pmatrix} 1\\0\\0\\0 \end{pmatrix}
      +
      \alpha_2 \begin{pmatrix} 7\\1\\0\\0 \end{pmatrix}
      +
      \alpha_3 \begin{pmatrix} 0\\1\\1\\0 \end{pmatrix}
      \] 
      \[
      = \begin{pmatrix} 
        \alpha_1\\
        0\\
        0\\
        0
      \end{pmatrix} 
      +
      \begin{pmatrix} 
        7\alpha_2\\
        \alpha_2\\
        0\\
        0
      \end{pmatrix}
      +
      \begin{pmatrix} 
        0\\
        \alpha_3\\
        \alpha_3\\
        0
      \end{pmatrix}
      \] 
      \[
      = \begin{pmatrix} 
        \alpha_1 + 7\alpha_2\\
        \alpha_2 + \alpha_3\\
        \alpha_3\\
        0
      \end{pmatrix}
      \] 
     Quindi \( \alpha_3 = 0 \), \(\; \alpha_2 = \alpha_2 + \alpha_3 = 0 \),
     \(\; \alpha_1 = \alpha_1 + 7 \alpha_2 = 0 \) 

   \item \textbf{Insieme di generatori} Proposizione \ref{prop:generatori}:
     \[
     \begin{pmatrix} 
       2\\
       0\\
       0\\
       0
     \end{pmatrix} 
     =
     2 \begin{pmatrix} 1\\0\\0\\0 \end{pmatrix}, \quad
     \begin{pmatrix} 
       3\\
       -1\\
       0\\
       0\\
     \end{pmatrix} 
     =
     -1 \begin{pmatrix} 7\\1\\0\\0 \end{pmatrix} +
     10 \begin{pmatrix} 1\\0\\0\\0 \end{pmatrix}
     \] 
     Quindi \( \left\{ \begin{pmatrix} 
         1\\0\\0\\0
     \end{pmatrix},
     \begin{pmatrix} 
       7\\1\\0\\0
     \end{pmatrix} ,
     \begin{pmatrix} 
       0\\1\\1\\0
     \end{pmatrix} 
   \right\}  \) è un insieme di generatori di \( C(U) \).
  \end{itemize}
\end{example}

\subsubsection{Osservazioni}
Le colonne dominanti di una matrice \( U \) in forma ridotta formano una base di \( C(U) \).
Inoltre le colonne non nulle di \( U^T \) (cioè le righe non nulle di \( U \) formano una
base di \( C(U^T) \).

\subsection{Proposizione}
Sia \( \mathcal{B} = \{v_1, \ldots, v_n\}  \) una base di uno spazio vettoriale su
\( \mathbb{K} \).
\begin{enumerate}
  \item \( \mathcal{B} \) è un insieme di generatori minimo, cioè nessun sottoinsieme di
    \( \mathcal{B} \) è un insieme di generatori
  \item \( \mathcal{B} \) è massimamente linearmente indipendente, cioè nessun insieme di
    vettori che contenga propriamente \( \mathcal{B} \) è linearmente indipendente.
\end{enumerate}

\subsection{Teorema}
Sia \( V \) uno spazio vettoriale su \( \mathbb{K} \) finitamente generato.
\begin{itemize}
  \item Se \( V \neq 0 \), allora \( V \) possiede una base.
  \item Se \( V = 0 \), allora \( V \) \textbf{non} possiede una base.
\end{itemize}

\subsubsection{Dimostrazione}
Se \( V = 0 = \{0_v\}  \), allora ogni sottoinsieme non vuoto di \( V \) contiene \( 0_v \) 
e quindi non può essere linearmente indipendente.

\vspace{1em}
\noindent Supponiamo \( V \neq 0 \).

\noindent Sia \( \mathcal{B}_n = \{v_1, \ldots, v_n\}  \) un insieme di generatori.
Se \( \mathcal{B}_n \) è linearmente indipendente, allora \( \mathcal{B}_n \) è una base
di \( V \). Altrimenti uno dei vettori di \( \mathcal{B}_n \) è combinazione lineare
dei rimanenti. Senza perdita di generalità supponiamo che:
\[
v_n = \sum_{i=1}^{n-1} \alpha_i v_i
\] 
Per \ref{prop:generatori} \( \mathcal{B}_{n-1} = \{v_1, \ldots, v_{n-1}\}  \) è un 
insieme di generatori. Se \( \mathcal{B}_{n-1} \) è linearmente indipendente, allora
\( \mathcal{B}_{n-1} \) è una base. Altrimenti continuiamo come sopra.
Proseguendo così si otterrà un sottoinsieme di \( \mathcal{B}_n \) che è una base.
\( \quad \square \) 

\begin{figure}[H]
  \begin{example}
    \[
    \mathbb{K} = \mathbb{R}, \quad V = \mathbb{R}^2
    \] 
    \[
      \mathcal{C}_3 = \left\{ \underset{v_1}{\begin{pmatrix} 
          0\\1
      \end{pmatrix}},
      \underset{v_2}{\begin{pmatrix} 
        1\\0
      \end{pmatrix}} ,
      \underset{v_3}{\begin{pmatrix} 
        3\\2
  \end{pmatrix}} \right\} 
    \] 
    \( \mathcal{C}_3 \) è un insieme di generatori, ma non è linearmente indipendente:
    \[
    v_3 = \begin{pmatrix} 
      3\\2
    \end{pmatrix} 
    =
    3 \begin{pmatrix} 1\\0 \end{pmatrix} 
    +
    2 \begin{pmatrix} 0\\1 \end{pmatrix} 
    = 3 v_2 + 2 v_1
    \] 
    Allora:
    \[
      \mathcal{C}_2 = \left\{ \begin{pmatrix} 
          0\\1
        \end{pmatrix},
        \begin{pmatrix} 
          1\\0
        \end{pmatrix}
      \right\}
    \] 
    è un insieme di generatori. Inoltre \( \mathcal{C}_2 \) è linearmente indipendente:
    \[
    \begin{pmatrix} 0\\0 \end{pmatrix} 
    =
    \alpha_1 \begin{pmatrix} 0\\1 \end{pmatrix} 
    +
    \alpha_2 \begin{pmatrix} 1\\0 \end{pmatrix} 
    =
    \begin{pmatrix} 0\\\alpha_1 \end{pmatrix} 
    +
    \begin{pmatrix} \alpha_2\\0 \end{pmatrix} 
    =
    \begin{pmatrix} \alpha_2\\\alpha_1\end{pmatrix} 
    \] 
    Quindi \( \alpha_1 = 0 = \alpha_2 \). Allora \( \mathcal{C}_2 \) è una base.
  \end{example}
\end{figure}

\subsection{Teorema di Steinitz}
Sia \( \mathcal{G} = \{v_1, \ldots, v_n\}  \) un insieme di generatori di \( V \) e
\( \mathcal{L} = \{u_1, \ldots, u_m\}  \) un insieme linearmente indipendente. Allora
\( m \le n \) ed esiste un insieme di generatori di \( V \) formato da \( \mathcal{L} \) 
e \( n-m \) vettori di \( \mathcal{G} \).

\subsection{Corollario}
Se \( \mathcal{B}_1 =\{v_1, \ldots, v_n\}  \) e \( \mathcal{B}_2 = \{u_1, \ldots, u_n\}  \)
sono basi di uno spazio vettoriale, allora \( m = n \).

\subsubsection{Dimostrazione}
Ponendo \( \mathcal{G} = \mathcal{B}_1 \) e \( \mathcal{L} = \mathcal{B}_2 \) nel teorema
di Steinitz, si ha \( m \le n \). 
Ponendo \( \mathcal{G} = \mathcal{B}_2 \) e \( \mathcal{L} = \mathcal{B}_1 \) si ha
\( n \le m \). Quindi \( m = n \). \( \quad \square \)

\subsection{Definizione}
Sia \( V \) uno spazio vettoriale finitamente generato. Il numero di vettori che formano
una base di \( V \) è detto \textbf{dimensione} di \( V \) e si indica con
\( dim_\mathbb{K}(V) \). 

\subsubsection{Esempi}
\begin{figure}[H]
  \begin{example}
    \[
    \mathbb{K} = \mathbb{C}, \quad V = \mathbb{C}
    \] 
    \( \{1\} \) è una base di \( V \) su \( \mathbb{C} \). Dunque 
    \( dim_\mathbb{K}(V) = dim_{\mathbb{C}}(\mathbb{C}) = 1 \) 
  \end{example}
\end{figure}

\begin{figure}[H]
  \begin{example}
    \[
    \mathbb{K} = \mathbb{R}, \quad V = \mathbb{C}
    \] 
    \( \{1, i\}  \) è una base di \( V \) su \( \mathbb{R} \).
    \begin{itemize}
      \item (Insieme di generatori): \( z \in \mathbb{C} = V \) 
        \[
        z = a + bi = a(1) + b(i), \quad a,b \in \mathbb{R}. 
        \] 
      \item (Linearmente indipendente): \( 0_v \in \mathbb{C} \) 
        \[
        0 = 0 + 0i
        \] 
        è l'unico modo di scrivere \( 0 \) come combinazione lineare di \( \{1,i\}  \).
        \[
          \Rightarrow dim_{\mathbb{K}}(V) = dim_{\mathbb{R}}(\mathbb{C}) = 2
        \] 
    \end{itemize}
  \end{example}
\end{figure}

\subsection{Corollario}
In uno spazio vettoriale \( V \) di dimensione \( dim_{\mathbb{K}}(V) = n \), si ha
\begin{enumerate}
  \item Un insieme con \( > n \) vettori è linearmente dipendente.
  \item Se \( n \) vettori sono linearmente indipendenti, allora formano una base di
    \( V \).
  \item Ogni insieme di generatori consiste di almeno \( n \) vettori
\end{enumerate}

\subsection{Proposizione}
Sia \( dim_{\mathbb{K}}(V) = n \). Allora ogni sottospazio \( U \) di \( V \) ha dimensione
\( dim_{\mathbb{K}}(U) \le n \). Inoltre \( dim_{\mathbb{K}}(U) = n \) se e solo se
\( U = V \) 

\subsubsection{Dimostrazione}
Sia \( \mathcal{B} = \{u_1, \ldots, u_n\}  \) una base di \( U \). Allora \( \mathbb{B} \) 
è linearmente indipendente in \( V \) perchè \( 0_v = 0_u \).
Quindi possiamo completare \( \mathbb{B} \) a una base di \( V \) (usando il teorema
di Steinitz). Allora \( \underbrace{\#\mathcal{B}}_{dim_{\mathbb{K}}(U)} \le 
\underbrace{\#\mathcal{B}'}_{dim_{\mathbb{K}}(V)} \).
Abbiamo che \( \mathcal{B} \) contiene \( n \) elementi (cioè \( dim_{\mathbb{K}}(U) = n \) )
se e solo se \( \mathcal{B} \) è una base di \( V \). Quindi in tal caso
abbiamo:
\[
U = \left< u_1, \ldots, u_n \right> = V \quad \square
\] 

\section{Applicazione lineare}
D'ora in poi, tutti gli spazi vettoriali saranno \textbf{finitamente generati}.

\subsection{Definizione} \label{def:applicazioneLineare}
Siano \( U \) e \( V \) spazi vettoriali su \( \mathbb{K} \). Un'applicazione
\( f: U \to V \) si dice \textbf{lineare} se, per \( u, u' \in U \) e \( \alpha \in \mathbb{K} \)
si ha:
\begin{enumerate}
  \item \( f(u+u') = f(u) + f(u') \) 
  \item \( f(\alpha u) = \alpha f(u) \) 
\end{enumerate}

\subsubsection{Osservazioni}
\begin{enumerate}
  \item \( f(0_u) \underset{\ref{oss:5.2}(b)}{=} f(0 \cdot 0_u) \underset{\ref{def:applicazioneLineare}(2)}{=}
    0 \cdot f(0_u) \underset{\ref{oss:5.2}(b)}{=} 0_v\) 

  \item \( f(-u) \underset{\ref{oss:5.2}(d)}{=} f((-1)\cdot u) \underset{\ref{def:applicazioneLineare}(2)}{=}
    (-1) \cdot f(u) \underset{\ref{oss:5.2}(d)}{=} -f(u)\) 
\end{enumerate}
Per tutti gli elementi di \( U \) 

\subsubsection{Esempi}
\begin{example}
  \[
    U = \mathbb{R}_2[x] = \{a_0+a_1x+a_2x^2 \;|\; a_0,a_1,a_2 \in \mathbb{R}\} 
  \] 
  \[
    V = \mathbb{R}^2 = M_{2 \times 1}(\mathbb{R})
  \] 
  \[
  p = a_0 + a_1x+a_2x^2
  \] 

  \[
  f: U \to V
\]
  \[
  f(p) = \begin{pmatrix} p(0)\\p(1) \end{pmatrix} =
  \begin{pmatrix} 
    a_0 + a_1 \cdot 0 + a_2 \cdot 0^2\\
    a_0 + a_1 \cdot 1 + a_2 \cdot 1^2
  \end{pmatrix} =
  \begin{pmatrix} 
    a_0 \\
    a_0 + a_1 + a_2
  \end{pmatrix} 
  \] 
  \( f \) è lineare. Infatti per ogni \( p=a_0+a_1x+a_2x^2 \), \( q=b_0+b_1x+b_2x^2 \)
  \begin{enumerate}
    \item \[ f(p+q) = \left( (a_0 + b_0) + (a_1+b_1)x + (a_2+b_2)x^2 \right) = \]
      \[
      \begin{pmatrix} 
        a_0+b_0\\
        (a_0+b_0) + (a_1+b_1) + (a_2 + b_2)
      \end{pmatrix} 
      \] 

      \vspace{1em}
      \[
        f(p) + f(q) = \begin{pmatrix} 
          a_0\\
          a_0 + a_1 + a_2
        \end{pmatrix} 
        +
        \begin{pmatrix} 
          v_0\\
          b_0 + b_1 + b_2
        \end{pmatrix} 
        =
      \] 
      \[
        \begin{pmatrix} 
          a_0 + b_0\\
          (a_0+a_1+a_2) + (b_0+b_1+b_2)
        \end{pmatrix} 
      \] 
      Quindi \( f(p+q) = f(p) + f(q) \) 

    \item \( \alpha \in \mathbb{R} \)
      \[
      f(\alpha p) = f((\alpha a_0) + (\alpha a_1)x + (\alpha a_2)x^2 ) =
      \] 
      \[
      \begin{pmatrix} 
        \alpha a_0\\
        \alpha a_0 + \alpha a_1 + \alpha a_2
      \end{pmatrix} 
      \] 

      \vspace{1em}
      \[
      \alpha f(p) = \alpha \begin{pmatrix} 
        a_0 \\
        a_0 + a_1 + a_2
      \end{pmatrix} 
      =
      \begin{pmatrix} 
        \alpha a_0\\
        \alpha a_0 + \alpha a_1 + \alpha a_2
      \end{pmatrix} 
      \] 
      Quindi \( f(\alpha p) = \alpha f(p) \) 
  \end{enumerate}
\end{example}

\subsection{Applicazioni lineari \texorpdfstring{\( \mathbb{K}^n \to \mathbb{K}^m \)}{K\^n -> K\^m}}
\label{7.2}
Sia \( A \in  M_{m \times n}(\mathbb{K}) \), definiamo \( f_A: \mathbb{K}^n \to \mathbb{K}^m \) 
per ogni \( v = \begin{pmatrix} 
  v_1\\
  \vdots\\
  v_n
\end{pmatrix} \in \mathbb{K}^n\) \( f(v) = Av \). \( f_A \) è lineare:
\begin{enumerate}
  \item \( f_A(v+w) = A(v+w) = Av + Aw = f_A(v) + f_A(w) \) 

  \item \( f_A(\alpha v) = A(\alpha v) = \alpha(Av) = \alpha f_A(v) \) 
\end{enumerate}

\subsubsection{Esempi}
\begin{figure}[H]
  \begin{example}
    \[
    A = \begin{pmatrix} 
      2 & i\\
      0 & 1-i \\
      1 & 0
    \end{pmatrix} \in M_{3 \times 2}(\mathbb{C})
    \] 
    \[
    f_A: \mathbb{C}^2 \to \mathbb{C}^2
    \] 
    \[
    f_A\left( \begin{pmatrix} x\\y \end{pmatrix}  \right) =
    \begin{pmatrix} 
      2 & i\\
      0 & 1-i \\
      1 & 0
    \end{pmatrix} 
    \begin{pmatrix} x\\y \end{pmatrix} 
    =
    \begin{pmatrix} 
      2x + iy\\
      (1-i)y\\
      x
    \end{pmatrix} 
    \] 
  \end{example}
\end{figure}

\begin{figure}[H]
  \begin{example}
    \[
    f: \mathbb{K}^3 \to \mathbb{K}^2, \quad f\left( \begin{pmatrix} 
        x\\
        y\\
        z
    \end{pmatrix}  \right) =
    \begin{pmatrix} 
      2x \\
      3z - y
    \end{pmatrix} 
    \] 
    \( f \) è lineare. Notiamo che, per ogni \( \begin{pmatrix} x\\y\\z \end{pmatrix} 
    \in \mathbb{K}^3\), abbiamo:
    \[
    f\left( \begin{pmatrix} x\\y\\z \end{pmatrix}  \right) =
    f\left( x \begin{pmatrix} 1\\0\\0 \end{pmatrix} +
    y \begin{pmatrix} 0\\1\\0 \end{pmatrix} +
  z \begin{pmatrix} 0\\0\\1 \end{pmatrix} \right) 
    \] 
    \[
    = f\left(x \begin{pmatrix} 1\\0\\0 \end{pmatrix} \right) +
    f\left( y \begin{pmatrix} 0\\1\\0 \end{pmatrix}  \right) +
    f\left( z \begin{pmatrix} 0\\0\\1 \end{pmatrix}  \right) 
    \] 
    \[
    = xf\left( \begin{pmatrix} 1\\0\\0 \end{pmatrix}  \right) +
    yf\left( \begin{pmatrix} 0\\1\\0 \end{pmatrix}  \right) +
    zf\left( \begin{pmatrix} 0\\0\\1 \end{pmatrix}  \right) 
    \] 
    \[
    = x \begin{pmatrix} 2\\0 \end{pmatrix} +
    y \begin{pmatrix} 0\\-1 \end{pmatrix} +
    z \begin{pmatrix} 0\\3 \end{pmatrix} 
    \] 
    \[
    = x \begin{pmatrix} 2\\0 \end{pmatrix} +
    y \begin{pmatrix} 0\\-1 \end{pmatrix} +
    z \begin{pmatrix} 0\\3 \end{pmatrix} 
    \] 
    \[
    = \begin{pmatrix} 
      2 & 0 & 0\\
      0 & -1 & 3
    \end{pmatrix} 
    \begin{pmatrix} x\\y\\z \end{pmatrix} 
    \] 
    Dunque \( f = f_A \) dove \( A = \begin{pmatrix} 2 & 0 & 0\\ 0 & -1 & 3 \end{pmatrix}  \) 
  \end{example}
\end{figure}

Per ogni applicazione lineare \( f: \mathbb{K}^n \to \mathbb{K}^m \) e per 
\( v = \begin{pmatrix} v_1\\\vdots\\v_n \end{pmatrix} \in \mathbb{K}^n \), abbiamo che
\[
  v = v_1 \underbrace{\begin{pmatrix} 
  1\\0\\\vdots\\0
\end{pmatrix}}_{e_1}
+ v_2 \underbrace{\begin{pmatrix} 
  0\\1\\\vdots\\0
\end{pmatrix}}_{e_2}
+ \ldots +
v_n \underbrace{\begin{pmatrix} 
  0\\0\\\vdots\\1
\end{pmatrix}}_{e_n}
\] 
\[
f(v) = f(v_1 e_1 + v_2 e_2 + \ldots + v_n e_n)
\] 
\[
= f(v_1 e_1) + f(v_2 e_2) + \ldots + f(v_n e_n)
\] 
\[
= v_1 f(e_1) + v_2 f(e_2) + \ldots + v_n f(e_n)
\] 
\[
= \left( f(e_1) \ldots f(e_n) \right) \begin{pmatrix} v_1\\\vdots\\v_n \end{pmatrix} 
\] 
\[
= Av
\] 
dove \( A = (f(e_1) \ldots f(e_n)) \) e \( \{e_1, \ldots, e_n\}  \) è la base canonica
di \( \mathbb{K}^n \). Allora \( f = f_A \). La matrice \( A \) è detta la
\textbf{matrice associata a \( f \) (rispetto alla base canonica)}

\vspace{1em}
\noindent \textbf{NB}: Per una matrice \( A \in M_{n \times n}(\mathbb{K}) \) invertibile
abbiamo \( f_A: \mathbb{K}^n \to \mathbb{K}^n \) e \( f_{A^{-1}}: \mathbb{K}^n \to \mathbb{K}^n \).
Osserviamo:
\[
  f_{A^{-1}}(f_A(v)) = f_{A^{-1}}(Av) = A^{-1}(Av) = (A_{-1}A)v = I_nv = v
\] 
\[
  f_A(f_{A^{-1}}(v)) = f_A(A^{-1}v) = AA^{-1}v = I_nv = v
\] 

\subsection{Definizione}
\label{def:7.3}
Un'applicazione lineare \( f: V \to W \) è detta \textbf{isomorfismo} se esiste
\( g: W \to V \) tale che \( g(f(v)) = V \) per ogni \( v \in V \) e
\( f(g(w)) = W \) per ogni \( w \in W \). L'applicazione lineare \( g \) è detta
\textbf{inversa di \( f \) } e si dice che \( V \) e \( W \) sono \textbf{isomorfi}.
Scriviamo \( f^{-1} = g \) e \( V \cong W \).

\begin{example}
  Sia \( f: \mathbb{K}^n \to \mathbb{K}^n \) un'applicazione lineare. Allora esiste
  una matrice \( A \in M_{n \times n}(\mathbb{K}) \) tale che \( f = f_A \).
  L'applicazione lineare \( f \) è un isomorfismo se e solo se \( A \) è invertibile.
  Infatti, supponiamo che esista \( f^{-1} \) e consideriamo la matrice associata \( B \),
  cioè \( f^{-1} = f_B \). Allora, per ogni \( v \in \mathbb{K}^n \), abbiamo:
  \[
    (BA)v = f_B f_A(v) = f^{-1} f(v) = v
  \] 
  \[
   = f f^{-1}(v) = f_A f_B(v) =f_A (BV) = (AB)v
  \] 
  Ne segue \( AB = I_n = BA \). Quindi \( B = A^{-1} \) 
\end{example}

\subsection{Applicazione delle coordinate}
Sia \( \mathcal{B} = \{b_1, \ldots, b_n\}  \) una base di uno spazio vettoriale \( V \) 
su \( \mathbb{K} \). Per ogni \( \alpha_1 b_1 + \ldots + \alpha_n b_n = v \in V \) abbiamo
definito il vettore: \[ [v]_{\mathcal{B}} = \begin{pmatrix} 
  \alpha_1\\
  \vdots\\
  \alpha_n
\end{pmatrix} \]
L'applicazione \( C_{\mathcal{B}}: V \to \mathbb{K}^n \) definita come:
\[
  C_{\mathcal{B}}(v) = [v]_{\mathcal{B}}
\] 
è lineare ed è detta \textbf{applicazione delle coordinate rispetto a \( \mathcal{B} \)}.
Infatti, per:
\[
v = \alpha_1 b_1 + \ldots + \alpha_n b_n, \quad w = \beta_1 b_1 + \ldots + \beta_n b_n \in V
\] 
e \( \alpha \in \mathbb{K} \), abbiamo
\begin{enumerate}
  \item \[
      C_{\mathcal{B}}(v+w) = C_{\mathcal{B}}(\alpha_1 b_1 + \ldots + \alpha_n b_n + 
      \beta_1 b_1 + \ldots + \beta_n b_n)
  \] 
  \[
    = C_{\mathcal{B}}((\alpha_1 + \beta_1) b_1 + \ldots + (\alpha_n + \beta_n) b_n)
  \] 
  \[
  = \begin{pmatrix} 
    \alpha_1 + \beta_1\\
    \vdots\\
    \alpha_n + \beta_n
  \end{pmatrix} 
  \] 
  \vspace{1em}
  \[
    C_{\mathcal{B}}(v) + C_{\mathcal{B}}(w) = \begin{pmatrix} 
      \alpha_1\\
      \vdots\\
      \alpha_n
    \end{pmatrix}
    +
    \begin{pmatrix} 
      \beta_1\\
      \vdots\\
      \beta_n
    \end{pmatrix}
  \] 
  \[
  = \begin{pmatrix} 
    \alpha_1 + \beta_1\\
    \vdots\\
    \alpha_n + \beta_n
  \end{pmatrix} 
  \] 
  Quindi \( C_{\mathcal{B}}(v+w) = C_{\mathcal{B}}(v) + C_{\mathcal{B}}(w) \) 

  \item 
    \[
      C_{\mathcal{B}}(\alpha v) = C_{\mathcal{B}}(\alpha(\alpha_1 b_1 + \ldots + \alpha_n b_n))
    \] 
    \[
    = C_{\mathcal{B}}((\alpha \alpha_1) b_1 + \ldots + (\alpha \alpha_n) b_n)
    \] 
    \[
    = \begin{pmatrix} 
      \alpha \alpha_1\\
      \vdots\\
      \alpha \alpha_n
    \end{pmatrix}
    \] 
    \vspace{1em}
    \[
    \alpha C_{\mathcal{B}}(v) = \alpha \begin{pmatrix} 
      \alpha_1\\
      \vdots\\
      \alpha_n
    \end{pmatrix}
    = \begin{pmatrix} 
      \alpha \alpha_1\\
      \vdots\\
      \alpha \alpha_n
    \end{pmatrix}
    \] 
    Quindi \( C_{\mathcal{B}}(\alpha v) = \alpha C_{\mathcal{B}}(v) \)
\end{enumerate}

\begin{example}
  \[
    V = \mathbb{R}_2[x], \quad \mathbb{K} = \mathbb{R}
  \] 
  \[
  V = \{a_0 + a_1x + a_2x^2 \;|\; a_0, a_1, a_2 \in \mathbb{R}\}
  \] 
  \[
  \mathcal{B} = \{b_1 = 1+x, b_2 = 1+x^2, b_3 = x + x^2\} 
  \] 
  è una base di \( V \).

  \vspace{1em}
  \noindent Prendiamo \( v = 6 + 3x - x^2 \in V\). Poichè \( \mathcal{B} \) è una base di
  \( V \), esistono \( \alpha_1, \alpha_2, \alpha_3 \in \mathbb{R} \) tali che
  \( v = \alpha_1 b_1 + \alpha_2 b_2 + \alpha_3 b_3 \).
  \[
  6 + 3x - x^2 = \alpha_1(1+x) + \alpha_2(1+x^2) + \alpha_3(x+x^2)
  \] 
  \[
  = (\alpha_1 + \alpha_1 x) + (\alpha_2 + \alpha_2 x^2) + (\alpha_3 x + \alpha_3 x^2)
  \] 
  \[
  = (\alpha_1 + \alpha_2) + (\alpha_1 + \alpha_3)x + (\alpha_2 + \alpha_3)x^2
  \] 
  Quindi:
  \[
  \begin{cases}
    \alpha_1 + \alpha_2 = 6 \\
    \alpha_1 + \alpha_3 = 3 \\
    \alpha_2 + \alpha_3 = -1
  \end{cases}
  \] 
  Risolviamo il sistema lineare usando l'Eliminazione di Gauss:
  \[
    \begin{pmatrix}[ccc|c]
      1 & 1 & 0 & 6\\
      1 & 0 & 1 & 3\\
      0 & 1 & 1 & -1
  \end{pmatrix} 
  \sim
  \ldots
  \sim
  \begin{pmatrix}[ccc|c]
    1 & 1 & 0 & 6\\
    0 & 1 & -1 & 3\\
    0 & 0 & 1 & -2
  \end{pmatrix} 
  \] 
  \[
  \leadsto \begin{cases}
    \alpha_1 + \alpha_2 = 6\\
    \alpha_2 - \alpha_3 = 3\\
    \alpha_3 = -2
  \end{cases}
  \to
  \begin{cases}
    \alpha_1 = 6-\alpha_2 = 5\\
    \alpha_2 = 3 + \alpha_3 = 1\\
    \alpha_3 = -2
  \end{cases}
  \] 
  Quindi:
  \[
  6 + 3x - x^2 = v = 5b_1 + b_2 - 2b_3 = 5(1+x) + (1+x^2) - 2(x+x^2)
  \] 
\end{example}

\subsection{Applicazione delle coordinate \texorpdfstring{\( C_{\mathcal{B}}: \mathbb{K}^n \to \mathbb{K}^n \)}{C\_B: K\^n -> K\^n}}
\label{7.5}
\begin{example}
  \[
  V = \mathbb{R}^2, \quad \mathcal{B} = \{b_1 = \begin{pmatrix}
      -1\\2
  \end{pmatrix},
  b_2 = \begin{pmatrix} 
    \frac{1}{2}\\
    3
  \end{pmatrix} 
\} \; \text{base}
  \] 
  Per ogni \( v = \begin{pmatrix} v_1\\v_2 \end{pmatrix} \in \mathbb{R}^2 \), esistono
  \( \alpha_1, \alpha_2 \in \mathbb{R} \) tali che:
  \[
  v = \alpha_1 b_1 + \alpha_2 b_2
  \] 
  \[
  = \alpha_1 \begin{pmatrix} 
    -1\\
    2
  \end{pmatrix} 
  +
  \alpha_2 \begin{pmatrix} 
    \frac{1}{2}\\
    3
  \end{pmatrix} 
  \] 
  \[
  = \begin{pmatrix} 
    -\alpha_1 + \frac{1}{2}\alpha_2\\
    2 \alpha_1 + 3 \alpha_2
  \end{pmatrix} 
  \] 
  \[
  = \begin{pmatrix} 
    -1 & \frac{1}{2}\\
    2 & 3
  \end{pmatrix} 
  \begin{pmatrix} 
    \alpha_1\\
    \alpha_2
  \end{pmatrix} 
  \] 
  Quindi \( C_{\mathcal{B}}(v) = \begin{pmatrix} \alpha_1\\\alpha_2 \end{pmatrix}  \) 
  è soluzione del sistema lineare \( Ax = v \) dove:
  \[
  A = \begin{pmatrix} 
    -1 & \frac{1}{2}\\
    2 & 3
  \end{pmatrix} 
  =
  \begin{pmatrix} b_1 & b_2 \end{pmatrix} 
  \] 
  Siccome \( \mathcal{B} \) è una base, \( \alpha_1 \) e \( \alpha_2 \) sono univocamente
  determinati e quindi \( Ax = v \) ha soluzione per ogni \( v \in \mathbb{R}^2 \).
  Per il teorema \ref{th:4.2}, \( A \) è invertibile e
  \[
  \begin{pmatrix} 
    \alpha_1\\
    \alpha_2
  \end{pmatrix} 
  = A^{-1}v
  \] 
  Calcolando \( A^{-1} \) 
  \[
    \begin{pmatrix}[cc|cc]
      -1 & \frac{1}{2} & 1 & 0\\
      2 & 3 & 0 & 1
  \end{pmatrix} 
  \sim
  \ldots
  \sim
  % \begin{pmatrix}[cc|cc]
  %   1 & 0 & \frac{3}{4} & \frac{1}{8}\\
  %   0 & 1 & \frac{1}{2} & \frac{1}{4}
  % \end{pmatrix} 
  \left(
  \begin{array}{cc}
    1 & 0\\
    0 & 1
  \end{array}
  \right|
  \underbrace{
    \left.
      \begin{array}{cc}
        \frac{3}{4} & \frac{1}{8}\\
        \frac{1}{2} & \frac{1}{4}
      \end{array}
    \right)
  }_{A^{-1}}
  \] 
  Dunque, per ogni \( v = \begin{pmatrix} v_1\\v_2 \end{pmatrix} \in \mathbb{R}^2 \),
  \[
    C_{\mathcal{B}}(v) = \begin{pmatrix} \alpha_1\\\alpha_2 \end{pmatrix} =
    A^{-1}v =
  \] 
  \[
    =
    \begin{pmatrix} 
      \frac{3}{4} & \frac{1}{8}\\
      \frac{1}{2} & \frac{1}{4}
    \end{pmatrix} 
    \begin{pmatrix} v_1\\v_2 \end{pmatrix} 
    =
    \begin{pmatrix} 
      \frac{3}{4}v_1 + \frac{1}{8}v_2\\
      \frac{1}{2}v_1 + \frac{1}{4}v_2
    \end{pmatrix} 
  \] 
\end{example}

\noindent In generale, per una base \( \mathcal{B} = \{b_1, \ldots, b_n\}  \) di
\( \mathbb{K}^n \), la matrice \( A = \begin{pmatrix} b_1, \ldots, b_n \end{pmatrix}  \) è
invertibile e \( C_{\mathcal{B}} = f_{A^{-1}} \). Dunque \( C_{\mathcal{B}}: \mathbb{K}^n
\to \mathbb{K}^n\) è un isomorfismo con inversa \( f_A \).

\subsection{Teorema}
Sia \( V \) uno spazio vettoriale su \( \mathbb{K} \) con base \( \mathcal{B} = \{b_1,
\ldots, b_n\}  \). L'applicazione lineare \( C_{\mathcal{B}}: V \to \mathbb{K}^n  \) è
un isomorfismo.

\subsubsection{Dimostrazione}
Definiamo \( g_{\mathcal{B}}: \mathbb{K}^n \to V \),
\[
  g_{\mathcal{B}}\left( \begin{pmatrix} \alpha_1\\\vdots\\\alpha_n \end{pmatrix}  \right)=
  \alpha_1 b_1 + \ldots + \alpha_n b_n
\] 
Mostriamo che \( g_{\mathcal{B}} \) è l'inversa di \( C_{\mathcal{B}} \). Infatti:
\[
  C_{\mathcal{B}}\left(g_{\mathcal{B}}\left( \begin{pmatrix} 
      \alpha_1\\
      \vdots\\
      \alpha_n
\end{pmatrix}  \right) \right) = C_{\mathcal{B}}(\alpha_1 b_1 + \ldots + \alpha_n b_n) =
\begin{pmatrix} \alpha_1\\\vdots\\\alpha_n \end{pmatrix} 
\] 
Per ogni \( v = \alpha_1 b_1 + \ldots + \alpha_n b_n \in V \),
\[
  g_{\mathcal{B}}\left( C_{\mathcal{B}}(v) \right) = g_{\mathcal{B}}\left( \begin{pmatrix} 
      \alpha_1\\
      \vdots\\
      \alpha_n
  \end{pmatrix}  \right) = \alpha_1 b_1 + \ldots + \alpha_n b_n = v
\] 
Dunque \( g_{\mathcal{B}} = C_{\mathcal{B}}^{-1} \quad \square\) 

\subsection{Osservazione}
Se \( f: V \to W \) è un isomorfismo e \( \mathcal{B} = \{b_1, \ldots, b_n\}  \) è una
base di \( V \), allora \( \{f(b_1), \ldots, f(b_n)\}  \) è una base di \( W \). In
particolare, \( dim_{\mathbb{K}}(V) = dim_{\mathbb{K}}(W) \).

\subsection{Corollario}
Due spazi vettoriali \( V \) e \( W \) sono isomorfi se e solo se:
\[
  dim_{\mathbb{K}}(V) = dim_{\mathbb{K}}(W)
\] 

\subsubsection{Dimostrazione}
Se \( f: V \to W \) è un isomorfismo, allora \( dim_{\mathbb{K}}(V) = dim_{\mathbb{K}}(W) \) 

\vspace{1em}
\noindent Supponiamo che \( V \) , \( W \) sono spazi vettoraili tali che:
\[
dim_{\mathbb{K}}(V) = dim_{\mathbb{K}}(W) = n
\] 
Allora esiste una base di \( V \) \( \mathcal{B} = \{b_1, \ldots, b_n\}  \) e
esiste una base di \( W \) \( \mathcal{C} = \{c_1, \ldots, c_n\}  \).
Consideriamo \( C_{\mathcal{B}}: V \to \mathbb{K}^n \) e \( C_{\mathcal{C}}: W \to \mathbb{K}^n \).
Notiamo che abbiamo:
\begin{figure}[H]
  \centering
  \begin{tikzcd}
    V \arrow[dr, swap, "C_{\mathcal{B}}"] \arrow[rr, "C_{\mathcal{C}}^{-1} \circ C_{\mathcal{B}}"] & & W\\
                                                                                                   & \mathbb{K}^n \arrow[ur, swap, "C_{\mathcal{C}}^{-1}"] &
  \end{tikzcd}
  \caption{Diagramma delle implicazioni}
\end{figure}
L'applicazione lineare ha inversa:
\[
C_{\mathcal{B}}^{-1} \circ C_{\mathcal{C}}: W \to V
\] 
dove
\[
C_{\mathcal{B}}^{-1} \circ C_{\mathcal{C}}(w) = C_{\mathcal{B}}^{-1}(C_{\mathcal{C}}(w))
\] 
per ogni \( w \in W \). Infatti:
\[
  C_{\mathcal{C}}^{-1} \circ C_{\mathcal{B}}\left( C_{\mathcal{B}}^{-1} \circ C_{\mathcal{C}}(w) \right) =
  C_{\mathcal{C}}^{-1} \left( C_{\mathcal{B}} \left( C_{\mathcal{B}}^{-1} \left( C_{\mathcal{C}}(w) \right)  \right)  \right) 
\] 
\[
  = C_{\mathcal{C}}^{-1} \left( C_{\mathcal{C}}(w) \right) = w
\] 

\vspace{1em}
\[
  C_{\mathcal{B}}^{-1} \circ C_{\mathcal{C}}\left( C_{\mathcal{C}}^{-1} \circ C_{\mathcal{B}}(v) \right)
\] 
\[
= C_{\mathcal{B}}^{-1} \left( C_{\mathcal{C}} \left( C_{\mathcal{C}}^{-1} \left( C_{\mathcal{B}}(v) \right)  \right)  \right)
\] 
\[
= C_{\mathcal{B}}^{-1} \left( C_{\mathcal{B}}(v) \right) = v
\] 
Dunque \( V \) e \( W \) sono isomorfi. \( \quad \square \) 

\vspace{1em}
\noindent \textbf{NB}: Per ogni \( b_i \in \mathcal{B} \), abbiamo:
\[
  C_{\mathcal{C}}^{-1} \circ C_{\mathcal{B}}(b_i) = C_{\mathcal{C}}^{-1} \left(
    C_{\mathcal{B}}(b_i) \right) 
\] 
\[
  = C_{\mathcal{C}}^{-1} \left( \begin{pmatrix} 
      0\\
      \vdots\\
      0\\
      1\\
      0\\
      \vdots\\
      0
  \end{pmatrix}  \right) = C_i
\] 

\vspace{1em}
\[
  C_{\mathcal{C}}^{-1} \circ C_{\mathcal{B}}(\mathcal{B}) = \{C_{\mathcal{C}}^{-1} \circ C_{\mathcal{B}}(b_1),
    \ldots, C_{\mathcal{C}}^{-1} \circ C_{\mathcal{B}}(b_n)\}
\} 
\] 
\[
= \{C_1, \ldots, C_n\}  = \mathcal{C}
\] 

\subsection{Matrice del cambio di base}
\begin{example}
  \[
  V = \mathbb{K}^2
  \] 
  con basi:
  \[
  \mathcal{B} = \left\{
    \begin{pmatrix} 
      3\\-1
    \end{pmatrix} ,
    \begin{pmatrix} 
      -2\\2
    \end{pmatrix} 
  \right\} , \quad
  \mathcal{D} = \left\{
    \begin{pmatrix} 
      \frac{1}{2}\\-\frac{1}{2}
    \end{pmatrix} ,
    \begin{pmatrix} 
      1\\\frac{1}{2}
    \end{pmatrix}
  \right\}
  \] 
  Sia \( v \in V \). Dati i numeri \( \alpha_1, \alpha_2 \in \mathbb{K} \) tali che
  \[ v = \alpha_1b_1 + \alpha_2b_2 = \alpha_1 \begin{pmatrix} 
    3\\-1
  \end{pmatrix} +
  \alpha_2 \begin{pmatrix} 
    -2\\2
  \end{pmatrix} 
\] 
come possiamo determinare \( \beta_1, \beta_2 \in \mathbb{K} \) tali che:
\[
v = \beta_1d_1 + \beta_2d_2 = \beta_1 \begin{pmatrix} 
  \frac{1}{2}\\-\frac{1}{2}
\end{pmatrix} +
\beta_2 \begin{pmatrix} 
  1\\\frac{1}{2}
\end{pmatrix} \quad ?
\] 
\[
  \begin{pmatrix} \beta_1\\\beta_2 \end{pmatrix} = [v]_{\mathcal{D}} = C_{\mathcal{D}}(v)
  = C_{\mathcal{D}}\left( \alpha_1 b_1 + \alpha_2 b_2 \right)
\] 
\[
  = C_{\mathcal{D}}\left( C_{\mathcal{B}}^{-1} \left(\begin{pmatrix} 
      \alpha_1\\
      \alpha_2
    \end{pmatrix} 
  \right)  \right)  
\] 
\[
  = C_{\mathcal{D}} \circ C_{\mathcal{B}}^{-1} \left( [v]_{\mathcal{B}} \right)
\] 
\begin{figure}[H]
  \centering
  \begin{tikzcd}
    & \stackrel{= \mathbb{K}^2}{V} \arrow[dl, swap, xshift=-5, "C_{\mathcal{B}}"] \arrow[dr, "C_{\mathcal{D}}"] & \\
    \mathbb{K}^2 \arrow[rr, swap, "C_{\mathcal{D}} \circ C_{\mathcal{B}}"] \arrow[ur, swap, xshift=5, red, "C_{\mathcal{B}}^{-1}"] & & \mathbb{K}^2
  \end{tikzcd}
  \caption{Diagramma delle implicazioni}
\end{figure}

\noindent Per \ref{7.5}, \( C_{\mathcal{B}} \circ C_{\mathcal{B}}^{-1} = f_C \) per una matrice
\( C \), cioè per ogni \( \begin{pmatrix} \alpha_1\\ \alpha_2 \end{pmatrix} \in \mathbb{K}^2 \),
\[
  C_{\mathcal{D}} \circ C_{\mathcal{B}}^{-1} \left( \begin{pmatrix} 
      \alpha_1\\
      \alpha_2
  \end{pmatrix}  \right) = C \begin{pmatrix} 
    \alpha_1\\
    \alpha_2
  \end{pmatrix}
\] 
In questo esempio, abbiamo che \( C_{\mathcal{D}}: \mathbb{K}^2 \to \mathbb{K}^2 \) e
\( C_{\mathcal{B}}^{-1}: \mathbb{K}^2 \to \mathbb{K}^2 \) sono della forma:
\[
  C_{\mathcal{D}} = f_{A^{-1}} \quad e \quad C_{\mathcal{B}}^{-1} = f_B
\] 
dove:
\[
  B = \begin{pmatrix} 
    3 & -2\\
    -1 & 2
  \end{pmatrix} 
  \quad e \quad
  \begin{pmatrix} 
    \frac{1}{2} & 1\\
    -\frac{1}{2} & \frac{1}{2}
  \end{pmatrix} 
\] 
Allora 
\[
  f_{A^{-1}B} = f_{A^{-1}} \circ f_B = C_{\mathcal{D}} \circ C_{\mathcal{B}}^{-1} = f_C
\] 
Quindi \( C = A^{-1}B \). Calcolando \( A^{-1} \):
\[
  \underbrace{
  \left(
  \begin{array}{cc}
    \frac{1}{2} & 1\\
    -\frac{1}{2} & \frac{1}{2}
  \end{array}
  \right|
  }_{A}
    \left.
      \begin{array}{cc}
        1 & 0\\
        0 & 1
      \end{array}
    \right)
    \stackrel{EG}{\sim}
  \underbrace{
  \left(
  \begin{array}{cc}
    1 & 0\\
    0 & 1
  \end{array}
  \right|
  }_{I_2}
  \underbrace{
    \left.
      \begin{array}{cc}
        \frac{2}{3} & -\frac{3}{4}\\
        \frac{2}{3} & \frac{2}{3}
      \end{array}
    \right)
  }_{A^{-1}}
\] 
Abbiamo:
\[
  C = A^{-1}B = \begin{pmatrix} 
    \frac{2}{3} & -\frac{3}{4}\\
    \frac{2}{3} & \frac{2}{3}
  \end{pmatrix}
  \begin{pmatrix} 
    3 & -2\\
    -1 & 2
  \end{pmatrix}
  = \begin{pmatrix} 
    \frac{10}{3} & -4\\
    \frac{4}{3} & 0
  \end{pmatrix}
\] 
Allora, per ogni \( v \in V \), abbiamo:
\[
\begin{pmatrix} 
  \frac{10}{3} & -4\\
  \frac{4}{3} & 0
\end{pmatrix}[v]_{\mathcal{B}} = [v]_{\mathcal{D}}
\] 
\end{example}

\begin{theorem}
  Siano \( \mathcal{B} \{b_1, \ldots, b_n\}  \) e \( \mathcal{C} = \{c_1, \ldots, c_n\}  \)
  basi di uno spazio vettoriale \( V \). Esiste una matrice \( A_{\mathcal{B} \to \mathcal{C}} \) 
  tale che:
  \[
    [v]_{\mathcal{C}} = A_{\mathcal{B} \to \mathcal{C}}[v]_{\mathcal{B}}
  \] 
  Le colonne di \( A_{\mathcal{B} \to \mathcal{C}} \) sono i vettori \( [b_1]_{\mathcal{C}},
  \ldots, [b_n]_{\mathcal{C}}\). La matrice \( A_{\mathcal{B} \to \mathcal{C}} \) è detta
  \textbf{matrice del cambio di base \( \mathcal{B} \to \mathcal{C} \)}.
\end{theorem}

\subsubsection{Dimostrazione}
\begin{figure}[H]
  \centering
  \begin{tikzcd}
    & V \arrow[dl, swap, "C_{\mathcal{B}}"] \arrow[dr, "C_{\mathcal{C}}"] &\\
    \mathbb{K}^n \arrow[rr, swap, "C_{\mathcal{C}} C_{\mathcal{B}}^{-1}"] & & \mathbb{K}^n
  \end{tikzcd}
  \caption{Diagramma delle implicazioni}
\end{figure}

Per \ref{7.2}, esiste una matrice \( A \) tale che \( C_{\mathcal{C}} C_{\mathcal{B}}^{-1} = f_A \).
Inoltre:
\[
 A_{\mathcal{B} \to \mathcal{C}} = A \stackrel{7.2}{=} \left( C_{\mathcal{C}} C_{\mathcal{B}}^{-1}(e_1) \ldots C_{\mathcal{C}} C_{\mathcal{B}}^{-1}(e_n) \right)
\] 
\[
  = \left( C_{\mathcal{C}}(b_1) \ldots C_{\mathcal{C}}(b_n) \right)
\] 
\[
  = \left( [b_1]_{\mathcal{C}} \ldots [b_n]_{\mathcal{C}} \right) \quad \square
\] 
dove \( \{e_1, \ldots, e_n\}  \) è la base canonica.

\subsubsection{Osservazione}
L'applicazione lineare \( C_{\mathcal{C}} C_{\mathcal{B}}^{-1} \) è un isomorfismo con
inversa \( C_{\mathcal{B}} C_{\mathcal{C}}^{-1} \). Dunque per \ref{def:7.3}, la matrice
\( A_{\mathcal{B} \to \mathcal{C}} \) è invertibile e la sua inversa è la matrice del 
cambio di base \( A_{\mathcal{B} \to \mathcal{C}}^{-1} = A_{\mathcal{C} \to \mathcal{B}} \).

\begin{example}
  \[
    V = \mathbb{R}_2[x]=\{a_0 + a_1 x + a_2 x^2 \;|\; a_0,a_1,a_2 \in \mathbb{R}\} 
  \] 
  \[
  \mathcal{B} = \{1 + x, 1 + x^2, x + x^2\} \quad \mathcal{C} = \{1, x, x^2\} 
  \] 
  \textbf{NB}: \[ C_ \mathcal{C}(a_0 + a_1 x + a_2 x^2) = \begin{pmatrix} 
    a_0\\
    a_1\\
    a_2
  \end{pmatrix}  \]

  \[
    A_{\mathcal{B} \to \mathcal{C}} =  \left( [b_1]_ \mathcal{C} \; [b_2]_ \mathcal{C}\; [b_3]_ \mathcal{C} \right) 
  \] 
  \[
   = \left( C_ \mathcal{C}(b_1) C_ \mathcal{C}(b_2) C_ \mathcal{C}(b_3) \right)
  \] 
  \[
  = \left( C_ \mathcal{C}(1+x) C_ \mathcal{C}(1+x^2) C_ \mathcal{C}(x+x^2) \right)
  \] 
  \[
  = 
  \begin{pmatrix} 
    1 & 1 & 0\\
    1 & 0 & 1\\
    0 & 1 & 1
  \end{pmatrix} 
  \] 
  Per ogni \( a_0 + a_1 x + a_2 x^2 \):
  \[
  \begin{pmatrix} 
    1 & 1 & 0\\
    1 & 0 & 1\\
    0 & 1 & 1
  \end{pmatrix} 
  \begin{pmatrix} 
    a_0\\
    a_1\\
    a_2
  \end{pmatrix} 
  =
  \begin{pmatrix} 
    a_0 + a_1\\
    a_0 + a_2\\
    a_1 + a_2
  \end{pmatrix} 
  =
  [a_0 + a_1 x + a_2 x^2]_ \mathcal{C}
  \] 
  \vspace{1em}
  \[
  a_0 + a_1 x + a_2 x^2 = (a_0 + a_1)(1+x) + (a_0 + a_2)(1+x^2) + (a_1 + a_2)(x+x^2)
  \] 
\end{example}

\subsection{Matrice associata a f rispetto a basi}
\begin{example}
  \[
    U = \mathbb{R}_2[x], \quad V = \mathbb{R}^2
  \] 
  \[
  f: U \to V \; \text{tale che } f(p) = \begin{pmatrix} p(0)\\p(1) \end{pmatrix}
  \] 
  Ovvero, per ogni \( p = a_0 + a_1x + a_2 x^2 \),
  \[
    f(p) = f(a_0 + a_1 x + a_2 x^2) = \begin{pmatrix} 
      a_0 + a_1 \cdot 0 + a_2 \cdot 0\\
      a_0 + a_1 \cdot 1 + a_2 \cdot 1
    \end{pmatrix} 
    =
    \begin{pmatrix} 
      a_0\\
      a_0 + a_1 + a_2
    \end{pmatrix} 
  \] 
  Abbiamo:
  \[
  \mathcal{C} = \{1,x,x^2\} \; \text{base di } U
  \] 
  \[
  \mathcal{B} = \{
    \begin{pmatrix} 1\\1 \end{pmatrix} ,
    \begin{pmatrix} 1\\0 \end{pmatrix} 
  \} \; \text{base di } V
  \] 
\begin{figure}[H]
  \centering
  \begin{tikzcd}
    U \arrow[d, swap, xshift=-5, "C_{\mathcal{C}}"] \arrow[r, "f"]& V \arrow[d, "C_{\mathcal{B}}"] \\
    \mathbb{R}^3 \arrow[r, swap, "C_{\mathcal{B}} \circ f \circ C_{\mathcal{C}}^{-1}"] \arrow[u, red, xshift=5, swap, "C_{\mathcal{C}}^{-1}"] & \mathbb{R}^2
  \end{tikzcd}
  \caption{Diagramma delle implicazioni}
\end{figure}

\noindent Per \ref{7.2} esiste una matrice \( A \) associata a \( C_{\mathcal{B}} \circ f \circ
C_{\mathcal{C}}^{-1}\) rispetto alla base canonica:
\[
  C_{\mathcal{B}} \circ f \circ C_{\mathcal{C}}^{-1} = f_A
\] 
dove:
\[
A = \begin{pmatrix} 
  C_{\mathcal{B}} \circ f \circ C_{\mathcal{C}}^{-1}(e_1) &
  C_{\mathcal{B}} \circ f \circ C_{\mathcal{C}}^{-1}(e_2) &
  C_{\mathcal{B}} \circ f \circ C_{\mathcal{C}}^{-1}(e_3)
\end{pmatrix} 
\] 
\[
= \begin{pmatrix} 
  C_{\mathcal{B}} \circ f (1) &
  C_{\mathcal{B}} \circ f (x) &
  C_{\mathcal{B}} \circ f (x^2)
\end{pmatrix} 
\] 
\[
= \begin{pmatrix} 
  C_{\mathcal{B}}\left( \begin{pmatrix} 1\\1 \end{pmatrix}  \right) &
  C_{\mathcal{B}}\left( \begin{pmatrix} 0\\1 \end{pmatrix}  \right) &
  C_{\mathcal{B}}\left( \begin{pmatrix} 0\\1 \end{pmatrix}  \right)
\end{pmatrix} 
\] 
Osserviamo \[ \begin{pmatrix} 1\\1 \end{pmatrix} = 1 \begin{pmatrix} 1\\1 \end{pmatrix} 
+ 0 \begin{pmatrix} 1\\0 \end{pmatrix}  \]
\[
\begin{pmatrix} 0\\1 \end{pmatrix} = 1 \begin{pmatrix} 1\\1 \end{pmatrix}
- 1 \begin{pmatrix} 1\\0 \end{pmatrix}
\] 
per ogni \( p \in U \), \[ [f(p)]_{\mathcal{B}} = \begin{pmatrix} 
  1 & 1 & 1\\
  0 & -1 & -1
\end{pmatrix}[p]_{\mathcal{C}}  \]

\vspace{1em}
Un esempio con \( p = 3 + 2x - x^2 \) \( \quad f(p)? \) 
\[
  [p]_{\mathcal{C}} = \begin{pmatrix} 
    3\\
    2\\
    -1
  \end{pmatrix}
\] 
\[
  [f(p)]_{\mathcal{B}} = \begin{pmatrix} 
    1 & 1 & 1\\
    0 & -1 & -1
  \end{pmatrix}
  \begin{pmatrix} 
    3\\
    2\\
    -1
  \end{pmatrix}
  = \begin{pmatrix} 
    4\\
    -1
  \end{pmatrix}
\] 
\[
f(p) = 4b_1 - b_2 = 4 \begin{pmatrix} 1\\1 \end{pmatrix} - \begin{pmatrix} 1\\0 \end{pmatrix}
= \begin{pmatrix} 
  4 - 1\\
  4
\end{pmatrix}  = \begin{pmatrix} 
3\\4
\end{pmatrix} 
\] 
\end{example}

\begin{theorem}
  Siano \( U,V \) spazi vettoriali su \( \mathbb{K} \), \( f: U \to V \), 
  \( \mathcal{C} = \{c_1, \ldots, c_n\}  \) base di \( U \), \( \mathcal{B} = \{b_1, \ldots, b_m\}  \)
  base di \( V \).

  \noindent Esiste una matrice \( A \in M_{m \times n}(\mathbb{K}) \) tale che \( A[u]_{\mathcal{C}} = [f(u)]_{\mathcal{B}} \) 
  per ogni \( u \in U \). \( A \) è detta \textbf{matrice associata a \( f \) rispetto
  alla base \( \mathcal{C} \) di \( U \) e la base \( \mathcal{B} \) di \( V \)}.
  Le sue colonne sono \( [f(c_1)]_{\mathcal{B}}, \ldots, [f(c_n)]_{\mathcal{B}} \).
\end{theorem}

\begin{example}
  Definiamo l'applicazione lineare \( id: V \to V \) come \( id(v) = v \) per ogni
  \( v \in V \). Allora la matrice associata a \( id \) rispetto alla base \( \mathcal{B} \)
  e alla base \( \mathcal{C} \) di \( V \) è la matrice del cambio di base \( A_{\mathcal{C} \to \mathcal{B}} \) 
\end{example}

\section{Rango + nullità}
\subsection{Definizione}
Sia \( f: V \to W \) un'applicazione lineare. Allora:
\[
N(f) := \{v \in V \;|\; f(v) = 0_w\} 
\] 
è un sottospazio di \( V \), detto \textbf{spazio nullo di \( f \)}. Inoltre \( Im(f) =
\{f(v) \;|\; v \in V\} \) è un sottospazio di \( W \) detto \textbf{immagine di \( f \)}.

\subsubsection{Esempi}
\begin{example}
  \[
    A = (a_{ij})_{1 \le i \le m, 1 \le j \le n} \in M_{m \times n}(\mathbb{K})
  \] 
  \[
  f_A: \mathbb{K}^n \to \mathbb{K}^m
  \] 
  \[
  N(f_A) = \{v \in \mathbb{K}^n \;|\; Av = 0\} = N(A)
  \] 
  \[
  Im(f_A) = \{Av \in \mathbb{K}^m \;|\; v \in \mathbb{K}^n\} =
  \] 
  \[
  = \left\{ \begin{pmatrix} 
      a_{11} & \ldots & a_{1n}\\
      \vdots & \ddots & \vdots\\
      a_{m1} & \ldots & a_{mn}
  \end{pmatrix} \begin{pmatrix} 
  v_1 \\
  \vdots \\
  v_n
  \end{pmatrix} =
  \begin{pmatrix} 
    a_{11}v_1 + \ldots + a_{1n}v_n\\
    \vdots\\
    a_{m1}v_1 + \ldots + a_{mn}v_n
  \end{pmatrix} 
  |
  \begin{pmatrix} 
    v_1\\
    \vdots\\
    v_n
  \end{pmatrix} 
  \in \mathbb{K}^n \right\}
  \] 
  \[
  = \left\{ \begin{pmatrix} 
      a_{11}v_1\\
      \vdots\\
      a_{m1}v_1
  \end{pmatrix}
  +
  \ldots
  +
  \begin{pmatrix} 
    a_{1n}v_n\\
    \vdots\\
    a_{mn}v_n
  \end{pmatrix}
  =
  \right.
\]
\[
  \left.
  v_1 \begin{pmatrix} 
    a_{11}\\
    \vdots\\
    a_{m1}
  \end{pmatrix}
  + 
  \ldots
  +
  v_n \begin{pmatrix} 
    a_{1n}\\
    \vdots\\
    a_{mn}
  \end{pmatrix}
  \; \Big | \;
  v_1, \ldots, v_n \in \mathbb{K} \right\} = C(A)
  \] 
  (Combinazioni lineari delle colonne di \( A \) )
\end{example}

\begin{example}
  \[
    V = \mathbb{R}_2[x], \quad W = \mathbb{R}^2, \quad f(p) = \begin{pmatrix} p(0)\\p(1) \end{pmatrix}
  \] 
  \[
  p = a_0 + a_1x + a_2x^2, \quad f(p) = \begin{pmatrix} 
    a_0\\
    a_0 + a_1 + a_2
  \end{pmatrix} 
  \] 
  \[
  N(f) = \{p = a_0 + a_1x + a_2x^2 \;|\; f(p) = \begin{pmatrix} a_0\\a_0+a_1+a_2 \end{pmatrix} = \begin{pmatrix} 0\\0 \end{pmatrix}  \} 
  \] 
  \[
  = \{ p = a_0 + a_1x + a_2x^2 \;|\; a_0 = 0, a_1+a_2 = 0\} 
  \] 
  \[
  Im(f) = \left\{f(p) = \begin{pmatrix} 
      a_0\\
      a_0 + a_1 + a_2
  \end{pmatrix} \Big |\; p = a_0 + a_1x + a_2x^2 \right\} 
  \] 
  \[
  = \left\{\begin{pmatrix} a_0\\
  a_0 + a_1 + a_2\end{pmatrix} \Big |\; a_0, a_1, a_2 \in \mathbb{R} \right\} 
  \] 
\end{example}

\begin{example}
 \[
 i: \mathbb{R}^2 \to \mathbb{R}^3
 \]  
 \[
 i \left( \begin{pmatrix} 
     v_1\\
     v_2
 \end{pmatrix}  \right) = \begin{pmatrix} 
     v_1\\
     v_2\\
     0
  \end{pmatrix}
 \] 
 \[
 N(i) = \left\{ \begin{pmatrix} 
     v_1\\
     v_2
 \end{pmatrix} \in \mathbb{R}^2 \;\Big |\; i \left( \begin{pmatrix} 
   v_1\\
   v_2
\end{pmatrix}  \right) = 
\begin{pmatrix} 
  v_1\\
  v_2\\
  0
\end{pmatrix} 
=
\begin{pmatrix} 
  0\\
  0\\
  0
\end{pmatrix} 
\right\} 
 \] 
 \[
 = \left\{ \begin{pmatrix} 
     v_1\\
     v_2
 \end{pmatrix} \in \mathbb{R}^2 
 \; \Big | \; v_1 = v_2 = 0
\right\} 
 \] 
 \[
 = \left\{ \begin{pmatrix} 
     0\\
     0
 \end{pmatrix}  \right\} 
 \] 
 \vspace{1em}
 \[
 Im(i) = \left\{ i \left( \begin{pmatrix} 
      v_1\\
      v_2
 \end{pmatrix}  \right) =
 \begin{pmatrix} 
   v_1\\
   v_2\\
   0
 \end{pmatrix} 
 \; \Big | \; \begin{pmatrix} 
   v_1\\
   v_2
  \end{pmatrix} \in \mathbb{R}^2 \right\}
 \] 
 \[
 = \left\{ \begin{pmatrix} 
     v_1\\
     v_2\\
     v_3
 \end{pmatrix} 
 \; \Big | \; v_3 = 0
\right\} 
 \] 
\end{example}

\end{document}
