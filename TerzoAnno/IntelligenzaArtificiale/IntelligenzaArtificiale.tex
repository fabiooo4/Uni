\documentclass[a4paper]{article}
\usepackage{import}
\input{../../preamble.sty}

\begin{document}

\input{title.tex}

\tableofcontents
\pagebreak

\section{Introduzione}
Nel 1950 Alan Turing pubblica un articolo intitolato "Computing Machinery and Intelligence"
in cui propone un esperimento per determinare se una macchina può essere considerata
intelligente. L'esperimento, noto come "test di Turing", coinvolge un interrogatore umano
che comunica con due entità nascoste: una macchina e un essere umano. L'interrogatore deve
fare domande a entrambe le entità e, basandosi sulle risposte, deve determinare quale delle
due è la macchina. Se l'interrogatore non riesce a distinguere tra le risposte
della macchina e quelle dell'essere umano, la macchina è considerata intelligente.

\vspace{1em}
\noindent
In futuro l'attenzione si è spostata sulla ricerca di metodi per risolvere problemi che richiedono intelligenza
umana, utilizzando algoritmi e modelli matematici fino ad arrivare alle reti neurali e intelligenza artificiale.

\begin{definition}
  L'intelligenza artificiale è una disciplina che studia come \textbf{simulare} l'intelligenza umana in
  scenari complessi
\end{definition}

\subsection{Tipi di intelligenza artificiale}
\subsubsection{Autonomous agents}
Sono sistemi che percepiscono l'ambiente e agiscono in modo autonomo per raggiungere obiettivi specifici.

\subsubsection{Data analysis}
Utilizzo di algoritmi per analizzare grandi quantità di dati e estrarre informazioni utili e correlazioni
complesse.

\subsubsection{Machine Learning}
È lo sviluppo di algoritmi che permettono a dei modelli di apprendere dai dati
di esempio e migliorare le loro prestazioni nel tempo senza essere esplicitamente programmati.
Ad esempio riconoscimento di immagini.

L'apprendimento automatico è diviso in tre categorie principali:
\begin{itemize}
  \item \textbf{Unsupervised learning}: il modello viene addestrato su un insieme di dati non etichettati,
  dove l'obiettivo è scoprire strutture nascoste o pattern nei dati senza avere risposte corrette predefinite.
\item \textbf{Supervised learning}: il modello viene addestrato su un insieme di dati etichettati,
  dove ogni esempio di input è associato a una risposta corretta. L'obiettivo è che il modello impari a
  mappare gli input alle risposte corrette.
  \item \textbf{Reinforced learning}: il modello impara attraverso interazioni con l'ambiente, ricevendo
  ricompense o penalità in base alle azioni intraprese. L'obiettivo è massimizzare la ricompensa totale nel
  tempo.
\end{itemize}

\subsubsection{Time series analysis}
L'analisi delle serie temporali è un'area dell'apprendimento automatico che si concentra sull'analisi di dati
collezionati nel tempo. Le serie temporali sono sequenze di dati misurati a intervalli regolari, come
temperatura giornaliera, prezzi delle azioni o dati di vendita mensili. L'obiettivo dell'analisi delle
serie temporali è identificare pattern, tendenze e stagionalità nei dati per fare previsioni future.

\vspace{1em}
\noindent
Gli approcci comuni per l'analisi delle serie temporali includono:
\begin{itemize}
  \item \textbf{Riconoscimento di anomalie e cause}:
    è un processo di identificazione di dati o eventi che si discostano
    significativamente dal comportamento normale o atteso. Queste anomalie possono indicare problemi,
    errori o situazioni insolite che richiedono attenzione.

  \item \textbf{Generative transformers}:
    sono una classe di modelli che permettono di predirre il prossimo elemento in una
    sequenza di dati partendo dagli elementi precedenti, come ad esempio la parola successiva in una frase o il
    pixel successivo in un'immagine. Si sfrutta il concetto di \textbf{attenzione} per pesare l'importanza
    relativa delle diverse parti della sequenza di input durante la generazione dell'output.
\end{itemize}

\subsubsection{Intelligent Agents}
Un agente intelligente è un sistema che percepisce l'ambiente circostante attraverso sensori e agisce su
l'ambiente per raggiungere un obiettivo specifico. Gli elementi chiave di un agente intelligente includono:
\begin{itemize}
  \item \textbf{Performance measure}: misura il successo dell'agente nel raggiungere i suoi obiettivi
  \item \textbf{Rationality}: l'agente deve agire in modo da massimizzare la sua performance measure attesa
\end{itemize}

\subsection{Markov Decision Process (MDP)}
Un MDP è un modello matematico utilizzato per rappresentare problemi di decisione sequenziali. Gli elementi
principali sono:
\begin{itemize}
  \item \textbf{State}: rappresenta l'ambiente in un dato momento
  \item \textbf{Actions}: insieme delle azioni che l'agente può intraprendere
  \item \textbf{Transition model}: effetto che le azioni hanno sull'ambiente (potrebbero essere parzialmente incognite
    \[
      T: (state, action) \to next\_state
    \] 
  \item \textbf{Reward}: valore \textbf{immediato} dell'esecuzione di un'azione
    \[
      R: (state, action, next\_state) \to real\_number
    \] 
  \item \textbf{Policy}: strategia che l'agente utilizza per decidere quale azione intraprendere in ogni stato
    con l'obiettivo di massimizzare la ricompensa totale attesa nel tempo
    \[
      \pi: (state) \to action
    \] 
\end{itemize}

\subsection{Generative AI}
L'intelligenza artificiale generativa si riferisce a una classe di modelli di intelligenza artificiale
che sono in grado di generare nuovi contenuti, come testo, immagini, musica o video, a partire da dati di
addestramento. Questi modelli hanno miliardi di parametri e sono \textbf{preaddestrati} su grandi quantità
di dati. In sostanza questi modelli "predicono il futuro" basandosi sui dati su cui sono stati addestrati e
un \textbf{propmpt} (input dell'utente).

\section{Agenti e ambiente}
Gli agenti includono umani, robot, softbot, termostati ecc... La funzione dell'agente
mappa lo storico di percezioni in azioni:
\[
  f: \mathcal{P}^* \mapsto \mathcal{A}
\] 
Il programma dell'agente è eseguito su architettura fisica per produrre la funzione \( f \).
\begin{example}
  Un esempio potrebbe essere un insieme di stanze \( \{A, B\}  \) e un robot aspirapolvere che può
  percepire la sua posizione e il contenuto della stanza. L'agente potrebbe quindi percepire
  \( [A, Sporco] \) se ci fosse dello sporco nella stanza A. Le azioni potrebbero essere
  di movimento o pulizia. Tutto questo dipende dalla squenza di percezioni, ad esempio 
  in una tabella:
  \begin{table}[H]
    \centering
    \begin{tabular}{|c|c|}
      \hline
      Percezione & Azione \\
      \hline
      \( [A, Pulito] \) & Vai a B \\
      \( [A, Sporco] \) & Pulisci \\
      \( [B, Pulito] \) & Vai ad A \\
      \( [B, Sporco] \) & Pulisci \\
      \( [A, Pulito], [A, Pulito] \) & Vai a B \\
      \( [A, Pulito], [A, Sporco] \) & Pulisci \\
      \hline
    \end{tabular}
    \caption{Esempio di tabella di percezioni e azioni}
  \end{table}
  \noindent
  Non possiamo dire se questa è una funzione corretta perchè non abbiamo una
  \textbf{performance measure} che ci dica se l'agente sta facendo un buon lavoro.
\end{example}

\begin{definition}
  Se un agente ha \( \left| \mathcal{P} \right|  \) possibili percezioni, allora al tempo
  \( T \) avrà:
  \[
    \sum_{t=1}^{T} \left| \mathcal{P} \right|^t
  \] 
\end{definition}
\noindent
Se lo storico di percezioni è irrilevante, cioè se ad ogni percezione è associata un'azione
la funzione viene chiamata \textbf{Reflex}.

\subsection{Razionalità}
Per definire l'intelligenza di un agente si utilizza una misura di performance che
valuta la sequenza di percezioni.
\begin{example}
  Tornando all'esempio del robot aspirapolvere si potrebbero assegnare i seguenti
  punteggi:
  \begin{itemize}
    \item Un punto per ogni stanza pulita per ogni unità di tempo
    \item Meno un punto per ogni mossa
    \item Penalizzazione per ogni stanza sporca
  \end{itemize}
\end{example}

\begin{example}
  Un altro esempio è il seguente ambiente:
  \begin{itemize}
    \item Ci sono 3 stanze \( (A,B,C) \) e due robot \( (r_1, r_2) \) 
    \item \( r_1 \) può sorvegliare solo \( A \) e \( B \) e \( r_2 \) solo \( B \) e \( C \)
    \item \( r_1 \) inizia dalla stanza \( A \) e \( r_2 \) dalla \( C \)
    \item Il tempo di percorrenza tra le stanze è 0
    \item Performance measure: minimizza il tempo in cui una stanza non è sorvegliata,
      cioè il tempo totale in cui una stanza non è visitata da nessun robot
  \end{itemize}

  Un possibile comportamento razionale potrebbe essere il seguente (alternata):
  \begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c||c|c|}
      \hline
      Stato & \( A \) & \( B \) & \( C \) & Tempo  \\
      \hline
      \( [A, C] \) & 0 & 1 & 0 & 1 \\
      \( [B, C] \) & 1 & 0 & 0 & 2 \\
      \( [A, C] \) & 0 & 1 & 0 & 3 \\
      \( [A, B] \) & 0 & 0 & 1 & 4 \\
      \hline
      Average idleness & \( \frac{1}{4} \)  & \( \frac{1}{2} \)  & \( \frac{1}{4} \)  & Tot: \( \frac{1}{3} \)  \\
      \hline
    \end{tabular}
  \end{table}
  \noindent
  Un altro comportamento potrebbe essere (fissata):
  \begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c||c|c|}
      \hline
      Stato & \( A \) & \( B \) & \( C \) & Tempo  \\
      \hline
      \( [A, C] \) & 0 & 1 & 0 & 1 \\
      \( [B, C] \) & 1 & 0 & 0 & 2 \\
      \( [A, C] \) & 0 & 1 & 0 & 3 \\
      \( [B, C] \) & 1 & 0 & 0 & 4 \\
      \hline
      Average idleness & \( \frac{1}{2} \)  & \( \frac{1}{2} \)  & \( 0 \)  & Tot: \( \frac{1}{3} \)  \\
      \hline
    \end{tabular}
  \end{table}
  \noindent
  Entrambi i comportamenti hanno la stessa performance measure, ma il primo è migliore
  del secondo perchè penalizza meno una singola stanza rispetto alle altre. Per capirlo
  bisogna non solo minimizzare la performance measure, ma anche minimizzare la varianza.
\end{example}

\subsection{PEAS}
Per progettare un agente intelligente bisogna definire l'ambiente in cui opera:
\begin{itemize}
  \item \textbf{Performance measure}: come viene valutato il successo dell'agente
  \item \textbf{Environment}: il contesto in cui l'agente opera
  \item \textbf{Actuators}: i mezzi attraverso cui l'agente agisce sull'ambiente
  \item \textbf{Sensors}: i mezzi attraverso cui l'agente percepisce l'ambiente
\end{itemize}

\begin{example}
  Prendiamo ad esempio un taxi automatico, il PEAS potrebbe essere:
  \begin{itemize}
    \item Performance measure:
      \begin{itemize}
        \item Soddisfazione del cliente
        \item Sicurezza
        \item Efficienza del carburante
        \item Rispetto delle leggi stradali
      \end{itemize}
    \item Environment:
      \begin{itemize}
        \item Traffico stradale
        \item Condizioni meteorologiche
        \item Segnali stradali
        \item Pedoni e altri veicoli
      \end{itemize}
    \item Actuators:
      \begin{itemize}
        \item Volante
        \item Acceleratore
        \item Freni
        \item Indicatori di direzione
      \end{itemize}
    \item Sensors:
      \begin{itemize}
        \item Telecamere
        \item Lidar
        \item Radar
        \item Sensori di velocità
        \item GPS
      \end{itemize}
  \end{itemize}
\end{example}

\subsection{Tipi di ambienti}
Gli ambienti possono essere classificati in base a diverse caratteristiche:
\begin{itemize}
  \item \textbf{Osservabile}: se l'agente può percepire completamente lo stato dell'ambiente
    in ogni momento
  \item \textbf{Deterministico}: se l'azione dell'agente determina in modo univoco il prossimo stato
    dell'ambiente
  \item \textbf{Episodico}: se l'esperienza dell'agente è divisa in episodi indipendenti,
    cioè l'azione in un episodio non influisce sugli episodi successivi
  \item \textbf{Statico}: se l'ambiente non cambia mentre l'agente sta prendendo una
    decisione
  \item \textbf{Discreto}: se l'insieme di stati, azioni e percezioni è finito o numerabile
  \item \textbf{Singolo agente}: se l'agente opera da solo nell'ambiente senza la presenza
    di altri agenti
\end{itemize}
\begin{example}
  Prendiamo ad esempio i seguenti ambienti provando a classificarli:
  \begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|}
      \hline
      & Crossword & Robo-selector & Poker & Taxi \\
      \hline
      Osservabile & Sì & Parziale & Parziale & Parziale \\
      Deterministico & Sì & No & No & No \\
      Episodico & No & Sì & No & No \\
      Statico & Sì & No & Sì & No \\
      Discreto & Sì & No & Sì & No \\
      Singolo agente & Sì & Sì & No & No \\
      \hline
    \end{tabular}
  \end{table}
\end{example}
Il tipo di ambiente cambia radicalmente la soluzione del problema:
\begin{itemize}
  \item \textbf{Deterministico, completamente osservabile}: Single-state problem
  \item \textbf{Completamente non osservabile}: Conformant problem, l'agente non sa in che
    stato si trova, ma potrebbe trovare una soluzione
  \item \textbf{Non deterministico e/o parzialmente osservabile}: Contingency problem,
    l'agente deve prevedere le possibili situazioni future e agire di conseguenza
  \item \textbf{Spazio degli stati sconosciuto}: Exploration problem, l'agente deve esplorare
    l'ambiente per scoprire gli stati e le azioni disponibili
\end{itemize}

\subsection{Agenti di problem solving}
È una forma ristretta di agenti che formulato un problema e un obiettivo partendo da uno stato
cerca una soluzione ignorando le percezioni, siccome ci si trova in un single-state problem.
Questo si chiama Offline problem solving perchè l'agente ha completa conoscenza dell'ambiente.
Online problem solving è quando l'agente non ha completa conoscenza dell'ambiente.
\begin{example}
  Il seguente è un esempio di problem solving agent:
  \begin{lstlisting}[language=Python]
function Simple-Problem-Solving-Agent(percept) returns action
  static: seq, an action sequence, initially empty
          state, some description of the current world state
          goal, a goal, initially null
          problem, a problem formulation

  state <- Update-State(state, percept)

  if seq is empty then
    goal <- Formulate-Goal(state)
    problem <- Formulate-Problem(state, goal)
    seq <- Search( problem)

  action <- First(seq)
  seq <- Rest(seq)
  return action
  \end{lstlisting}
\end{example}
\begin{example}
  Consideriamo il problema "Vacanze in Romania". Bisogna formulare un viaggio da Arad a
  Bucarest sapendo che l'aereo parte domani.
  \begin{itemize}
    \item \textbf{Goal}: Arrivare a Bucarest
    \item \textbf{Formulazione del problema}:
      \begin{itemize}
        \item Stati: città della Romania
        \item Azioni: volare tra le città
      \end{itemize}
    \item \textbf{Soluzione}: Sequenza di città
  \end{itemize}
  \noindent
  Si potrebbe usare una mappa per trovare il percorso più breve (visione completa del mondo)
  e trovare una soluzione ottimale.
  Questo problema è definito da 4 componenti:
  \begin{itemize}
    \item \textbf{Stato iniziale}: ad esempio "ad Arad"
    \item \textbf{Funzione di transizione}: insieme di coppie (stato, azione) che mappano
      uno stato in un altro, ad esempio:
      \[
        S(A) = \left\{ \left< \text{Arad} \to \text{Zerind}, \text{Zerind} \right>, \ldots \right\}
      \] 
    \item \textbf{Test dell'obiettivo}: una funzione che verifica se lo stato corrente
      soddisfa l'obiettivo, ad esempio:
      \[
        Goal\text{-}Test(s) = \begin{cases}
          \text{true} & \text{se } s = \text{Bucarest} \\
          \text{false} & \text{altrimenti}
        \end{cases}
      \]
    \item \textbf{Path cost}: è una funzione che assegna un costo (additivo) a ogni azione,
      ad esempio la somma di distanze o il numero di azioni:
      \[
        c(x, a, y) \ge 0
      \] 
    \item \textbf{Soluzione}: Una sequenza di azioni che portano dallo stato iniziale allo
      stato obiettivo.
  \end{itemize}
\end{example}

\section{Ricerca nello spazio degli stati}
\subsection{Ricerca generale}
\subsubsection{Tree search}
Un algoritmo di ricerca ad albero esplora lo spazio degli stati partendo dallo stato iniziale
e generando nuovi stati (successori) applicando le azioni disponibili, cioè 
\textbf{espandendo} gli stati:
\begin{lstlisting}[language=Python]
function Tree-Search(problem, strategy) runction Tree-Search( problem, strategy) returns a solution, or failure
initialize the search tree using the initial state of problem
loop do
if no candidates for expansion then return failure
choose a leaf node for expansion according to strategy
if node contains a goal state then return the solution
else add successor nodes to the search tree (expansion)
endeturns a solution, or failure
  initialize the search tree using the initial state of problem
  loop do
    if no candidates for expansion then return failure
      choose a leaf node for expansion according to strategy
    if node contains a goal state then return the solution
      else add successor nodes to the search tree (expansion)
end
\end{lstlisting}

\subsubsection{Stato e nodo}
Stato e nodo non sono la stessa cosa, infatti:
\begin{itemize}
  \item \textbf{Stato}: rappresenta una configurazione dell'ambiente
  \item \textbf{Nodo}: è una struttura dati che costituisce una parte dell'albero di ricerca
    e include informazioni aggiuntive come il genitore, l'azione che ha portato a quello stato,
    il costo del percorso o la profondità nell'albero, ecc...
\end{itemize}

\subsubsection{Tree search generale}
Espandere un nodo significa generare i suoi figli, cioè i nodi successori e tutti i nodi
non esplorati sono chiamati \textbf{frontiera}.
\begin{lstlisting}[language=Python]
function Tree-Search( problem, frontier) returns a solution, or failure
  frontier <- Insert(Make-Node(problem.Initial-State))
  while not IsEmty(frontier) do
    node <- Pop(frontier)
    if problem.Goal-Test(node.State) then return node
      frontier <- InsertAll(Expand(node, problem))
  end loop
  return failure
\end{lstlisting}
La strategia è quella di scegliere l'ordine in cui i nodi vengono espansi, cioè
come viene gestita la frontiera. Le strategie sono valutate in base a:
\begin{itemize}
  \item \textbf{Completezza}: se garantisce di trovare una soluzione quando esiste
  \item \textbf{Complessità di tempo}: numero di nodi generati o espansi
  \item \textbf{Complessità di spazio}: numero massimo di nodi memorizzati in memoria
  \item \textbf{Ottimalità}: se garantisce di trovare la soluzione migliore
\end{itemize}
Le complessità di spazio e di tempo sono misurate in termini di:
\begin{itemize}
  \item \( b \): maximum branching factor, numero massimo di figli per nodo
  \item \( d \): profondità della soluzione meno costosa
  \item \( m \): profondità massima dell'albero di ricerca (potrebbe essere infinita)
\end{itemize}

\subsubsection{Stati ripetuti}
Fallire nel riconoscere stati ripetuti può trasformare un problema lineare in un problema
esponenziale. Bisogna quindi mantenere una lista di stati già visitati e non espandere
nodi che portano a stati già visitati:
\begin{lstlisting}[language=Python]
function Graph-Search( problem, frontier) returns a solution, or failure
  explored <- an empty set
  frontier <- Insert(Make-Node(problem.Initial-State))
  while not IsEmty(frontier) do
    node <- Pop(frontier)
    if problem.Goal-Test(node.State) then return node
    if node.State is not in explored then
      add node.State to explored
      frontier <- InsertAll(Expand(node, problem))
    end if
  end loop
  return failure
\end{lstlisting}

\subsection{Ricerca non informata}
Gli algoritmi di ricerca non informata utilizzano soltanto i dati disponibili nella
definizione del problema e i principali sono:
\begin{itemize}
  \item Breadth-first search
  \item Uniform-cost search (Dijkstra)
  \item Depth-first search
  \item Depth-limited search
  \item Iterative deepening search
\end{itemize}

\subsubsection{Breadth-first search}
Questo algoritmo espande il nodo non esplorato più superficiale, cioè il nodo più vicino
alla radice. Utilizza una coda FIFO per la frontiera e i nuovi successori vengono
aggiunti alla fine della coda.
\begin{lstlisting}[language=Python]
function BFS( problem) returns a solution, or failure
  node <- node with State=problem.Initial-State,Path-Cost=0
  if problem.Goal-Test(node.State) then return node
  explored <- empty set frontier <- FIFO queue with node as the only element
  loop do
    if frontier is empty then return failure
    node <- Pop(frontier)
    add node.State to explored
    for each action in problem.Actions(node.State) do
      child <- Child-Node(problem,node,action)
      if child.State is not in (explored or frontier) then
        if problem.Goal-Test(child.State) then return child
        frontier <- Insert(child)
      end if
    end for
  end loop
\end{lstlisting}
\noindent
Questo tipo di ricerca è:
\begin{itemize}
  \item \textbf{Completa}: Sì, soltanto se \( b \) è finito, cioè se il branching factor
    è limitato
  \item \textbf{Complessità di tempo}: \( b + b^2 + b^3 + \ldots + b^d = O(b^d) \)
  \item \textbf{Complessità di spazio}: \( O(b^d) \), perchè bisogna memorizzare tutti i nodi
    generati
  \item \textbf{Ottimale}: Sì, soltanto se il costo delle azioni è uniforme
\end{itemize}

\subsubsection{Uniform-cost search}
Questo algoritmo espande il nodo non esplorato con il \textbf{costo del percorso più basso}.
La frontiera è una coda di priorità ordinata in base al costo del percorso.
Questo tipo di ricerca è:
\begin{itemize}
  \item \textbf{Completa}: Sì, se il costo minimo delle azioni \( \ge \varepsilon \) 
    (con piccola ma \( \varepsilon > 0 \))
  \item \textbf{Complessità di tempo}: Numero di nodi \( g \le  \) del costo del percorso
    ottimale \( C^* \). \( O(b^{1+\lfloor C^*/\epsilon \rfloor}) \)
  \item \textbf{Complessità di spazio}: \( O(b^{1+\lfloor C^*/\epsilon \rfloor}) \)
  \item \textbf{Ottimale}: Sì perchè i nodi vengono espansi in ordine di costo del percorso
\end{itemize}
Ci sono due modifiche principali rispetto alla BFS che garantiscono l'ottimalità:
\begin{enumerate}
  \item Il goal test viene fatto quando il nodo viene estratto dalla frontiera, non quando
    viene generato. (Questo elemento spiega il \( +1 \) nella complessità
  \item Controllare se un nodo generato è già presente nella frontiera con un costo più
    alto e in tal caso sostituirlo con il nuovo nodo a costo più basso
\end{enumerate}

\subsubsection{Depth-first search}
Questo algoritmo espande il nodo non esplorato più profondo, cioè il nodo più lontano
dalla radice. Utilizza una pila LIFO per la frontiera e i nuovi successori vengono
aggiunti all'inizio.
Questo tipo di ricerca è:
\begin{itemize}
  \item \textbf{Completa}: No, perchè può rimanere bloccata in un ramo infinito,
    a meno che l'albero di ricerca non abbia una profondità limitata. Si potrebbero
    evitare loop modificando l'algoritmo per evitare stati ripetuti sul percorso corrente
  \item \textbf{Complessità di tempo}: \( O(b^m) \), dove \( m \) è la profondità massima
    dell'albero di ricerca
  \item \textbf{Complessità di spazio}: \( O(bm) \), bisogna memorizzare soltanto il
    percorso corrente e i nodi fratelli
  \item \textbf{Ottimale}: No, perchè non garantisce di trovare la soluzione migliore
\end{itemize}

\subsubsection{Iterative deepening search}
Questo algoritmo combina i vantaggi della BFS e della DFS. Esegue una serie di ricerche
in profondità limitata, aumentando progressivamente il limite di profondità fino a
trovare una soluzione.
\begin{lstlisting}[language=Python]
# Depth-Limited Search
function DLS(problem, limit) returns soln/fail/cutoff
  R-DLS(Make-Node(problem.Initial-State), problem, limit)


function R-DLS(node, problem, limit) returns soln/fail/cutoff
  if problem.Goal-Test(node.State) then return node
  else if limit = 0 then return cutoff # raggiunta la profondita' massima
  else
    # flag: c'e' stato un cutoff in uno dei sottoalberi?
    cutoff-occurred? <- false
    for each action in problem.Actions(node.State) do
      child <- Child-Node(problem, node, action)
      result <- R-DLS(child, problem, limit-1)
      if result = cutoff then cutoff-occurred? <- true
      else if result 6 = failure then return result
    end for
    if cutoff-occurred? then return cutoff else return failure
  end else

# Iterative Deepening Search
function IDS(problem) returns a solution
  inputs: problem, a problem
  for depth <- 0 to infinity do
    result <- DLS(problem, depth)
    if result 6 = cutoff then return result
  end
\end{lstlisting}
Questo tipo di ricerca è:
\begin{itemize}
  \item \textbf{Completa}: Sì
  \item \textbf{Complessità di tempo}: \( db^1 + (d-1)b^2 + \ldots + b^d = O(b^d) \) 
  \item \textbf{Complessità di spazio}: \( O(bd) \) 
  \item \textbf{Ottimale}: Sì, se il costo delle azioni è uniforme
\end{itemize}

\begin{exercise}
  Assumi:
  \begin{enumerate}
    \item Un albero di ricerca ben bilanciato, tutti i nodi hanno lo stesso numero di figli
    \item Il goal state è l'ultimo che viene espanso nel suo livello (il più a destra)
    \item Se il branching factor è 3, la soluzione più superficiale è a profondità 3
      (la radice è a profondità 0) e si utilizza la ricerca in ampiezza quanti nodi
      vengono generati?
    \item Se il branching factor è 3, la soluzione più superficiale è a profondità 3
      (la radice è a profondità 0) e si utilizza la iterative deepening quanti nodi
      vengono generati?
  \end{enumerate}
\end{exercise}
\begin{exercise}
  Un uomo ha un lupo, una pecora e un cavolo. L'uomo è sulla riva di un fiume con una
  barca che può trasportare solo lui e un altro oggetto. Il lupo mangia la pecora e la
  pecora mangia il cavolo, quindi non può lasciarli insieme da soli.
  \begin{enumerate}
    \item Formalizza il problema come un problema di ricerca
    \item Usa BFS per risolvere il problema
  \end{enumerate}

  \vspace{1em}
  \noindent
  \textbf{Soluzione:}

  Formalizziamo gli stati come una tupla:
  \[
    <W, S, C, M, B>
  \] 
  dove:
  \begin{itemize}
    \item \( W \): posizione del lupo
    \item \( S \): posizione della pecora
    \item \( C \): posizione del cavolo
    \item \( M \): posizione dell'uomo
    \item \( B \): stato della barca
  \end{itemize}
  La posizione può essere \( 0 \) (left) o \( 1 \) (right).

  Lo stato iniziale è:
  \[
    <0, 0, 0, 0, 0>
  \] 
  Lo stato obiettivo è:
  \[
    <1, 1, 1, 1, 1>
  \]
  Le azioni possibili sono:
  \begin{itemize}
    \item Porta il lupo (CW)
    \item Porta la pecora (CS)
    \item Porta il cavolo (CC)
    \item Porta niente (CN)
  \end{itemize}
  \begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|}
      \hline
      Operatore & Precondizione & Funzione \\
      \hline
      \footnotesize CW & \footnotesize \( M = B, M = W, S \neq C \) & \footnotesize\( \left<W,S,C,M,B\right> \mapsto \left<\bar{W},S,C,\bar{M},\bar{B}\right> \)\\
      \footnotesize CS & \footnotesize \( M = B, M = S \) & \footnotesize\( \left<W,S,C,M,B\right> \mapsto \left<W,\bar{S},C,\bar{M},\bar{B}\right> \)\\
      \footnotesize CC & \footnotesize \( M = B, M = C, W \neq S \) & \footnotesize\( \left<W,S,C,M,B\right> \mapsto \left<W,S,\bar{C},\bar{M},\bar{B}\right> \)\\
      \footnotesize CN & \footnotesize \( M = B \) & \footnotesize\( \left<W,S,C,M,B\right> \mapsto \left<W,S,C,\bar{M},\bar{B}\right> \)\\
      \hline
    \end{tabular}
  \end{table}
  Notiamo che in tutte le precondizioni c'è \( M = B \) perchè l'uomo deve essere
  sempre con la barca, quindi si possono unire i due stati in uno solo \( M \).
\end{exercise}

\subsection{Ricerca informata}
Gli algoritmi di ricerca informata utilizzano informazioni aggiuntive (euristiche)
per guidare la ricerca verso la soluzione in modo più efficiente.

\subsubsection{Best-first search}
Questo algoritmo usa una \textbf{funzione di valutazione} per ogni nodo che stima la
"desiderabilità". La frontiera è una coda ordinata in ordine decrescente di desiderabilità.
A seconda di come viene definita la desiderabilità si ottengono diversi algoritmi:
\begin{itemize}
  \item Greedy best-first search
  \item A*
\end{itemize}

\subsubsection{Greedy best-first search}
Questo algoritmo espande il nodo che sembra essere il più vicino alla soluzione
secondo una funzione di valutazione euristica \( h(n) \) che stima il costo
rimanente per raggiungere l'obiettivo da un nodo \( n \).
\begin{example}
  In una mappa di una città, la funzione di valutazione potrebbe essere la distanza
  in linea d'aria dal nodo corrente alla destinazione. In questo modo, l'algoritmo
  esplora prima i nodi che sembrano più vicini alla destinazione, riducendo il numero
  di nodi esplorati rispetto a una ricerca non informata.
\end{example}
Questo tipo di ricerca è:
\begin{itemize}
  \item \textbf{Completa}: No, perchè può rimanere bloccata in un ciclo infinito. È
    completo se lo spazio di ricerca è finito e ci sono controlli per evitare stati
    ripetuti
  \item \textbf{Complessità di tempo}: \( O(b^m) \) nel peggiore dei casi, ma può essere
    molto più veloce con una buona euristica
  \item \textbf{Complessità di spazio}: \( O(b^m) \), bisogna memorizzare tutti i nodi
    generati
  \item \textbf{Ottimale}: No
\end{itemize}

\subsubsection{A* search}
Questo algoritmo evita di espandere cammini che sono già molto costosi e ha come
funzione di valutazione:
\[
  f(n) = g(n) + h(n)
\] 
dove:
\begin{itemize}
  \item \( g(n) \): costo del percorso dal nodo iniziale a \( n \)
  \item \( h(n) \): stima del costo rimanente per raggiungere l'obiettivo da \( n \)
  \item \( f(n) \): stima del costo totale del percorso passando per \( n \)
\end{itemize}
L'euristica, per poter garantire l'ottimalità, deve essere \textbf{ammissibile}, cioè
per ogni nodo la stima di quel nodo deve essere minore o uguale del vero costo per arrivare
all'obbiettivo, quindi non deve \textbf{sovrastimare} il costo rimanente:
\[
  h(n) \le h^*(n) \quad h(n) \ge 0 \to h(G) = 0
\] 
dove \( h^*(n) \) è il costo effettivo del percorso da \( n \).
\begin{theorem}
  Per A* l'euristica ammissibile implica l'ottimalità
\end{theorem}
\noindent
Questo tipo di ricerca è:
\begin{itemize}
  \item \textbf{Completa}: Sì, tranne se ci sono infiniti nodi con \( f \le f(G) \) 
  \item \textbf{Complessità di tempo}: Esponenziale in errore relativo in \( h \times  \) 
    lunghezza del numeo di passi della soluzione ottimale. (Se l'euristica è buona, la
    complessità sarà molto più bassa)
  \item \textbf{Complessità di spazio}: \( O(b^d) \), bisogna memorizzare tutti i nodi
    generati
  \item \textbf{Ottimale}: Sì, ma richiede assunzioni sull'euristica (ammissibilità,
    consistenza) e una strategia di ricerca (ricerca ad albero o grafo)
\end{itemize}

\subsubsection{Consistenza e ammissibilità}
\begin{definition}
  Un euristica è \textbf{consistente} se:
  \[
    h(n) \le c(n,a,n') + h(n')
  \] 
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}[->, node distance=2cm, on grid, auto]
      \node[state] (n) {$n$};

      \node[state, below of=n] (n1) {$n'$};

      \node[state, below right of=n1] (G) {$G$};

      \draw (n) edge[left] node{\( c(n,a,n') \)} (n1);
      \draw (n) edge[right] node{\( h(n) \)} (G);
      \draw (n1) edge[left] node{\( h(n') \)} (G);
    \end{tikzpicture}
    \caption{Esempio di euristica consistente}
  \end{figure}
  \begin{itemize}
    \item Si può dimostrare che se \( h \) è consistente, allora \( f(n) \) non decresce
      lungo qualsiasi cammino

    \item A* espande i nodi in ordine crescente di \( f \), quindi trova sempre la soluzione
      ottimale
  \end{itemize}
  Quindi si espande sempre prima un cammino ottimo rispetto a un cammino non ottimo.
\end{definition}

\noindent
La consistenza implica l'ammissibilità e può essere dimostrato per induzione sul cammino
verso il goal. L'ammissibilità però non implica la consistenza.
\[
  \begin{aligned}
    \text{Consistenza} \to \text{Ammissibilità}\\
    \text{Ammissibilità} \not\to \text{Consistenza}
  \end{aligned}
\] 
\begin{itemize}
  \item Tree-Search + euristica ammissibile \( \to \) A* ottimale
  \item Graph-Search + euristica ammissibile \( \not\to \) A* ottimale (può scartare
    il cammino ottimale per un nodo ripetuto)
  \item Graph-Search + euristica consistente \( \to \) A* ottimale
\end{itemize}

\subsubsection{Euristiche}
Le euristiche possono essere create in diversi modi, prendiamo ad esempio l'8-puzzle:
\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{8-puzzle}
  \caption{Esempio di 8-puzzle}
\end{figure}
Per questo problema si potrebbe utilizzare come euristica:
\begin{itemize}
  \item \( h_1(n) = \) numero di pezzi fuori posto
  \item \( h_2(n) = \) somma delle distanze di Manhattan (numero di mosse orizzontali e verticali
    necessarie per portare ogni pezzo alla sua posizione obiettivo)
\end{itemize}
Entrambe le euristiche sono ammissibili, ma \( h_2 \) è più precisa di \( h_1 \) perchè
fornisce una stima più vicina al costo reale per raggiungere l'obiettivo.

In questo caso si dice che \( h_2 \) \textbf{domina} \( h_1 \) se sono entrambe ammissibili
e \( h_2(n) \) è sempre maggiore o uguale a \( h_1 \):
\[
  h_2(n) \ge h_1(n) \quad \forall n
\] 

\begin{theorem}
  Date due qualsiasi euristiche \textbf{ammissibili} \( h_a \) e \( h_b \),
  allora l'euristica definita come:
  \[
    h(n) = \max(h_a(n), h_b(n))
  \]
  è anch'essa ammissibile e domina sia \( h_a \) che \( h_b \)
\end{theorem}

\noindent
Le euristiche ammissiibli possono essere derivate dall'esatto costo della soluzione di un
problema \textbf{rilassato}, cioè un problema simile a quello originale ma con
restrizioni rimosse. Ad esempio, per l'8-puzzle si potrebbe rilassare il problema
permettendo di muovere una casella ovunque (in questo caso \( h_1(n) \) da la soluzione
migliore), oppure permettendo di muovere una casella in qualsiasi casella adiacente (
in questo caso \( h_2(n) \) da la soluzione migliore)

\begin{define}
  Il costo della soluzione ottimale di un problema rilassato non è maggiore del costo
  della soluzione ottimale del problema reale.
\end{define}

\subsection{Ricerca locale}
La ricerca locale è una tecnica di ricerca che si concentra su una soluzione
cercando di migliorarla iterativamente. Gli algoritmi più comuni sono:
\begin{itemize}
  \item Hill climbing
  \item Simulated annealing
  \item Algoritmi genetici
\end{itemize}
Questo tipo di ricerca è utile quando il percorso per arrivare alla soluzione non è
importante, ma solo la soluzione finale. Ci sono due approcci principali:
\begin{itemize}
  \item Trovare la configurazione ottimale (ad esempio (TSP - Traveling Salesman Problem))
  \item Trovare una configurazione che soddisfa dei vincoli (ad esempio il problema delle
    \( n \) regine)
\end{itemize}
Si possono anche usare algoritmi di \textbf{iterative improvement} che partono da una
configurazione iniziale e cercano di migliorarla iterativamente fino a raggiungere
un punto di ottimo locale.
\begin{example}
  Un esempio di ricerca locale è il Traveling Salesman Problem (TSP), dove
  l'obiettivo è trovare il percorso più breve che visita un insieme di città
  esattamente una volta e ritorna alla città di partenza.
  \begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{tsp}
    \caption{Esempio di TSP}
  \end{figure}
  \noindent
  Varianti di questo approccio arrivano fino a 1\% della soluzione ottimale in tempi
  ragionevoli per migliaia di città.
\end{example}
\begin{example}
  Un altro esempio è il problema delle \( n \) regine, dove l'obiettivo è posizionare
  \( n \) regine su una scacchiera \( n \times n \) in modo che nessuna regina
  minacci un'altra.
  \begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{n-queens}
    \caption{Esempio di n-regine}
  \end{figure}
  \noindent
  Risolve quasi sempre il problema quasi istantaneamente per \( n \) molto grande,
  ad esempio \( n = 1.000.000 \).
\end{example}

\subsubsection{Hill climbing}
Hill climbing è un algoritmo di ricerca locale che parte da una soluzione iniziale
e cerca di migliorarla iterativamente spostandosi verso la soluzione migliore
nella sua vicinanza. L'algoritmo continua a muoversi finché non trova un punto
dove nessuna delle soluzioni vicine è migliore della soluzione corrente.
Ritorna un massimo locale, che potrebbe non essere il massimo globale.
\begin{lstlisting}[language=Python]
function Hill-Climbing(problem) returns a state that is a local maximum
  inputs: problem, a problem
  local variables: current, a node
                   neighbor, a node
  current <- Make-Node(problem.Initial-State)
  loop do
    neighbor <- a highest-valued successor of current
    if neighbour.Value <= current.Value then return
      current.State
    end if
    current <- neighbor
  end
\end{lstlisting}
La rappresentazione del problema assumendo di avere tutti gli stati sull'asse x
e il valore della funzione obiettivo sull'asse y è chiamata \textbf{state space landscape}:
\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{hill-climbing}
  \caption{Esempio di state space landscape per hill climbing}
\end{figure}
\begin{itemize}
  \item \textbf{Random-restart hill climbing}: esegue hill climbing più volte
    da stati iniziali casuali per aumentare le probabilità di trovare il massimo globale
  \item \textbf{Random sideways moves}: permette di fare mosse che non migliorano
    la soluzione corrente per evitare di rimanere bloccati in una sezione piatta
\end{itemize}

\subsubsection{Simulated annealing}
Simulated annealing è un algoritmo di ricerca locale ispirato al processo di
raffreddamento dei metalli. L'algoritmo parte da una soluzione iniziale e
cerca di migliorarla iterativamente, ma a differenza di hill climbing, permette
di \textbf{accettare soluzioni peggiori} con una certa probabilità che
\textbf{diminuisce nel tempo}. Questo aiuta a evitare di rimanere bloccati in massimi
locali.
\begin{lstlisting}[language=Python]
function Simulated-Annealing( problem, schedule) returns a solution state
  inputs: problem, a problem
          schedule, a mapping from time to 'temperature'
  local variables: current, a node
                   next, a node
                   T, a 'temperature' controlling prob. of downward steps
  current <- Make-Node(problem.Initial-State)
  for t <- 1 to infinity do
    T <- schedule(t) // temperature at time t
    if T = 0 then return current
    next <- a randomly selected successor of current
    deltaE <- next.Value - current.Value
    if deltaE > 0 then current <- next
    else current <- next only with probability e^(deltaE/T)
\end{lstlisting}
\noindent
Se la "temperatura" \( T \) diminuisce lentamente abbastanza, allora l'algoritmo
converge \textbf{sempre} alla soluzione ottimale \( x^* \) . Questo perchè:
\[
  e^{\frac{E(x^*)}{kT}}/e^{\frac{E(x)}kT}
  =
  e^{\frac{E(x^*) - E(x)}{kT}} \gg 1 \quad \text{per } T \to 0
\] 

\subsubsection{Local beam search}
Local beam search è un algoritmo di ricerca locale che mantiene \textbf{un insieme di
soluzioni candidate} e cerca di migliorarle iterativamente.
In ogni iterazione, l'algoritmo 
\textbf{seleziona casualmente i successori di \( k \), ma con un bias verso i migliori}
per formare il nuovo insieme di soluzioni candidate.

\subsubsection{Algoritmi genetici}
Gli algoritmi genetici sono una classe di algoritmi di ricerca ispirati ai
processi di evoluzione biologica. Questi algoritmi utilizzano meccanismi simili
alla selezione naturale, alla mutazione e al crossover per evolvere una popolazione
di soluzioni candidate verso soluzioni migliori. Gli algoritmi genetici hanno bisogno
di stati encodati come stringhe di caratteri. Il crossover combina due stringhe
per creare una nuova stringa e ha senso solo se le sottostringhe hanno un significato
indipendente.
\begin{example}
  Prendiamo ad esempio il problema delle \( n \) regine in cui si ha un encoding
  della scacchiera come un numero in cui la cifra in posizione \( i \) rappresenta
  la riga in cui si trova la regina nella colonna \( i \).
  \begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{genetic-n-queens}
    \caption{Esempio di algoritmo genetico per il problema delle n-regine}
  \end{figure}
  \noindent
  Questo genera una configurazione nuova a partire da coppie di configurazioni
  esistenti, selezionate in base alla loro "fitness" (un valore che misura
  quanto una configurazione si avvicina alla soluzione ottimale).
  \begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{genetic-n-queens-2}
    \caption{Rappresentazione della scacchiera}
  \end{figure}
  \noindent
  In questo caso la fitness è calcolata come il numero di regine che si minacciano
  a vicenda. Il caso peggiore è quello in cui tutte le regine si minacciano a vicenda,
  quindi la fitness è \( 28 \) (per \( n = 8 \)) perchè:
  \[
    \text{num\_minacce} = \frac{n(n-1)}{2} = \frac{8 \cdot 7}{2} = 28
  \] 
  quindi:
  \[
    \text{fitness} = 28 - \text{num\_minacce}
  \] 
\end{example}

\subsection{Ricerca locale in uno spazio continuo}
La ricerca locale può essere estesa a spazi di stato continui. Per risolvere questi
problemi si possono utilizzare tecniche come:
\begin{itemize}
  \item \textbf{Discretizzazione}: suddividere lo spazio continuo in una griglia di punti
    discreti con una risoluzione \( \delta \) e utilizzare algoritmi di ricerca locale
  \item \textbf{Random perturbations}: si prende una soluzione e si applicano piccole
    perturbazioni casuali per esplorare lo spazio delle soluzioni
  \item \textbf{Gradient}: si calcola analiticamente il gradiente della
    funzione obiettivo \( f(x) \) 
\end{itemize}

\subsubsection{Gradient ascent/descent}
Il gradiente di una funzione \( f(x) \) è il seguente:
\[
  \nabla f(x) = \left( \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2},
  \ldots, \frac{\partial f}{\partial x_n} \right)
\] 
Per trovare la direzione di massima crescita della funzione obiettivo si pone
il gradiente uguale a zero:
\[
  \nabla f(x) = 0
\] 
Spesso non si può porre il gradiente a 0 globalmente, ma si può migliorare localmente:
\begin{itemize}
  \item Aggiornare la soluzione nella direzione massima del gradiente per ogni coordinata
  \item Più la funzione è "ripida" più si fanno passi grandi
\end{itemize}
Aggiornare una coordinata viene effettuato tramite una funzione generale \( g(x_1, x_2) \):
\[
  x_1 \leftarrow x_1 + \alpha \frac{\partial g(x_1, x_2)}{\partial x_1}
  \quad
  x_2 \leftarrow x_2 + \alpha \frac{\partial g(x_1, x_2)}{\partial x_2}
\] 
Oppure in forma vettoriale:
\[
  X = \begin{bmatrix}
    x_1 \\
    x_2 \\
  \end{bmatrix}
  ,\quad
  \nabla g(X) = \begin{bmatrix}
    \frac{\partial g(X)}{\partial x_1} \\
    \frac{\partial g(X)}{\partial x_2} \\
  \end{bmatrix}
\] 
\[
  X \leftarrow X + \alpha \nabla g(X)
\] 
Dove \( \alpha \) è lo "step size", cioè la dimensione del passo da fare:
\begin{itemize}
  \item Se \( \alpha \) è troppo grande si rischia di saltare soluzioni
  \item Se \( \alpha \) è troppo piccolo i passi richiesti possono essere troppi
\end{itemize}

\subsubsection{Algoritmo di Newton-Raphson}
È una tecnica generale per trovare le radici di una funzione, cioè risolvere un'equazione
del tipo \( g(x) = 0 \). Per farlo si trova un'approssimazione iniziale \( \bar{x}_0 \) della
soluzione e iterativamente si aggiorna l'approssimazione usando la formula:
\[
  \bar{x}_{n+1} = \bar{x}_n - \frac{g(\bar{x}_n)}{g'(\bar{x}_n)}
\] 
dove \( g' \) è la derivata di \( g \):
\[
  g'(x) = \frac{dg(x)}{dx}
\] 
\begin{example}
  Consideriamo la funzione \( g(x) = x^2 - a \).
  \begin{enumerate}
    \item 
      Mostra che il
      metodo di Newton-Raphson porta alla formula:
      \[
        x_{n+1} = \frac{1}{2} \left( x_n + \frac{a}{x_n} \right)
      \] 

      \vspace{1em}
      \noindent
      \textbf{Soluzione:}\\
      La formula di Newton-Raphson è:
      \[
        \begin{aligned}
          \bar{x}_{n+1} &= \bar{x}_n - \frac{g(\bar{x}_n)}{g'(\bar{x}_n)}\\
                        &= \bar{x}_n - \frac{\bar{x}_n^2 - a}{2\bar{x}_n} \\
                        &= \frac{2 \bar{x}_n^2 - (\bar{x}_n^2 - a)}{2\bar{x}_n}\\
                        &= \frac{\bar{x}_n^2 + a}{2\bar{x}_n} \\
                        &= \frac{1}{2} \left( \bar{x}_n + \frac{a}{\bar{x}_n} \right)
        \end{aligned}
      \] 


    \item Fissato \( a = 4 \) e \( x_0 = 1 \), calcola \( x_i, \; i \in \{1,2,3\}  \) 

      \vspace{1em}
      \noindent
      \textbf{Soluzione:}\\
      Sostituendo i valori nella formula ottenuta al passo precedente si ottiene:
      \[
      \begin{aligned}
        x_1 &= \frac{1}{2} \left( 1 + \frac{4}{1} \right) = \frac{5}{2} = 2.5\\
        x_2 &= \frac{1}{2} \left( \frac{5}{2} + \frac{4}{\frac{5}{2}} \right)
              = \frac{1}{2} \left( \frac{25}{10} + \frac{8}{5} \right)
              = \frac{1}{2} \cdot \frac{41}{10} = \frac{41}{20} = 2.05\\
        x_3 &= \frac{1}{2} \left( \frac{41}{20} + \frac{4}{\frac{41}{20}} \right)
              = \frac{1}{2} \left( \frac{41}{20} + \frac{80}{41} \right)
              = \frac{1}{2} \cdot \frac{1681 + 1600}{820}
              \approx 2.0006\\
      \end{aligned}
      \] 
      Graficamente si può vedere che la funzione converge rapidamente a \( 2 \):
      \begin{figure}[H]
        \centering
        \includegraphics[width=0.7\textwidth]{newton_raphson}
        \caption{Esempio grafico del metodo di Newton-Raphson}
      \end{figure}
  \end{enumerate}
\end{example}

\subsubsection{Calcolo degli zeri del gradiente}
Utilizzando il metodo di Newton-Raphson si possono trovare gli zeri del gradiente,
cioè dove la funzione generica \( g(x) \) è \( \nabla f(X) \). In questo caso
le funzioni di aggiornamento in forma vettoriale diventano:
\[
  x \leftarrow x - H^{-1}_f(x) \nabla f(x)
\] 
dove \( H_{ij} = \frac{\partial^2 f}{\partial x_i \partial x_j} \) è la matrice Hessiana.
Per problemi in più dimensioni calcolare tutte le entrate della matrice Hessiana può
essere computazionalmente costoso, quindi spesso si usano metodi approssimati.

\vspace{1em}
\noindent
Questo è ancora un metodo \textbf{locale}, quindi soffre degli stessi problemi
della ricerca locale, come massimi locali e punti sella. Random restart e
simulated annealing possono essere utili anche per spazi continui.

\subsubsection{Gradiente empirico}
A volte si può calcolare \( f(X) \) per un certo input, ma non si può calcolare
\( \nabla f(X) = 0 \) neanche localmente. L'\textbf{empirical gradient} è la risposta
di \( f(X) \) a piccoli incrementi o decrementi di \( X \).

\subsection{Constrained satisfaction problem}
Assumiamo di avere un singolo agente, azioni deterministiche e un ambiente completamente
osservabile (discreto). In questo tipo di problemi:
\begin{itemize}
  \item Lo stato è definito da un insieme di variabili \( X = X_i \) che può assumere
    valori in un insieme di domini \( D = D_i \).
  \item Il goal test è un insieme di vincoli che specificano le combinazioni ammissibili
    per sottoinsiemi di variabili
  \item Si possono usare algoritmi che sfruttano queste proprietà che sono più efficenti
    degli algoritmi di ricerca generici
\end{itemize}

\begin{example}
  Un esempio di problema CSP è il problema del map coloring, in cui si deve colorare
  una mappa in modo che nessuna regione confinante abbia lo stesso colore.
  \begin{itemize}
    \item \textbf{Variabili}: \( WA, NT, Q, NSW, V, SA, T \) 
    \item \textbf{Domini}: \( D_i = \{\text{red}, \text{green}, \text{blue}\}  \) 
    \item \textbf{Vincoli}: Regioni adiacenti devono avere colori diversi. Si rappresenta
      con: \( WA \neq NT \)
  \end{itemize}
  \begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{map_coloring}
    \caption{Esempio di map coloring}
  \end{figure}
  \noindent
  Una soluzione è ad esempio:
  \[
    \begin{aligned}
      \{ WA = \text{red}, NT = \text{green}, Q = \text{red}, NSW = \text{green},\\
      V = \text{red}, SA = \text{blue}, T = \text{green} \}
    \end{aligned}
  \] 
  \begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{map_coloring_solution}
    \caption{Esempio di soluzione del map coloring}
  \end{figure}
\end{example}

\subsubsection{Grafo dei vincoli}
Il grafo dei vincoli, detto anche primal graph, è una rappresentazione grafica
di un problema CSP in cui è presente un nodo per ogni variabile e un arco per ogni
vincolo tra due variabili:
\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{constraint_graph}
  \caption{Esempio di grafo dei vincoli per il problema del map coloring}
\end{figure}

\begin{definition}
  Il grafo dei vincoli è definito come una tupla di 3 elementi:
  \[
    CN = \left< X, D, C \right>
  \] 
  dove:
  \begin{itemize}
    \item \( X = \{x_1, \ldots, x_n\}  \): insieme di variabili
    \item \( D = \{D_1, \ldots, D_n\}  \): insieme di domini
    \item \( C = \{(S_1, R_1), \ldots, (S_m, R_m)\}  \): insieme di vincoli, dove ogni vincolo
      \( (S_i, R_i) \) è composto da:
      \begin{itemize}
        \item \( S_i \subseteq X \): sottoinsieme di variabili coinvolte nel vincolo (scope)
        \item \( R_i \): sottoinsieme del prodotto cartesiano delle variabili in \( S_i \),
          cioè l'insieme delle combinazioni ammissibili delle variabili in \( S_i \)
      \end{itemize}
    \item Soluzione: un'assegnazione di \textbf{tutte} le variabili che soddisfa
      \textbf{tutti} i vincoli. Esistono anche soluzioni parziali consistenti, cioè
        una soluzione parziale che soddisfa tutti i vincoli in cui lo scope contiene
        solo variabili assegnate. La soluzione parziale consistente non è necessariamente
        parte di una soluzione completa.
    \item Tasks: è una funzione di ottimizzazione, ad esempio controllo di consistenza,
      trovare una o tutte le soluzioni
  \end{itemize}
\end{definition}

\begin{example}
  Consideriamo seguente crossword:
  \begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{crossword}
    \caption{Esempio di crossword}
  \end{figure}
  \noindent
  Bisogna assegnare le lettere delle parole disponibili alle caselle vuote in modo
  che le parole risultanti siano valide.
  \begin{itemize}
    \item \textbf{Variabili}: parole possibili: \( {\text{MAP}, \text{ARC}} \) 
    \item \textbf{Domini}: \( D_i = \text{ lettere dell'alfabeto} \) 
    \item \textbf{Vincoli}: lettere condivise devono essere uguali:
      \[
        \begin{aligned}
          C_1 \left[ \{x_1, x_2, x_3\}, \left( \text{MAP} \right), \left( \text{ARC} \right) \right]\\
          C_2 \left[ \{x_2, x_4, x_5\}, \left( \text{MAP} \right), \left( \text{ARC} \right) \right]
        \end{aligned}
      \]
  \end{itemize}
  Il grafo dei vincoli è il seguente:
  \begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{crossword_constraint_graph}
    \caption{Esempio di grafo dei vincoli per il problema del crossword}
  \end{figure}
  \noindent
  Esiste un altro grafo chiamato \textbf{grafo duale} in cui i nodi rappresentano
  i vincoli e gli archi rappresentano le variabili condivise tra i vincoli (vincolo di
  uguaglianza):
  \begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{crossword_dual_graph}
    \caption{Esempio di grafo duale per il problema del crossword}
  \end{figure}
\end{example}
\noindent

\subsubsection{Problemi combinatori}
I problemi combinatori sono tutti quei problemi in cui dato un insisme di soluzioni
si vuole trovare la soluzione migliore. Il CSP è un sottoinsieme di problemi combinatori.
I principali tipi di problemi combinatori sono:
\begin{itemize}
  \item \textbf{Decisioni}: dato un insieme di soluzioni, decidere se esiste una soluzione
    che soddisfa certi criteri. Ad esempio colora un grafo con \( k \) colori, bisogna
    dire se è possibile o no fissato un \( k \).
  \item \textbf{Ottimizzazione}: bisogna ottimizzare un obiettivo. Ad esempio colorare
    un grafo con \( k \) colori minimizzando i conflutti.
  \item \textbf{Ottimizzazione multiobiettivo}: bisogna ottimizzare più obiettivi
    contemporaneamente. Ad esempio minimizzare il rischio e massimizzare il profitto in
    un portafoglio di investimenti.
  \item \textbf{Graphical models}: sono problemi definiti da:
    \begin{itemize}
      \item Insieme di variabili
      \item Domini delle variabili
      \item Funzioni \textbf{locali} che definiscono i vincoli
      \item Funzione \textbf{globale} che rappresenta un'aggregazione delle funzioni locali
      \item Soluzioni, ovvero assegnazioni delle variabili, che ottimizzano la funzione
        globale
    \end{itemize}
\end{itemize}

\subsubsection{Backtracking search}
Il backtracking search è un algoritmo di ricerca per i problemi CSP. Questo algoritmo
è utile quando le assegnazioni delle variabili sono commutative, cioè l'ordine in cui
le variabili vengono assegnate non cambia il risultato finale:
\[
  WA = \text{red}, NT = \text{green}
  \quad \text{è equivalente a} \quad
  NT = \text{green}, WA = \text{red}
\] 
Questo algoritmo è semplicemente una ricerca in profondità per CSP con assegnamenti
alle variabili singoli. L'ordine delle variabili può impattare la performance dell'algoritmo.
\begin{lstlisting}[language=Python]
function Backtracking-Search(csp) returns solution or failure
  return Backtrack({ }, csp)

function Backtrack(assignment, csp) returns solution or failure
  if assignment is complete then return assignment
  var <- Select-Unassigned-Variable(csp)
  for each value in Order-Domain-Values(var, assignment, csp) do
    if value is consistent with assignment then
      add {var = value} to assignment
      inferences <- Inferences(csp, var, value)
      if inferences 6 = failure then
        add inferences to assignment
        result <- Backtrack(assignment, csp)
        if result 6 = failure then
          return result
        endif
      endif
    endif
    remove {var = value} and inferences from assignment
  endfor
  return failure
\end{lstlisting}
Le principali decisioni che impattano l'algoritmo sono:
\begin{itemize}
  \item Come selezionare la variabile
  \item Come selezionare il valore
  \item Come fare inferenze
\end{itemize}
Per migliorare l'algoritmo si possono usare le seguenti tecniche:
\begin{itemize}
  \item \textbf{Ordinare le variabili}:
    \begin{itemize}
      \item Minimum Remaining Values: Seleziona la variabile con il minor numero di valori
        legali rimasti nel dominio, quindi prima si fallisce meglio è
        \begin{figure}[H]
          \centering
          \includegraphics[width=0.7\textwidth]{minimum_remaining_value}
          \caption{Esempio di Minimum Remaining Values}
        \end{figure}
      \item Degree Heuristic: Seleziona la variabile che è coinvolta nel maggior numero
        di vincoli con altre variabili non assegnate
        \begin{figure}[H]
          \centering
          \includegraphics[width=0.7\textwidth]{degree_heuristic}
          \caption{Esempio di Degree Heuristic}
        \end{figure}
    \end{itemize}

  \item \textbf{Ordinare i valori delle variabili}:
    \begin{itemize}
      \item Least Constraining Value: Seleziona il valore che lascia il maggior numero
        di opzioni aperte per le altre variabili non assegnate
        \begin{figure}[H]
          \centering
          \includegraphics[width=0.7\textwidth]{least_constraining_value}
          \caption{Esempio di Least Constraining Value}
        \end{figure}
    \end{itemize}

  \item \textbf{Consistenza locale}: Arc o Path consistency per ridurre i domini delle
    variabili e individuare fallimenti precoci

  \item \textbf{Look-ahead}: Predirre i conflitti futuri per evitare di esplorare
    rami che porteranno a fallimenti

  \item \textbf{Look-back}: Analizzare verso dove fare backtracking

  \item \textbf{Tree decomposition}: Sfrutta la struttura del problema per dividerlo in
    sotto-problemi più piccoli e risolverli separatamente
\end{itemize}

\subsubsection{Inferenza}
Si possono fare ragionamenti riguardo ai vincoli per fare inferenze per nuovi vincoli.
Se consideriamo l'esempio del graph coloring:
\begin{itemize}
  \item Dati \( \{x_1, x_2, x_3\} \), \( \{D_1 = D_2 = D_3\}, D_i = \{R, B\}  \) e\\
    \( C = \left\{ C_1: (x_1 \neq x_2), C_2: (x_2 \neq x_3) \right\} \)
  \item Si può inferire che \( C_3: (x_1 \neq x_3) \) perchè se \( x_1 = x_3 \)
    allora \( x_2 \) non può assumere nessun valore legale
\end{itemize}
\noindent
I vantaggi e svantaggi dell'aggiunta di vincoli sono:
\begin{itemize}
  \item Vincoli più stringenti portano ad uno spazio di ricerca più piccolo
  \item Aggiungere vincoli richiede più computazione
  \item Ogni volta che una nuova variabile viene assegnata bisogna controllare più
    vincoli
  \item Se il problema consiste soltanto in vincoli binari allora non si hanno mai più
    di \( O(n) \) controlli
  \item Se il problema consiste in vincoli di ordine superiore \( r \) allora si hanno
    \( O(n^{r-1}) \) controlli
\end{itemize}
Questa inferenza rende il grafo backtrack free.

\vspace{1em}
\noindent
Un grafo si dice \textbf{backtrack free} se ogni foglia è un goal state. Una DFS su un
grafo backtrack-free garantisce un assegnamento completo e consistente

\subsubsection{Look Ahead}
Look ahead è una tecnica che permette di fare inferenze sui vincoli futuri
per evitare di esplorare rami che porteranno a fallimenti. Quindi data un'inferenza
approssimata si vuole predirre l'impatto della prossima assegnazione di una variabile
e vedere come impatta i futuri assegnamenti. Ci sono due strategie principali:
\begin{itemize}
  \item \textbf{Forward checking}: Controlla le variabili assegnate separatamente
    da quelle non assegnate.
  \item \textbf{Arc consistency look ahead}: Propaga la consistenza di arco, cioè quella
    che assicura che per ogni valore di una variabile esista un valore legale
    nella variabile connessa tramite un vincolo binario, in tutta la rete
\end{itemize}

\subsubsection{Forward checking look ahead}
È la forma più limitata di propagazione dei vincoli. Propaga l'effetto di un valore
selezionato su tutte le variabili future \textbf{separatamente}, cioè una ad una.
Se il dominio di una variabile futura diventa vuoto, allora si tenta il valore successivo
per la variabile corrente.
\begin{example}
  Prendiamo ad esempio il problema del map coloring, l'idea principale è quella di tenere
  traccia dei valori legali rimanenti per le variabili non assegnate. Viene terminata
  la ricerca quando qualsiasi variabile non ha più valori legali.
  \begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{map_coloring_forward_check}
    \caption{Esempio di forward checking per il problema del map coloring}
  \end{figure}
\end{example}
La complessità del forward checking è: \( O(ek^2) \), dove:
\begin{itemize}
  \item \( e \) il numero di vincoli
  \item \( k \) valore per ogni variabile futura
  \item \( k \) valore per la variabile corrente
\end{itemize}

\subsubsection{Arc consistency look ahead}
L'arc consistency look ahead forza la consistenza di arco su tutte le variabili rimanenti.

\begin{itemize}
  \item Un arco \( x_k \to x_j \) è \textbf{consistente} se e solo se per ogni assegnamento di
    \( x_k \) c'è almeno un assegnamento di \( x_j \) che è consistente con il vincolo
    \( (x_k, x_j) \). 

  \item Forzare la consistenza di arco: se nessun valore di \( x_j \) è consistente
    con un dato valore di \( x_k = c \) allora si rimuove \( c \) dal dominio di \( x_k \)

  \item Forward checking: si impone la consistenza da ogni variabile non assegnata
    a un nuovo assegnamento

  \item Nota: nel forward checking non si controlla mai la consistenza tra variabili
    non assegnate, nell'arc consistency look-ahead per ogni nuovo assegnamento
    si controlla la consistenza di arco tra ogni coppia di variabili
\end{itemize}
L'arc consistency viene rappresentata da una funzione che modifica il dominio di
\( x_k \) se non è consistente con \( x_j \):
\[
  rev(x_k, x_j)
\] 

\begin{example}
  Consideriamo il problema del map coloring. I passi dell'algoritmo sono:
  \begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{map_coloring_arc_consistency_1}
    \caption{Passo 1}
  \end{figure}
  \begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{map_coloring_arc_consistency_2}
    \caption{Passo 2}
  \end{figure}
  \begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{map_coloring_arc_consistency_3}
    \caption{Passo 3}
  \end{figure}
  \begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{map_coloring_arc_consistency_4}
    \caption{Passo 4}
  \end{figure}
\end{example}

\vspace{1em}
\noindent
La complessità dell'algoritmo arc consistency migliore (AC-4) è \( O(ek^3) \).

\subsubsection{Forzare l'arc consistency}
\begin{definition}
  \begin{itemize}
    \item Un arco \( x_i, x_j \) è arc consistent se e solo se \( x_i \) è arc consistent
    rispetto a \( x_j \) e \( x_j \) è arc consistent rispetto a \( x_i \).
    
    \item 
    Un grafo è arc consistent se e solo se tutti i suoi vincoli sono arc consistent.

    \item 
    Si può facilmente assicurare che una coppia di variabili siano arc consistent
    (procedura Revise),
    Ma rivedere la consistenza di arco su una variabile potrebbe rendere un'altra
    variabile non arc consistent.

    \item 
    Sono necessarie procedure sistematiche per garantire l'arc consistency sulle reti.
  \end{itemize}
\end{definition}
\begin{itemize}
  \item L'arc consistency con un dominio vuoto implica che il problema non ha soluzione.

  \item 
    L'arc consistency con tutti i domini non vuoti \textbf{non} implica che ci sia una soluzione.
    Quindi l'arc consistency non è completa. 

  \item L'unica cosa che si può dire a riguardo è che
    se l'arc consistency con tutti i domini non vuoti non ha cicli nel grafo dei vincoli
    e ha solo vincoli binari, allora il grafo è backtrack free ed esiste una soluzione.
\end{itemize}

\subsubsection{Tree decomposition}
Consiste nel decomporre il problema in una struttura ad albero che permette di sfruttare
la proprietà degli alberi di essere backtrack free. L'idea più semplice è quella
di togliere le variabili assegnate fino a che il grafo non diventa un albero.
\begin{definition}
  Dato un grafo non orientato, il sottoinsieme di nodi del grafo è definito
  \textbf{cycle cutset} se la rimozione di questi nodi rende il grafo aciclico.
\end{definition}
Per trovare una soluzione al problema si devono provare tutte le possibili assegnazioni
delle variabili nel cycle cutset e per ogni assegnazione si risolve il problema tramite
arc propagation.
La complessità è esponenziale, ma dipende solo dal numero di nodi nel cycle cutset.

\section{Logical Agents}
Un agente logico è un agente che utilizza la logica per rappresentare la conoscenza
e ragionare su di essa. La logica fornisce un linguaggio formale per esprimere fatti
e regole sul mondo, permettendo all'agente di dedurre nuove informazioni e prendere decisioni
basate sulla conoscenza acquisita.

\subsection{Knowledge based agents}
Gli agenti knowledge based sono divisi in due componenti principali:
\begin{itemize}
  \item \textbf{Inference engine}: si occupa di dedurre nuove informazioni dalla
    knowledge base utilizzando regole logiche
  \item \textbf{Knowledge base}: contiene la conoscenza dell'agente rappresentata
    in forma logica, cioè è un insieme di frasi logiche appartenenti ad un linguaggio
    formale
\end{itemize}
L'approccio dichiarativo per costruiere un agente consiste nel \textbf{dirgli} (Tell) cosa
deve sapere e poi l'agente può \textbf{chiedere} (Ask) alla sua knowledge base per
sapere cosa fare.

\vspace{1em}
\noindent
Un esempio di agente knowledge based è il seguente:
\begin{lstlisting}[language=Python]
function KB-Agent( percept) returns an action
  static: KB, a knowledge base
          t, a counter, initially 0, indicating time
  Tell(KB, Make-Percept-Sentence( percept, t))
  action <- Ask(KB, Make-Action-Query(t))
  Tell(KB, Make-Action-Sentence(action, t))
  t <- t + 1
  return action
\end{lstlisting}
L'agente deve essere in grado di:
\begin{itemize}
  \item Rappresentare stati, azioni ecc...
  \item Incorporare nuove percezioni nella knowledge base
  \item Aggiornare le rappresentazioni interne del mondo
  \item \textbf{Dedurre} proprietà nascoste del mondo
  \item \textbf{Dedurre} azioni appropriate
\end{itemize}

\begin{example}
  Consideriamo il problema del Wumpus World:
  \begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{wumpus_world}
    \caption{Esempio di Wumpus World}
  \end{figure}
  \begin{itemize}
    \item \textbf{Performance measure}:
      \begin{itemize}
        \item +1000 per uscire con l'oro
        \item -1 per ogni azione
        \item -1000 per morire
        \item -10 per ogni freccia usata
      \end{itemize}
    \item \textbf{Environment}:
      \begin{itemize}
        \item 4x4 griglia
        \item Wumpus in una casella
        \item Pozzi in alcune caselle
        \item Oro in una casella

        \item Caselle adiacenti al Wumpus hanno puzza
        \item Caselle adiacenti ai pozzi hanno brezza
        \item Glitter nella casella con l'oro
        \item Prendi per prendere l'oro dalla casella
        \item Rilascia per lasciare l'oro nella casella
        \item Sparare uccide il Wumpus se è nella stessa direzione della freccia
        \item Sparare usa l'unica freccia disponibile
      \end{itemize}

    \item \textbf{Actuators:}
      \begin{itemize}
        \item Gira a sinistra
        \item Gira a destra
        \item Avanti
        \item Prendi
        \item Rilascia
      \end{itemize}

    \item \textbf{Sensors:}
      \begin{itemize}
        \item Puzza
        \item Brezza
        \item Glitter
      \end{itemize}
  \end{itemize}
  Questo problema è:
  \begin{itemize}
    \item \textbf{Non osservabile} perchè si ha solo percezione locale
    \item \textbf{Deterministico} perchè le azioni hanno effetti certi
    \item \textbf{Non episodico} perchè le azioni sono sequenziali
    \item \textbf{Statico} perchè l'ambiente non cambia
    \item \textbf{Discreto} perchè ci sono un numero finito di stati e azioni
    \item \textbf{Singolo agente} perchè c'è solo un agente che agisce nell'ambiente
      (il wumpus è parte dell'ambiente)
  \end{itemize}
\end{example}

\subsection{Logica in generale}
La logica è un sistema formale per rappresentare informazioni e ragionare su di esse.
\begin{itemize}
  \item \textbf{Sintassi}: definisce le regole per costruire frasi valide nel linguaggio
  \item \textbf{Semantica}: definisce il significato delle frasi nel linguaggio, ad
    esempioo definisce quando una frase è vera o falsa in un certo mondo
\end{itemize}

\subsubsection{Entailment (Derivazione logica)}
La derivazione o entailment indica che da una certa cosa ne segue un altra:
\[
  KB \models \alpha
\] 
La knowledge base implica una frase \( \alpha \) se e solo se \( \alpha \) è vera
in tutti i mondi in cui è vera la knowledge base. La derivazione è una relazione tra
frasi logiche (sintassi) che si basa sulla semantica. Un modello è un mondo formalmente
strutturato rispetto a quale verità o falsità di frasi logiche può essere valutata.
\( m \) è un modello di una frase \( \alpha \), se \( \alpha \) è vera in \( m \),
allora \( M(\alpha) \) è l'insieme di tutti i modelli di \( \alpha \). Di conseguenza
\[
  KB \models \alpha \iff M(KB) \subseteq M(\alpha)
\] 
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\textwidth]{entailment}
  \caption{Esempio di entailment}
\end{figure}

\begin{example}
  Un esempio potrebbe essere:
  \begin{itemize}
    \item \( KB = \text{ Juventus ha vinto e la Roma ha vinto} \) 
    \item \( \alpha = \text{ Juventus ha vinto} \) 
  \end{itemize}
\end{example}

\subsubsection{Inferenza}
L'inferenza è il processo di derivare nuove frasi logiche da frasi esistenti
nella knowledge base utilizzando regole logiche. Una frase \( \alpha \) può essere
derivata dalla knowledge base \( KB \) con una procedura \( i \):
\[
  KB \vdash_i \alpha
\] 
Un sistema di inferenza ha due proprietà importanti:
\begin{itemize}
  \item \textbf{Soundness}: se \( KB \vdash_i \alpha \) allora \( KB \models \alpha \)
    cioè tutto ciò che viene derivato è vero
  \item \textbf{Completeness}: se \( KB \models \alpha \) allora \( KB \vdash_i \alpha \)
    cioè tutto ciò che è vero può essere derivato
\end{itemize}

\begin{example}
  Rappresentiamo il problema del Wumpus World in logica proposizionale:
  \begin{itemize}
    \item \( P_{i,j} \): è vero se c'è un pozzo nella casella \( (i,j) \)
    \item \( B_{i,j} \): è vero se c'è brezza nella casella \( (i,j) \)
  \end{itemize}
  Consideriamo i seguenti fatti:
  \[
    \begin{aligned}
      R_1: & \neg P_{1,1} \\
      R_2: & \neg B_{1,1}\\
      R_3: & B_{2,1}
    \end{aligned}
  \] 
  Rappresentiamo in forma logica la frase "i pozzi causano brezza nelle caselle adiacenti":
  \begin{itemize}
    \item Considerando caselle specifiche:
      \[
        \begin{aligned}
          R_4: &B_{1,1} \iff (P_{1,2} \vee  P_{2,1})\\
          R_5: &B_{2,1} \iff (P_{1,1} \lor P_{2,2} \lor P_{3,1})
        \end{aligned}
      \] 
  \end{itemize}

  \begin{itemize}
    \item \( R \) rappresenta le regole del mondo
    \item \( P \) rappresenta le variabili da assegnare
    \item \( KB \) è la knowledge base, cioè l'and logico di tutte le regole
  \end{itemize}
\end{example}

\subsubsection{Inferenza mediante enumerazione}
Esiste un algoritmo che verifica per ogni possibile assegnamento se \( KB \models \alpha \):
\begin{lstlisting}[language=Python]
function TT-Entails?(KB, alpha) returns true or false
  inputs: KB, the knowledge base, a sentence in prop. logic
          alpha, the query, a sentence in prop. logic
  symbols <- a list of the proposition symbols in KB and alpha
  return TT-Check-All(KB, alpha, symbols, [ ])

function TT-Check-All(KB, alpha, symbols, model) returns true or false
  if Empty?(symbols) then
    if PL-True?(KB, model) then return PL-True?(alpha, model)
    else return true
  else do
    P <- First(symbols); rest <- Rest(symbols)
    return TT-Check-All(KB, alpha, rest, Extend(P, true, model)) and
           TT-Check-All(KB, alpha, rest, Extend(P, false, model))
\end{lstlisting}
Siccome si devono provare tutte le possibili combinazioni di verità per \( n \)
variabili, la complessità è \( O(2^n) \), quindi il problema è co-NP-completo.

\subsubsection{Metodi di dimostrazione}
I metodi di dimostrazione si dividono in due categorie principali:
\begin{itemize}
  \item \textbf{Model checking}:
    \begin{itemize}
      \item Truth table enumeration: verifica tutti i modelli possibili
      \item Improved backtracking: usa backtracking per evitare di esplorare
        modelli non validi. Ad esempio DPLL (Davis-Putnam-Logemann-Loveland)
      \item Heuristic search in model space: cerca modelli validi usando
        euristiche per guidare la ricerca (corretto ma non completo). Ad esempio
        algoritmo come hill-climbing
    \end{itemize}

  \item \textbf{Applicazione delle regole di inferenza}
    \begin{itemize}
      \item Generazione di nuove frasi logiche a partire da quelle esistenti
      \item Dimostrazione: Una sequenza di applicazioni di regole di inferenza
      \item Richiedono la traduzione di frasi logiche in \textbf{forma normale}
    \end{itemize}
\end{itemize}

\subsubsection{Equivalenza logica}
Due frasi logiche sono logicamente equivalenti se e solo se sono vere negli stessi
modelli:
\[
  \alpha \equiv \beta \iff \alpha \models \beta \wedge \beta \models \alpha
\] 
\begin{example}
  Delle tautologie importanti sono:
  \begin{equation}
    \begin{aligned}
      \left( \alpha \wedge \beta \right) & \equiv \left( \beta \wedge \alpha \right)\\
      \left( \alpha \vee \beta \right) & \equiv \left( \beta \vee \alpha \right)\\
      %
      \left( \left( \alpha \wedge \beta \right) \wedge \gamma \right) & \equiv
        \left( \alpha \wedge \left( \beta \wedge \gamma \right) \right)\\
      \left( \left( \alpha \vee \beta \right) \vee \gamma \right) & \equiv
        \left( \alpha \vee \left( \beta \vee \gamma \right) \right)\\
      %
      \neg \left( \neg \alpha \right) & \equiv \alpha \\
      \left( \alpha \implies \beta \right) & \equiv \left( \neg \alpha \implies \neg \beta \right) \\
      \left( \alpha \implies \beta \right) & \equiv \left( \neg \alpha \vee \beta \right) \\
      %
      \left( \alpha \iff \beta \right) & \equiv \left( \left( \alpha \implies \beta \right) \wedge
      \left( \beta \implies \alpha \right) \right) \\
      %
      \neg \left( \alpha \wedge \beta \right) & \equiv \left( \neg \alpha \vee \neg \beta \right) \\
      %
      \neg \left( \alpha \vee \beta \right) & \equiv \left( \neg \alpha \wedge \neg \beta \right) \\
      \left( \alpha \wedge \left( \beta \vee \gamma  \right)  \right) & \equiv
      %
        \left( \left( \alpha \wedge \beta \right) \vee \left( \alpha \wedge \gamma \right) \right)\\
      %
      \left( \alpha \vee \left( \beta \wedge \gamma  \right)  \right) & \equiv
        \left( \left( \alpha \vee \beta \right) \wedge \left( \alpha \vee \gamma \right) \right)
      %
    \end{aligned}
  \end{equation}
\end{example}

\subsubsection{Validità e soddisfacibilità}
Una frase logica è \textbf{valida} se è vera in tutti i modelli, ad esempio:
\[
  True, \quad \alpha \vee \neg \alpha, \quad \alpha \implies \alpha
\] 
La validità è collegata alla derivazione dal teorema della deduzione:
\[
  KB \models \alpha \iff \left( KB \implies \alpha \right) \text{ è valida}
\] 

\vspace{1em}
\noindent
Una frase logica è \textbf{soddisfacibile} se è vera in almeno un modello, ad esempio:
\[
  A \vee B, \quad C
\] 
Invece è \textbf{insoddisfacibile} se non è vera in nessun modello, ad esempio:
\[
  False, \quad A \wedge \neg A
\]
La soddisfacibilità è collegata alla derivazione dal seguente teorema:
\[
  KB \models \alpha \iff \left( KB \wedge \neg \alpha \right) \text{ è insoddisfacibile}
\]

\subsection{Sistema di inferenza}
Un sistema di inferenza è un insieme di regole che permettono di derivare nuove frasi
logiche da frasi esistenti. Le regole sono scritte nella forma:
\[
  \frac{A_1 \ldots A_k}{A} \quad \frac{\text{Premesse}}{\text{Conclusioni}}
\] 
\begin{definition}
  Una derivazione \( A \) è derivata da un insieme di formule \( \Gamma  \) con un
  sistema di inferenza \( \mathcal{R} \) \( \left( \Gamma \vdash_{\mathcal{R}} A \right)  \) 
  se esiste una sequenza \( A_1, \ldots, A_n \) di formule tale che:
  \begin{itemize}
    \item \( A_n = A \)
    \item \( \forall i \in \left\{ 1, \ldots, n \right\} \) è vera una delle seguenti:
      \begin{enumerate}
        \item \( A_i \in \Gamma  \) 
        \item \( A_i \) è una derivazione diretta delle formule nella sequenza precedente
      \end{enumerate}
  \end{itemize}
  La sequenza \( A_1, \ldots, A_n \) è una \textbf{dimostrazione} di \( A \). \( \Gamma  \) 
  sono le \textbf{premesse} (assunzioni o ipotesi) per \( A \).
\end{definition}

\subsubsection{Proprietà di un sistema di inferenza}
\begin{itemize}
  \item \textbf{Correttezza delle regole di inferenza}: Le conclusioni devono essere
    delle conseguenze logiche delle premesse
  \item \textbf{Completezza}: Se una formula è una conseguenza logica delle premesse,
    allora deve essere possibile derivarla usando le regole di inferenza
  \item \textbf{Completezza refutazionale}: Esiste una derivazione di \( \square \) se
    le ipotesi unite alla negazione della conclusione sono insoddisfacibili:
    \[
      \square \text{ è derivabile da } H \cup \left\{ \neg \psi \right\} \iff
      \text{ è insoddisfacente}
    \] 
\end{itemize}

\subsection{Problema di deduzione}
Un problema di deduzione consiste nel determinare se una certa formula logica
può essere derivata da un insieme di formule esistenti:
\[
  \Gamma \models \alpha
\] 
Per risolverlo si possono usare due approcci principali:
\begin{itemize}
  \item \textbf{Dimostrazione per assurdo} (reductio ad absurdum): Si dimostra che
    \[
      \Gamma \wedge \neg \alpha \text{ è insoddisfacibile}
    \] 
    I principali metodi sono la \textbf{resolution}
  \item \textbf{Forware/backward reasoning}: È un algoritmo polinomiale corretto e
    completo per un insieme limitato di formule logiche (Horn clauses)
\end{itemize}

\subsubsection{Resolution}
Per usare la resolution si deve prima convertire ogni formula logica in \textbf{forma
normale congiuntiva} (CNF). La CNF è una congiunzione di clausole (letterali), ad esempio:
\[
  \left( A \vee \neg B \right) \wedge \left( B \vee \neg C \vee \neg D \right) 
\] 
La regola di inferenza della resolution è:
\[
\frac{l_1 \vee \ldots \vee l_k \quad m_1 \vee \ldots \vee m_n}
{
  l_1 \vee \ldots \vee l_{i-1} \vee l_{i+1} \vee \ldots \vee
  l_k \vee m_1 \vee \ldots \vee m_{j-1} \vee m_{j+1} \vee \ldots \vee m_n
}
\] 
Dove \( l_i \) e \( m_j \) sono letterali complementari, cioè uno è la negazione
dell'altro. La risoluzione è corretta e completa per la logica proposizionale.
\begin{example}
  Alcune applicazioni della risoluzione sono le seguenti:
  \[
    \frac{A \vee B \quad \neg B}{A}
  \]
\end{example}

\subsubsection{Conversione in CNF}
Consideriamo la formula logica:
\[
  B_{1,1} \iff (P_{1,2} \vee P_{2,1})
\] 
Per convertire una formula logica in CNF si seguono i seguenti passi:
\begin{enumerate}
  \item Elimina \( \iff \), sostituendo \( \alpha \iff \beta \) con
    \( \left( \alpha \implies \beta \right) \wedge \left( \beta \implies \alpha \right)  \) 
    \[
      \left( B_{1,1} \implies \left( P_{1,2} \vee P_{2,1} \right)  \right) 
      \wedge 
      \left( \left( P_{1,2} \vee P_{2,1} \right) \implies B_{1,1} \right) 
    \] 

  \item Elimina \( \implies \), rimpiazzando \( \alpha \implies \beta \) con
    \( \neg \alpha \vee \beta \) 
    \[
      \left( \neg B_{1,1} \vee P_{1,2} \vee P_{2,1} \right) \wedge 
      \left( \neg \left( P_{1,2} \vee P_{2,1} \right) \vee B_{1,1}  \right) 
    \] 

  \item Muovi \( \neg \) dentro usando le leggi di De Morgan e doppia negazione:
    \[
      \left( \neg B_{1,1} \vee P_{1,2} \vee P_{2,1} \right) \wedge 
      \left( \left( \neg P_{1,2} \wedge \neg P_{2,1} \right) \vee B_{1,1} \right) 
    \] 

  \item Applica la regola della distribuzione:
    \[
      \left( \neg B_{1,1} \vee P_{1,2} \vee P_{2,1} \right) \wedge 
      \left( \neg P_{1,2} \vee B_{1,1} \right) \wedge 
      \left( \neg P_{2,1} \vee B_{1,1} \right) 
    \] 
\end{enumerate}

\vspace{1em}
\noindent
L'algoritmo della risoluzione è il seguente:
\begin{lstlisting}[language=Python]
function PL-Resolution(KB, alpha) returns true or false
  inputs: KB, the knowledge base, a sentence in propositional logic
          alpha, the query, a sentence in propositional logic
  clauses <- the set of clauses in the CNF representation of KB and not alpha
  new <- { }
  loop do
    for each Ci , Cj in clauses do
      resolvents <- PL-Resolve(Ci , Cj )
      if resolvents contains the empty clause then return true
      new <- new union resolvents
    if new is included in clauses then return false
    clauses <- clauses union new
\end{lstlisting}
La risoluzione è da applicare solo a due singoli letterali alla volta.
\begin{example}
  Consideriamo la knowledge base:
  \[
    KB = \left( B_{1,1} \iff \left( P_{1,2} \vee P_{2,1}  \right) \wedge \neg B_{1,1} \right) 
    \quad \alpha = \neg P_{1,2}
  \] 
  \begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{resolution}
    \caption{Esempio di risoluzione}
  \end{figure}
\end{example}

\subsection{Forward e backward chaining}
Per usare il forward e backward chaining si deve prima convertire ogni formula
logica in \textbf{Horn clauses}.
Ciascuna clausola deve rispettare una delle seguenti regole:
\begin{itemize}
  \item È un singolo simbolo proposizionale, oppure
  \item È una congiunzione di simboli proposizionali che implica un singolo simbolo
    proposizionale, ad esempio:
    \[
      \left( A \wedge B \right) \implies C
    \] 
\end{itemize}
Questa forma permette di avere come regola il \textbf{modus ponens}, cioè:
\[
  \frac{\alpha_1, \ldots, \alpha_n \quad \alpha_1 \wedge \ldots \wedge \alpha_n \implies \beta}
  {\beta}
\] 
Questi algoritmi hanno entrambi complessità lineare.

\subsubsection{Forward chaining}
L'idea di questo algoritmo è quella di trovare qualsiasi regola le cui premesse
siano soddisfatte dalla knowledge base e aggiungere la conclusione alla knowledge base
finchè non si trova la formula desiderata.
\begin{lstlisting}[language=Python]
function PL-FC-Entails?(KB, q) returns true or false
  inputs: KB # knowledge base, a set of propositional Horn clauses
          q # query, a prop. symbol
  local variables: count # a table, indexed by clause, initially the number of premises
                   inferred # a table, indexed by symbol, each entry initially false
                   agenda # a list of symbols, initially the symbols known in KB
  while agenda is not empty do
    p <- Pop(agenda)
    unless inferred[p] do
      inferred[p] <- true
      for each Horn Clause c in whose premise p appears do
        decrement count[c]
        if count[c] = 0 then do
          if Head[c] = q then return true
          Push(Head[c], agenda)
  return false
\end{lstlisting}

\begin{definition}[Dimostrazione di completezz]
  Forward checking deriva tutte le formule atomiche che possono essere derivate dalla knowledge
  base.
  \begin{enumerate}
    \item Raggiunge un \textbf{punto fisso} dove non si possono più derivare nuove formule
      atomiche
    \item Si considera lo stato finale come un modello \( m \) assegnando true o false
      ai simboli
    \item Ogni clausola nella knowledge base originale è vera in \( m \)
    \item Quindi \( m \) è un modello di \( KB \)
    \item Per ogni \( a \) (formula atomica), se la knowledge base deriva \( a \):
      \[
        KB \models a
      \] 
      allora \( a \) è vera in \textbf{ogni} modello di \( KB \), incluso \( m \)
  \end{enumerate}
\end{definition}

\subsubsection{Backward chaining}
L'idea di questo algoritmo è quella di iniziare dalla formula desiderata e
cercare di dimostrare che è vera cercando regole che la concludono
e poi cercando di dimostrare che le premesse di queste regole sono vere.
Bisogna fare attenzione a:
\begin{itemize}
  \item Evitare cicli: controllare se il nuovo subgoal è già nella lista dei subgoal
  \item Evitare ripetizioni: evitare di dimostrare lo stesso subgoal più volte
\end{itemize}

\subsubsection{Differenze tra forward e backward chaining}
\begin{itemize}
  \item \textbf{Forward chaining}:
    È \textbf{data-driven}, cioè un processo automatico che parte dai fatti noti
  \item \textbf{Backward chaining}:
    È \textbf{goal-driven}, cioè parte da un obiettivo specifico da raggiungere.
    La complessità può essere molto meglio di \( O(n) \) nella dimensione
    della knowledge base
\end{itemize}

\section{Rappresentare l'incertezza}
\begin{example}
  Consideriamo l'azione \( A_t \) = lasciare l'aereoporto \( t \) minuti prima
  del volo.

  \vspace{1em}
  \noindent
  Ci chiediamo se \( A_t \) ci porterà al volo in orario. I problemi sono:
  \begin{itemize}
    \item \textbf{Osservabilità parziale}: non sappiamo se ci saranno ingorghi
      stradali, condizioni meteo avverse, ecc...
    \item \textbf{Sensori rumorosi}: le previsioni del tempo e le condizioni del traffico
      non sono sempre accurate
    \item \textbf{Incertezza nel risultato delle azioni}: si potrebbero avere imprevisti
      come incidenti stradali, guasti dell'auto, ecc...
    \item \textbf{Complessità immensa}: non è possibile modellare e predirre 
      ogni possibile evento
  \end{itemize}
  Ci sono talmente tante precondizioni che non possiamo tenerle tutte in considerazione.
\end{example}
Ci sono tre principali metodi per gestire l'incertezza:
\begin{itemize}
  \item \textbf{Default o nonmonotonic logic}: si fanno assunzioni che sono vere a meno
    che non si dimostri il contrario

  \item \textbf{Fuzzy logic}: cerca di rendere la logica non binaria, permettendo
    di esprimere \textbf{gradi di verità}, ad esempio valori tra 0 e 1

  \item \textbf{Probabilità}: gli eventi sono binari (veri o falsi), ma con una
    certa probabilità.
\end{itemize}

\subsection{Probabilità}
Le asserzioni probabilistiche sommano gli effetti di:
\begin{itemize}
  \item \textbf{Lazyness}: mancanza di rappresentare informazioni complete
  \item \textbf{Ignorance}: mancanza di conoscenza su fatti specifici
\end{itemize}
Esiste anche la probabilità \textbf{soggettiva} o \textbf{Bayesiana}, che
rappresenta il grado di fiducia in una certa asserzione, basata sulle
conoscenze attuali, ad esempio:
\[
  P(A_25 \;|\; \text{ nessun incidente riportato}) = 0.06
\] 
Questi non sono gradi di verità, ma gradi di \textbf{conoscenza} (belief). Le probabilità
cambiano in base alle nuove evidenze.

\subsubsection{Decidere con incertezza}
Consideriamo di avere le seguenti probabilità:
\[
  \begin{aligned}
    P(A_25 \text{ mi fa arrivare in tempo} \;|\; \ldots) & = 0.04 \\
    P(A_90 \text{ mi fa arrivare in tempo} \;|\; \ldots) & = 0.70 \\
    P(A_120 \text{ mi fa arrivare in tempo} \;|\; \ldots) & = 0.95 \\
    P(A_1440 \text{ mi fa arrivare in tempo} \;|\; \ldots) & = 0.9999 \\
  \end{aligned}
\] 
Non è immediato quale azione scegliere, questo dipende sulle \textbf{preferenze} di
chi sceglie. Le preferenze sono rappresentate dalla \textbf{Utility theory}. Per scegliere
l'azione che massimizza l'utilità attesa si usa la \textbf{Decision theory} che combina
l'utility theory con la \textbf{Probability theory Maximum Expected Utility (MEU)}.

\subsubsection{Basi di probabilità}
\begin{itemize}
  \item Il \textbf{sample space} \( \Omega  \) è l'insieme di tutti i possibili risultati
    di un esperimento casuale

  \item \( \omega \in \Omega \) è un \textbf{sample point} (o mondo possibile, o evento atomico)

  \item Un \textbf{modello probabilistico} è un \textbf{sample space} con un assegnamento
    di probabilità \( P(\omega) \) per ogni \( \omega \in \Omega  \)

  \item Un evento \( A \) è un sottoinsieme di \( \Omega  \). La probabilità di un evento
    è la somma delle probabilità dei sample point che lo compongono:
    \[
      P(A) = \sum_{\omega \in A} P(\omega)
    \]

  \item Le variabili casuali possono avere vari domini e sono soggette al cambiamento,
    cioè non si possono decidere a priori i loro valori. \( P \) induce una \textbf{distribuzione
    di probabilità} su ogni variabile casuale, cioè una funzione che assegna una probabilità
    ad ogni possibile valore della variabile:
    \[
      P(X = x_i) = \sum_{\omega \in \Omega \;|\; X(\omega) = x_i} P(\omega)
    \] 

  \item Una \textbf{proposizione} è la probabilità che una variabile casuale assuma
    un certo valore:
    \[
      P(X = x_i) \text{ è la proposizione "X assume il valore } x_i \text{"}
    \]
    Le proposizioni possono essere usate anche per creare modelli di logica proposizionale,
    ad esempio date due variabili casuali \( A \) e \( B \):
    \begin{itemize}
      \item evento \( a \) = insieme di sample points in cui \( A(\omega) = true \)
      \item evento \( \neg a \) = insieme di sample points in cui \( A(\omega) = false \)
      \item evento \( a \wedge b \) = punti dove \( A(\omega) = true \) e \( B(\omega) = true \)
    \end{itemize}
\end{itemize}
La probabilità implica che due eventi in relazione tra loro hanno probabilità 
collegate. Quando si calcolano le probabilità quindi bisogna considerare l'intersezione
degli eventi, sottraendo le sovrapposizioni dalla somma delle probabilità.

\subsubsection{Variabili casuali}
Le variabili casuali sono proposizioni atomiche, mentre invece le proposizioni composte
sono create combinando variabili casuali. I tipi di variabili casuali sono:
\begin{itemize}
  \item \textbf{Proposizionali o booleane}: assumono valori vero o falso, ad esempio:
    \[
     \text{Carie } = \text{ ho una carie?}
    \] 
    \( \text{Carie } = true \) è una proposizione atomica, scritta anche come \( carie \) 
  \item \textbf{Discrete} (finite o infinite): assumono un insieme numerabile di valori
    \[
      \text{Previsioni del tempo } = \left< \text{sole, pioggia, neve, nuvoloso} \right>
    \] 
    \( \text{Previsioni del tempo } = \text{ pioggia} \) è una proposizione. I valori
    devono essere esaustivi e mutuamente esclusivi
  \item \textbf{Continue} (limitate o illimitate): assumono un insieme non numerabile di valori
    \[
      \text{Temp } = 21.6
    \] 
\end{itemize}

\subsubsection{Eventi atomici}
Gli eventi atomici sono assegnazioni di tutte le variabili casuali. Gli eventi atomici
hanno le seguenti proprietà:
\begin{enumerate}
  \item Mutuamente esclusivi, cioè non possono essere veri contemporaneamente
  \item Esaustivi, cioè uno di essi deve essere vero
  \item Consegue una verità per ogni proposizione
  \item Qualsiasi proposizione è logicamente equivalente ad una disgiunzione di eventi atomici
    rilevanti
\end{enumerate}
Gli eventi atomici sono come i modelli per la logica proposizionale.

\subsubsection{Probabilità a priori}
La probabilità a priori è la probabilità di un evento senza alcuna conoscenza
aggiuntiva. È analoga ai fatti nella knowledge base logica.

\subsubsection{Probabilità congiunta}
La probabilità congiunta rappresenta la probabilità di ogni evento atomico di un
insieme di variabili casuali.
\begin{example}
  Consideriamo la probabilità:
  \[
    P(\text{Weather, Cavity})
  \] 
  Questa probabilità condizionata è una matrice di valori:
  \begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
      \hline
      \textbf{Weather =} & \text{Sunny} & \text{Rain} & \text{Cloudy} & \text{Snow} \\
      \hline
      \text{Cavity = True} & 0.144 & 0.02 & 0.016 & 0.02 \\
      \hline
      \text{Cavity = False} & 0.576 & 0.08 & 0.064 & 0.08 \\
      \hline
    \end{tabular}
    \caption{Esempio di probabilità condizionata}
  \end{table}
  Se si sommano tutte queste probabilità si ottiene 1, mentre se si sommano
  le probabilità per una riga o colonna si ottiene la probabilità marginale, cioè
  la probabilità incondizionata di una variabile casuale.
\end{example}

\subsubsection{Probabilità continua}
La probabilità continua è definita tramite una funzione parametrica di un valore chiamata
\textbf{funzione di densità di probabilità} (PDF).

\subsubsection{Probabilità condizionata}
La probabilità condizionata rappresenta la probabilità di un evento dato che
un altro evento sia vero. Ad esempio:
\[
  P(\text{carie} \;|\; \text{dolore}) = 0.6
\] 
rappresenta che la probabilità di avere una carie, dato che l'unica informazione disponibile
è quella in cui si ha dolore, è del 60\%.

\vspace{1em}
\noindent
La notazione per distribuzioni condizionate è:
\[
  P(C \;|\; T)
\] 
che è un vettore da due vettori di due elementi:
\[
  P(C \;|\; T) = \left[ \left[ P(c \;|\;| t), P(\neg c \;|\; t) \right],
  \left[ P(c \;|\; \neg t), P(\neg c \;|\; \neg t) \right] \right]
\] 
Alcune informazioni possono essere irrilevanti e quindi possono essere ignorate.
La probabilità condizionata può essere calcolata come:
\[
  P(a \;|\; b) = \frac{P(a \wedge b)}{P(b)} \text{ se } P(b) > 0
\] 
dove \( P(a \wedge b) \) è la probabilità congiunta di \( a \) e \( b \) e può essere
anche scritta come (regola del prodotto):
\[
  P(a \wedge b) = P(a \;|\; b) \cdot P(b) = P(b \;|\; a) \cdot P(a)
\] 
Esiste anche la regola della catena che permette di rappresentare una probabilità generica
utilizzando i condizionali:
\[
  \begin{aligned}
    P(X_1, \ldots, X_n) &= P(X_1, \ldots, X_{n-1}) \cdot P(X_n \;|\; X_1, \ldots, X_{n-1}) \\
                        &= P(X_1, \ldots, X_{n-2}) P(X_{n-1} \;|\; X_1, \ldots, X_{n-2})
                        P(X_n \;|\; X_1, \ldots, X_{n-1}) \\
                        &= \ldots \\
                        &= \prod_{i=1}^{n} P(X_i \;|\; X_1, \ldots, X_{i-1})
  \end{aligned}
\] 

\subsubsection{Inferenza per enumerazione}
Per ogni proposizione \( \varphi  \), si sommano gli eventi atomici dove è vero:
\[
  P(\varphi) = \sum_{\omega : \omega \models \varphi} P(\omega)
\] 
Ricordando che:
\begin{itemize}
  \item Ogni proposizione \( \varphi  \) è equivalente alla disgiunzione degli eventi atomici
    in cui è vera
  \item Gli eventi atomici sono mutuamente esclusivi
\end{itemize}
\begin{example}
  Consideriamo la seguente tabella di probabilità congiunta:
  \begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{joint_probability_1}
    \caption{Tabella di esempio}
  \end{figure}
  La probabilità di avere \textit{toothache} è data dalla somma delle probabilità
  degli eventi atomici in cui \textit{toothache} è vero:
  \begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{joint_probability_2}
    \caption{Calcolo della probabilità di toothache}
  \end{figure}
  \[
    P(toothache) = 0.108 + 0.012 + 0.016 + 0.064 = 0.2
  \] 
  Si può anche calcolare la probabilità congiunta di due eventi:
  \begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{joint_probability_3}
    \caption{Calcolo della probabilità congiunta di toothache e cavity}
  \end{figure}
  \[
    P(cavity \wedge toothache) = 0.108 + 0.012 + 0.072 + 0.008 + 0.016 + 0.064 = 0.28
  \] 
  Oppure anche probabilità condizionate:
  \begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{joint_probability_4}
    \caption{Calcolo della probabilità condizionata di not cavity dato toothache}
  \end{figure}
  \[
    \begin{aligned}
      P(\neg cavity \;|\; toothache) &= \frac{P(\neg cavity \wedge toothache)}{P(toothache)}\\
                                     &= \frac{0.016 + 0.064}{0.108 + 0.012 + 0.016 + 0.064} \\
                                     &= 0.4
    \end{aligned}
  \] 
\end{example}

\subsubsection{Normalizzazione}
L'idea è quella di calcolare la distribuzione di probabilità su una variabile di interesse
fissando delle \textbf{variabili di evidenza} e sommando su tutte le \textbf{variabili
nascoste}, cioè quelle non di interesse.
\begin{example}
  Consideriamo la tabella dell'esempio precedente, si può calcolare la probabilità
  condizionata e considerare il denominatore come una costante di normalizzazione:
  \begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{joint_probability_5}
    \caption{Calcolo della probabilità condizionata di cavity dato toothache}
  \end{figure}
  \[
    \begin{aligned}
      P(Cavity \;|\; Toothache) &= \alpha P(Cavity, Toothache)\\
                                &= \alpha \left[ 
                                  \begin{aligned}
                                    & P(Cavity = true, Toothache = true) \\
                                    & P(Cavity = false, Toothache = true)
                                  \end{aligned}
                                \right] \\
                                &= \alpha \left[ 
                                  \begin{aligned}
                                    & \left<0.108, 0.016 \right> \\
                                    & \left<0.012, 0.064 \right>
                                  \end{aligned}
                                \right] \\
                                &= \alpha \left< 0.12, 0.08 \right> \\
                                &= \left< 0.6, 0.4 \right>
    \end{aligned}
  \] 
  Il risultato finale è un vettore la cui somma dei valori è 1.
\end{example}

\subsection{Indipendenza}
Due variabili casuali \( A \) e \( B \) sono indipendenti se e solo se:
\[
  P(A \;|\; B) = P(A) \quad \text{ oppure } \quad P(B \;|\; A) = P(B) 
  \text{ oppure } \quad P(A, B) = P(A) \cdot P(B)
\]

\subsubsection{Indipendenza condizionata}
Non sempre si possono considerare due variabili casuali come indipendenti, ma 
possono esistere delle terze variabili che non influenzano la relazione tra le prime due.
\begin{example}
  Consideriamo l'esempio del dentista, se sappiamo che una persona ha una carie, la
  probabilità che la sonda del dentista si incastri tra i denti non dipende dal fatto che
  la persona abbia dolore ai denti o meno. Quindi:
  \[
    P(catch \;|\; toothache, cavity) = P(catch \;|\; cavity)
  \] 
  e lo stesso vale se la persona non ha una carie:
  \[
    P(catch \;|\; toothache, \neg cavity) = P(catch \;|\; \neg cavity)
  \] 
  Quindi \( Catch \) è \textbf{condizionatamente indipendente} da \( Toothache \) dato\\
  \( Cavity \):
  \[
    P(Catch \;|\; Toothache, Cavity) = P(Catch \;|\; Cavity)
  \] 
  Si può quindi ignorare una variabile quando si calcola la probabilità condizionata.
\end{example}

\subsubsection{Regola di Bayes}
La regola di Bayes permette di calcolare la probabilità condizionata invertendo
i condizionali:
\[
  P(causa \;|\; effetto) = \frac{P(effetto \;|\; causa) \cdot P(causa)}{P(effetto)}
\]
Questa regola è utile quando \( P(effetto \;|\; causa) \) è più facile da calcolare rispetto a
\( P(causa \;|\; effetto) \).

\section{Sequential Decision Making}
Le azioni vengono raramente scelte individualmente, ma sono parte di \textbf{sequenze} di azioni.
Per valutare un'azione si deve andare oltre il semplice risultato immediato e considerare:
\begin{itemize}
  \item Utilità a lungo termine che deriva dall'azione
  \item Acquisire nuove informazioni che possono influenzare le azioni future
\end{itemize}

\begin{example}
  Consideriamo il problema di esplorazione di un labirinto in cui le azioni
  hanno una certa probabilità di successo:
  \begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{esempio_labirinto}
    \caption{Esempio di labirinto}
  \end{figure}
  \begin{itemize}
    \item Stati \( s \in S \)
    \item Azioni \( a \in A \)
    \item Modello: \( T(s,a,s') \equiv P(s'|s, a) \) è la probabilità che l'azione \( a \) 
      nello stato \( s \) porti allo stato \( s' \)
    \item Funzione di ricompensa: \( R(s) \) (o \( R(s,a), R(s,a,s') \))
      \[
        R(s) = \begin{cases}
          -0.04 & \text{ (piccola penalità) se lo stato non è terminale} \\
          \pm 1 & \text{Per gli stati terminali}
        \end{cases}
      \] 

      Un approccio può essere il seguente:
      \begin{figure}[H]
        \centering
        \includegraphics[width=0.9\textwidth]{esempio_labirinto_2}
        \caption{Esempio di approccio al labirinto}
      \end{figure}
      \begin{itemize}
        \item I nodi rossi sono nodi chance, cioè rappresentano l'incertezza
        \item I nodi blu sono nodi di decisione, cioè rappresentano le azioni
      \end{itemize}
      I passi per calcolare la miglior sequenza di azioni sono:
      \begin{enumerate}
        \item Assegnare un utilità per ogni traiettoria, ad esempio:
          \[
            u(s_1 \to s_2 \to s_6)
          \] 
        \item Per ogni sequenza di azioni calcola la probabilità di ogni traiettoria:
          \[
            Pr(s_1 \to s_2 \to s_6 \;|\; [a_1, a_1]) = 0.9 * 0.7 = 0.63
          \] 
        \item Calcola EU (Expected Utility) per ogni sequenza di azioni:
          \[
            EU([a_1, a_1]), EU([a_1, a_2]), \ldots
          \] 
        \item Scegli la sequenza di azioni con la massima EU
      \end{enumerate}
      I problemi di questo approccio sono:
      \begin{itemize}
        \item \textbf{Concettuale}: valutare tutte le sequenze delle azioni \textbf{senza
          considerare l'outcome reale non è la miglior strategia} 

        \item \textbf{Pratico}: l'utilità per una sequenza è tipicamente più difficile da stimare rispetto
          all'utilità di singoli stati

        \item \textbf{Computazionale}: il numero di traiettorie cresce esponenzialmente
          \[
            k^t n^t
          \] 
          dove:
          \begin{itemize}
            \item \( k \) = numero di azioni
            \item \( n \) = outcome per ogni azione
            \item \( t \) = lunghezza della sequenza
          \end{itemize}
      \end{itemize}
  \end{itemize}
\end{example}
Nei problemi di ricerca l'obiettivo è quello di trovare una \textbf{sequenza di azioni
ottimale}. Considerando l'incertezza, l'obiettivo diventa quello di trovare una
\textbf{politica ottimale} (policy) \( \pi(s) \), ad esempio l'azione migliore per ogni
possibile stato. La politica ottimale massimizza \textbf{la somma delle ricompense attese}.

\subsection{Decision trees}
Un albero di decisione è una rappresentazione ad albero delle decisioni che l'agente
deve fare.
\begin{example}
  Consideriamo il seguente albero di decisione non deterministico:
  \begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{esempio_decision_tree}
    \caption{Esempio di albero di decisione}
  \end{figure}
  \noindent
  In questo albero:
  \begin{itemize}
    \item I nodi quadrati rappresentano le decisioni dell'agente. Il valore rappresenta
      il massimo tra le expected utility dei nodi figli
    \item I nodi rotondi rappresentano i nodi chance. Il valore del nodo è l'expected
      utility, ad esempio:
      \[
        0.2 \cdot 3000 + 0.5 \cdot 2000 + 0.3 \cdot (-6000) = -200
      \] 
  \end{itemize}
\end{example}

\subsubsection{Risoluzione di alberi di decisione}
Esiste un algoritmo che risolve gli alberi di decisione chiamato \textbf{backward induction}
(o rollback o expectimax). L'idea dell'algoritmo è:
\begin{enumerate}
  \item Si parte dalle radici e si usa la maximum expected utility
  \item Il valore di un nodo foglia \( C \) è:
    \[
      EU(C) = V(C)
    \] 
  \item Il valore di un nodo chance \( C \) non foglia è:
    \[
      EU(C) = \sum_{D \in Child(C)} Pr(D) \cdot EU(D)
    \] 
  \item Il valore di un nodo decisione \( D \) è:
    \[
      EU(D) = \max_{C \in Child(D)} EU(C)
    \] 
  \item La policy massimizza l'utility dei nodi decisione:
    \[
      \pi(D) = {\arg\max}_{C \in Child(D)} EU(C)
    \] 
\end{enumerate}
Un decision tree con nodi ripetuti si espande in un albero molto grande.

\subsection{Markov Decision Processes}
Un Markov Decision Process (MDP) è una classe generale di problemi di decisione sequenziali
più efficiente dei decision tree. Un MDP è definito da una tupla di 4 componenti:
\[
  \left<S, A, R, Pr\right>
\] 
dove:
\begin{itemize}
  \item \( S \): insieme finito di stati \( |S| = n \) 
  \item \( A \): insieme finito di azioni \( |A| = m \)
  \item \( p(s' \;|\; s, a) = Pr \left\{ S_{t+1} = s' \;|\; S_t = s, A_t = a \right\} \): 
    Funzione di transizione
  \item \( r(s', a, s) = \mathbb{E}\left[ R_{t+1} \;|\; S_{t+1} = s', A_t = a, S_t = s \right] \):
    Funzione di ricompensa
\end{itemize}
Questa rappresentazione deriva dalla \textbf{Markov Chain}, cioè che dato uno stato corrente
il futuro è indipendente dal passato. Nelle MDP le azioni e gli stati passati sono
irrilevanti quando si prende una decisione in un certo stato.

\subsubsection{Proprietà}
Le proprietà delle MDP sono:
\begin{itemize}
  \item \textbf{Markov Dynamics}: Indipendenza dalla storia:
    \[
      Pr\left\{ R_{t+1}, S_{t+1} \;|\; S_0, A_0, R_1, \ldots, S_{t-1}, A_{t-1}, R_t, S_t, A_t \right\}
    \] 
    Se ci si trova in uno stato, le azioni e le ricompense passate non influenzano
    la probabilità del prossimo stato e della prossima ricompensa

  \item \textbf{Stazionarietà}: Le dinamiche non cambiano nel tempo:
    \[
      Pr\left\{ R_{t+1}, S_{t+1} \;|\; S_t, A_t \right\} = Pr\left\{ R_{t'+1}, S_{t'+1} \;|\; S_{t'}, A_{t'} \right\}
      \forall t, t'
    \] 
    Cioè le probabilità di transizione e le ricompense non dipendono dal tempo

  \item \textbf{Completa osservabilità}: Non si può predirre esattamente che stato si
    raggiungerà, però si conosce sempre lo stato corrente
\end{itemize}

\begin{example}
  Consideriamo l'esempio di un robot che deve muoversi in un ambiente e riciclare
  delle lattine. Le possibili azioni sono:
  \begin{itemize}
    \item Cerca una lattina (alta probabilità, ma potrebbe finire la batteria)
    \item Aspettare che qualcuno gli porti una lattina (bassa probabilità, ma non consuma
      batteria)
    \item Andare alla stazione di ricarica per caricare la batteria
  \end{itemize}
  L'agente decide in base al livello di batteria: \( \left\{ low, high \right\} \).
  Le azioni dipendono dallo stato:
  \[
    \begin{aligned}
      A(low) & = \left\{ search, wait \right\} \\
      A(high) & = \left\{ search, wait, recharge \right\}
    \end{aligned}
  \] 
  \begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{esempio_MDP}
    \caption{Esempio di MDP}
  \end{figure}
  Si vuole capire la scelta migliore che il robot deve fare in ogni stato per massimizzare
  la ricompensa attesa.
\end{example}

\subsubsection{Tipi di policy}
Esistono diversi tipi di policy:
\begin{itemize}
  \item \textbf{Policy non stazionaria}: la politica dipende dal tempo \( t \):
    \[
      \pi : S \times T \to A
    \] 
    \( \pi(s, t) \) è l'azione allo stato \( s \) con \( t \) passi rimanenti

  \item \textbf{Policy stazionaria}: la politica non dipende dal tempo:
    \[
      \pi : S \to A
    \] 
    \( \pi(s) \) è l'azione allo stato \( s \) (indipendente dal tempo)

  \item \textbf{Policy stocastica}: la politica assegna una distribuzione di probabilità
    sulle azioni:\\
    \( \pi(a \;|\; s) \) è la probabilità di scegliere l'azione \( a \) nello stato \( s \)
\end{itemize}

\subsubsection{Decisione tra policy}
Bisogna trovare un metodo per valutare e confrontare le policy, cioè una sequenza di
ricompense. Tipicamente si considerano le \textbf{stationary preferences}:
\[
  \left[ r, r_0, r_1, r_2, \ldots \right] \succ \left[ r, r'_0, r'_1, r'_2, \ldots \right]
  \iff
  \left[ r_0, r_1, r_2, \ldots \right] \succ \left[ r'_0, r'_1, r'_2, \ldots \right]
\] 

\begin{theorem}
  Ci sono soltanto due modi per combinare le ricompense nel tempo:
  \begin{enumerate}
    \item \textbf{Funzione di utilità additiva}:
      \[
        U\left( [s_0, s_1, s_2, \ldots] \right) = R(s_0) + R(s_1) + R(s_2) + \ldots
      \] 
      Le ricompense future hanno lo stesso peso di quelle presenti.

    \item \textbf{Funzione di utilità scontata}:
      \[
        U\left( [s_0, s_1, s_2, \ldots] \right) = R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2) + \ldots
      \] 
      dove \( 0 < \gamma < 1 \) è il fattore di sconto.
      Più la ricompensa è nel futuro, meno valore ha.
  \end{enumerate}
\end{theorem}

\subsubsection{Valore di una policy}
Bisogna trovare un valore per determinare quanto è buona una policy:
\[
  V : S \to \mathcal{R}
\] 
Questa funzione associa un valore considerando le \textbf{ricompense acccumulate}.
Mentre invece la funzione \( v_{\pi}(s) \) rappresenta il valore della policy \( \pi \)
nello stato \( s \), cioè le ricompense accumulate attese in un orizzonte di tempo
di interesse.

\vspace{1em}
\noindent
Se le sequenze di ricompense sono infinite, cioè si hanno infiniti stati (infinite
horizon problems), si possono utilizzare le seguenti soluzioni:
\begin{itemize}
  \item \textbf{Si sceglie un orizzonte finito}:
    \begin{itemize}
      \item Si termina dopo un certo numero di passi \( T \) 
      \item Produce policy non stazionarie
    \end{itemize}

  \item \textbf{Absorbing states}: garantiscono che per ogni policy verrà raggiunto
    eventualmente uno stato terminale
  \item \textbf{Discounting}: si usa un fattore di sconto \( \gamma \) per
    ridurre l'importanza delle ricompense future:
    \[
      U\left( [r_0, \ldots, r_{\infty}] \right) = \sum_{t=0}^{\infty} \gamma^t r_t
      \le \frac{r_{max}}{1 - \gamma} \quad \forall 0 < \gamma < 1
    \] 
    Quindi la serie converge.\\
    Se si utilizza un \( \gamma  \) più basso si ragiona su un orizzonte più breve e
    quindi ricompense ottenute prima hanno più utilità rispetto a quelle ottenute nel
    futuro.
\end{itemize}

\vspace{1em}
\noindent
Quindi in breve:
\begin{itemize}
  \item Se l'orizzonte \( T \) è finito si usa la ricompensa totale attesa data la policy
    \( \pi  \) 

  \item Se l'orizzonte è infinito si usa la somma delle ricompense accumulate attese 
    scontate data la policy \( \pi  \)

  \item Si può anche usare la ricompensa media per ogni passo
\end{itemize}

\subsubsection{Risolvere un MDP}
La soluzione di un MDP è una policy ottimale \( \pi^* \) che massimizza l'expected
utility da ogni stato \( s \) seguendo quella policy \( \pi  \), cioè la ricompensa
scontata accumulata attesa:
\[
  v_{\pi}(s) = \mathbb{E} \left\{ \sum_{k=0}^{\infty} \gamma^k r_{t+k+1} \;|\; s_t = s \right\}
\] 
La Q-value (quality functiono action-value) è il vaore di prendere un'azione \( a \) nello stato \( s \)
seguendo la policy \( \pi  \):
\[
  q_{\pi}(s, a) = \sum_{s'} p(s' \;|\; a, s) \left( r(s, a, s') + \gamma v_{\pi}(s') \right)
\] 
\begin{define}
  Si nota che:
  \[
    v_{\pi}(s) = q_{\pi}(s, \pi(s))
  \] 
\end{define}

\subsubsection{Equazioni di Bellman}
Le equazioni di Bellman forniscono una relazione ricorsiva tra i valori degli stati
e i valori delle azioni. I valori dello stato iniziale deve essere uguale al valore
(scontato) atteso dello stato successivo più la ricompensa attesa lungo la transizione:
\[
  v_{\pi}(s) = \sum_{s'} p(s' \;|\; \pi(s), s) \left( r(s, \pi(s), s') + \gamma v_{\pi}(s') \right)
\] 
\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{diagramma_bellman}
  \caption{Rappresentazione delle equazioni di Bellman}
\end{figure}

\vspace{1em}
\noindent
Una policy è ottima se e solo se:
\[
  v_{\pi^*}(s) \ge v_{\pi}(s) \quad \forall s, \pi 
\] 
Il valore dell'expected utility partendo da \( s \) e agendo in modo ottimale dopo è:
\[
  v_{*}(s) = \max_{\pi} v_{\pi}(s)
\] 
Il valore ottimale del Q-value è:
\[
  q_{*}(s, a) = \max_{\pi} q_{\pi}(s, a)
\] 
\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{diagramma_bellman_ottimale}
  \caption{Rappresentazione delle equazioni di Bellman per la policy ottimale}
\end{figure}

\vspace{1em}
\noindent
La policy ottima è calcolata come:
\[
  \pi^*(s) = \max_{a \in \mathcal{A}(s)} q_{*}(s, a) =
  \max_{a \in \mathcal{A}(s)} \sum_{s'} p(s' \;|\; a, s) \left( r(s, a, s') + \gamma v_{*}(s') \right)
\] 

\subsubsection{Value iteration}
Il value iteration è un algoritmo per calcolare la policy ottimale. L'idea è
quella di trasformare l'equazione di ottimalità di Bellman in un'operazione
di aggiornamento iterativo, combinando la valutazione della policy (calcolando
il valore \( v_{\pi } \) di una data policy) e il miglioramento della policy
(rendendo la policy greedy rispetto al valore calcolato).
La funzione di aggiornamento è:
\[
  v_{k+1}(s) = \max_{a} \sum_{s'} p(s' \;|\; a, s) \left( r(s, a, s') + \gamma v_k(s') \right)
\] 
L'algoritmo è il seguente:
\begin{lstlisting}[language=Python]
Initialize array v with zeros for all s in S

do
  delta = 0
  for s in S:
      v_old = v[s]
      v[s] = max_a sum_{s'} p(s'|a,s) * (r(s,a,s') + gamma * v[s'])
      delta = max(delta, abs(v_old - v[s]))
while delta > threshold

Output a deterministic policy pi such that:
  pi(s) = argmax_a sum_{s'} p(s'|s,a) * (r(s,a,s') + gamma * v[s'])
\end{lstlisting}
Le caratteristiche dell'algoritmo sono:
\begin{itemize}
  \item Garantisce la convergenza al valore ottimale \( v_* \).

  \item Per orizzonti infiniti la policy ottima è stazionaria.

  \item La complessità per ogni iterazione è quadratica nel numero degli stati
    e lineare nel numero delle azioni: \( O(n^2 m) \).

  \item Il rate di convergenza è lineare.
\end{itemize}

\subsubsection{Policy iteration}
La policy iteration è un altro algoritmo per calcolare la policy ottimale. L'idea è
quella di alternare la valutazione della policy e il miglioramento della policy
fino a quando la policy non cambia più. Quindi i passi dell'algoritmo sono:
\begin{lstlisting}[language=Python]
pi = an arbitrary initial policy
repeat until no change in pi
  compute utilities given pi # policy evaluation
  update pi as if utilities are correct # policy improvement
\end{lstlisting}
\begin{enumerate}
  \item \textbf{Policy evaluation}: per calcolare le utilities data una policy \( \pi  \)
    fissa, si risolve il seguente sistema di \( n \) equazioni lineari ad \( n \) incognite:
    \[
      v_(s) = \sum_{s'} p(s' \;|\; \pi(s), s) \left( r(s, \pi(s), s') + \gamma v(s') \right)
    \] 
    La risoluzione viene fatta in \( O(n^3) \).

  \item \textbf{Policy improvement}: dato il valore di tutti gli stati \( v(s) \):
    \begin{itemize}
      \item Cambiare in modo greedy la prima azione presa in uno stato basandosi
        sul valore corrente degli stati
      \item Se il valore dello stato può essere migliorato, la nuova azione viene
        utilizzata dalla policy e quindi la performance della policy migliora
    \end{itemize}
\end{enumerate}

\vspace{1em}
\noindent
Questo algoritmo è molto costoso, quindi una modifica è quella di eseguire pochi step
di value iteration (ma con \( \pi  \) fissa) iniziando dalla funzione di valore
dell'iterazione precedente e poi eseguire la policy iteration. L'algoritmo è il
seguente:
\[
\begin{aligned}
  &\text{1. Initialization}\\
  &v(s) \in \mathbb{R} \text{ and } \pi(s) \in A(s) \text{ arbitrarily } \forall s \in S\\
  \\
  &\text{2. Policy Evaluation}\\
  &\text{Repeat}\\
  &\quad \Delta \leftarrow 0\\
  &\quad \text{For each } s \in S:\\
  &\quad \quad v_{old} \leftarrow v(s)\\
  &\quad \quad v(s) \leftarrow \sum_{s'} p(s' \;|\; s, \pi(s)) \left[ r(s, \pi(s), s') + \gamma v(s') \right]\\
  &\quad \quad \Delta \leftarrow \max(\Delta, |v_{old} - v(s)|)\\
  &\text{Until } \Delta < \theta \text{ (small pos number)}\\
  \\
  &\text{3. Policy Improvement}\\
  &\text{policy\_stable} \leftarrow true\\
  &\text{For each } s \in S:\\
  &\quad \text{old\_action} \leftarrow \pi(s)\\
  &\quad \pi(s) \leftarrow {\arg\max}_a \sum_{s'} p(s' \;|\; s, a) \left[ r(s, a, s') + \gamma v(s') \right]\\
  &\quad \text{If old\_action } \ne \pi(s) \text{ then policy\_stable } \leftarrow false\\
  &\text{If policy\_stable}\\
  &\quad \text{Then return } \pi \text{ and } v\\
  &\quad \text{Else go to step 2.}
\end{aligned}
\] 
Questo algoritmo garantisce l'ottimalità della policy.

\end{document}
