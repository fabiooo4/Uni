{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planning-Lab Lesson 3: Markov Decision Process\n",
    "\n",
    "In the third session we will work on the Markov decision process (MDP)\n",
    "\n",
    "## Lava environments\n",
    "The environments used are LavaFloor (visible in the figure) and its variations.\n",
    "\n",
    "![Lava](images/lava.png)\n",
    "\n",
    "The agent starts in cell $(0, 0)$ and has to reach the treasure in $(2, 3)$. In addition to the walls of the previous environments, the floor is covered with lava, there is a black pit of death.\n",
    "\n",
    "Moreover, the agent can't comfortably perform its actions that instead have a stochastic outcome (visible in the figure):\n",
    "\n",
    "![Dynact](images/dynact.png)\n",
    "\n",
    "The action dynamics is the following:\n",
    "- $P(0.8)$ of moving **in the desired direction**\n",
    "- $P(0.1)$ of moving in a direction 90Â° with respect to the desired direction\n",
    "\n",
    "Finally, since the floor is covered in lava, the agent receives a negative reward for each of its steps!\n",
    "\n",
    "- -0.04 for each lava cell (L)\n",
    "- -5 for the black pit (P). End of episode\n",
    "- +1 for the treasure (G). End of episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, random\n",
    "module_path = os.path.abspath(os.path.join('../tools'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import gym, envs\n",
    "from utils.ai_lab_functions import *\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Properties \n",
    "\n",
    "In addition to the varables of the environments you have been using in the previous sessions, there are also a few more:\n",
    "\n",
    "- $T$: matrix of the transition function $T(s, a, s') \\rightarrow [0, 1]$\n",
    "- $RS$: matrix of the reward function $R(s) \\rightarrow \\mathbb{R}$\n",
    "\n",
    "The available actions are still Left, Right, Up, Down.\n",
    "\n",
    "#### Code Hints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions:  4\n",
      "Actions:  {0: 'L', 1: 'R', 2: 'U', 3: 'D'}\n",
      "Reward of starting state: -0.04\n",
      "Reward of goal state: 1.0\n",
      "Probability from (0, 0) to (0, 1) with action right: 0.8\n",
      "Probability from (0, 0) to (2, 3) with action right: 0.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LavaFloor-v0\")\n",
    "\n",
    "current_state = env.pos_to_state(0, 0)\n",
    "next_state = env.pos_to_state(0, 1)\n",
    "goal_state = env.pos_to_state(2, 3)\n",
    "\n",
    "print(\"Number of actions: \", env.action_space.n)\n",
    "print(\"Actions: \", env.actions)\n",
    "print(\"Reward of starting state:\", env.RS[current_state])\n",
    "print(\"Reward of goal state:\", env.RS[goal_state])\n",
    "print(\"Probability from (0, 0) to (0, 1) with action right:\", env.T[current_state, 1, next_state])\n",
    "print(\"Probability from (0, 0) to (2, 3) with action right:\", env.T[current_state, 1, goal_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of time agent reaches the state to the right:  76.0\n",
      "Transition model for  LavaFloor-v0  : \n",
      "probability of reaching  (0, 1) from  (0, 0)  executing action  L  :  0.0\n",
      "probability of reaching  (0, 1) from  (0, 0)  executing action  R  :  0.8\n",
      "probability of reaching  (0, 1) from  (0, 0)  executing action  U  :  0.1\n",
      "probability of reaching  (0, 1) from  (0, 0)  executing action  D  :  0.1\n",
      "Reward for non terminal states:  -0.04\n",
      "Reward for state : (1, 3)  (state type:  P ) :  -5.0\n",
      "Reward for state : (2, 3)  (state type:  G ) :  1.0\n"
     ]
    }
   ],
   "source": [
    "env_name = \"LavaFloor-v0\"\n",
    "env = gym.make(env_name)\n",
    "\n",
    "c=0\n",
    "state_right = env.pos_to_state(0, 1) #state to the tight of start state\n",
    "for i in range(1,101):\n",
    "    current_state = env.pos_to_state(0, 0)\n",
    "    state = env.sample(current_state, 1) #trying to go right\n",
    "    if (state==state_right): \n",
    "        c+=1 #counting how many times the agent reaches the state to the right\n",
    "        \n",
    "#computing percentage of time agent reached the state to the right going right, should be around 80%           \n",
    "print(\"percentage of time agent reaches the state to the right: \", c/i*100) \n",
    "\n",
    "print(\"Transition model for \", env_name, \" : \") #assume transition functions is the same for all states\n",
    "state=0\n",
    "next_state=1\n",
    "for i in range(0,env.action_space.n):\n",
    "    print(\"probability of reaching \", env.state_to_pos(next_state), \"from \", env.state_to_pos(state), \" executing action \", env.actions[i], \" : \", env.T[state, i, next_state])\n",
    "print(\"Reward for non terminal states: \",env.RS[env.pos_to_state(0,0)]) #assume all states have same reward\n",
    "for state in range(0,env.observation_space.n):\n",
    "    if env.grid[state] == \"P\" or env.grid[state] == \"G\":\n",
    "                    print(\"Reward for state :\", env.state_to_pos(state) ,\" (state type: \", env.grid[state],\") : \",env.RS[state])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1: Value Iteration Algorithm\n",
    "\n",
    "Your first assignment is to implement the Value Iteration algorithm on LavaFloor. The solution returned by your algorithm must be a 1-d array of action identifiers where the $i$-th action refers to the $i$-th state.  You can perform all the test on a different versions of the environment, but with the same structure: *HugeLavaFloor*, *NiceLavaFloor* and *VeryBadLavaFloor*.\n",
    "\n",
    "<img src=\"images/value-iteration.png\" width=\"600\">\n",
    "\n",
    "The *value_iteration* function has to be implemented. Notice that the value iteration approach return a matrix with the value for eacht state, the function *values_to_policy* automatically convert this matrix in the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(environment, maxiters=300, discount=0.9, max_error=1e-3):\n",
    "    \"\"\"\n",
    "    Performs the value iteration algorithm for a specific environment\n",
    "    \n",
    "    Args:\n",
    "        environment: OpenAI Gym environment\n",
    "        maxiters: timeout for the iterations\n",
    "        discount: gamma value, the discount factor for the Bellman equation\n",
    "        max_error: the maximum error allowd in the utility of any state\n",
    "        \n",
    "    Returns:\n",
    "        policy: 1-d dimensional array of action identifiers where index `i` corresponds to state id `i`\n",
    "    \"\"\"\n",
    "    \n",
    "    U_1 = [0 for _ in range(environment.observation_space.n)] # vector of utilities for states S\n",
    "    delta = 0 # maximum change in the utility o any state in an iteration\n",
    "    U = U_1.copy()\n",
    "    #\n",
    "    # Code Here!\n",
    "    #\n",
    "    return values_to_policy(np.asarray(U), env) # automatically convert the value matrix U to a policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following code executes Value Iteration and prints the resulting policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ENV RENDER:\n",
      "[['S' 'L' 'L' 'L']\n",
      " ['L' 'W' 'L' 'P']\n",
      " ['L' 'L' 'L' 'G']]\n",
      "Transition model for  LavaFloor-v0  : \n",
      "probability of reaching  (0, 1) from  (0, 0)  executing action  L  :  0.0\n",
      "probability of reaching  (0, 1) from  (0, 0)  executing action  R  :  0.8\n",
      "probability of reaching  (0, 1) from  (0, 0)  executing action  U  :  0.1\n",
      "probability of reaching  (0, 1) from  (0, 0)  executing action  D  :  0.1\n",
      "Reward for non terminal states:  -0.04\n",
      "Reward for state : (1, 3)  (state type:  P ) :  -5.0\n",
      "Reward for state : (2, 3)  (state type:  G ) :  1.0\n",
      "\n",
      "EXECUTION TIME: \n",
      "0.002\n",
      "\n",
      "\u001b[96m#################################################################\u001b[0m\n",
      "\u001b[96m#######  Environment: LavaFloor-v0 \tValue Iteration  ########\u001b[0m\n",
      "\u001b[96m#################################################################\u001b[0m\n",
      "\n",
      "\u001b[91m> Your policy\n",
      " [['L' 'L' 'L' 'L']\n",
      " ['L' 'L' 'L' 'L']\n",
      " ['L' 'L' 'L' 'L']] is not optimal!\n",
      "\n",
      "Our policy is:\n",
      " [['D' 'L' 'L' 'U']\n",
      " ['D' 'L' 'L' 'L']\n",
      " ['R' 'R' 'R' 'L']]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env_name = \"LavaFloor-v0\"\n",
    "#env_name = \"HugeLavaFloor-v0\"\n",
    "#env_name = \"NiceLavaFloor-v0\"\n",
    "#env_name = \"VeryBadLavaFloor-v0\"\n",
    "\n",
    "\n",
    "\n",
    "env = gym.make(env_name)\n",
    "print(\"\\nENV RENDER:\")\n",
    "env.render()\n",
    "\n",
    "print(\"Transition model for \", env_name, \" : \") #assume transition functions is the same for all states\n",
    "state=0\n",
    "next_state=1\n",
    "for i in range(0,env.action_space.n):\n",
    "    print(\"probability of reaching \", env.state_to_pos(next_state), \"from \", env.state_to_pos(state), \" executing action \", env.actions[i], \" : \", env.T[state, i, next_state])\n",
    "print(\"Reward for non terminal states: \",env.RS[env.pos_to_state(0,0)]) #assume all states have same reward\n",
    "for state in range(0,env.observation_space.n):\n",
    "    if env.grid[state] == \"P\" or env.grid[state] == \"G\":\n",
    "                    print(\"Reward for state :\", env.state_to_pos(state) ,\" (state type: \", env.grid[state],\") : \",env.RS[state])\n",
    "\n",
    "t = timer()\n",
    "policy = value_iteration(env)\n",
    "\n",
    "print(\"\\nEXECUTION TIME: \\n{}\".format(round(timer() - t, 4)))\n",
    "policy_render = np.vectorize(env.actions.get)(policy.reshape(env.rows, env.cols))\n",
    "results = CheckResult_L3(env_name, policy_render)\n",
    "results.check_value_iteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: Policy Iteration Algorithm (<span style=\"color:red\">*OPTIONAL*</span>)\n",
    "\n",
    "Your <span style=\"color:red\"> *optional*</span> assignment is to implement the Policy Iteration algorithm on LavaFloor. The solution returned by your algorithm must be a 1-d array of action identifiers where the $i$-th action refers to the $i$-th state. You can perform all the test on a different versions of the environment, but with the same structure: *HugeLavaFloor*, *NiceLavaFloor* and *VeryBadLavaFloor*.\n",
    "\n",
    "<img src=\"images/policy-iteration.png\" width=\"600\">\n",
    "\n",
    "For the *policy evaluation step*, it is necessary to implement this function:\n",
    "\n",
    "<img src=\"images/policy-evaluating.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following function has to be implemented:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(environment, maxiters=150, discount=0.9, maxviter=10):\n",
    "    \"\"\"\n",
    "    Performs the policy iteration algorithm for a specific environment\n",
    "    \n",
    "    Args:\n",
    "        environment: OpenAI Gym environment\n",
    "        maxiters: timeout for the iterations\n",
    "        discount: gamma value, the discount factor for the Bellman equation\n",
    "        \n",
    "    Returns:\n",
    "        policy: 1-d dimensional array of action identifiers where index `i` corresponds to state id `i`\n",
    "    \"\"\"\n",
    "    \n",
    "    policy = [0 for _ in range(environment.observation_space.n)] #initial policy\n",
    "    U = [0 for _ in range(environment.observation_space.n)] #utility array\n",
    "\n",
    "    # Step (1): Policy Evaluation\n",
    "    #\n",
    "    # Code Here!\n",
    "    #\n",
    "    \n",
    "    # Step (2) Policy Improvement\n",
    "    unchanged = True  \n",
    "    #\n",
    "    # Code Here!\n",
    "    #\n",
    "    \n",
    "    return np.asarray(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following code executes and Value Iteration and prints the resulting policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ENV RENDER:\n",
      "[['S' 'L' 'L' 'L']\n",
      " ['L' 'W' 'L' 'P']\n",
      " ['L' 'L' 'L' 'G']]\n",
      "Transition model for  LavaFloor-v0  : \n",
      "probability of reaching  (0, 1) from  (0, 0)  executing action  L  :  0.0\n",
      "probability of reaching  (0, 1) from  (0, 0)  executing action  R  :  0.8\n",
      "probability of reaching  (0, 1) from  (0, 0)  executing action  U  :  0.1\n",
      "probability of reaching  (0, 1) from  (0, 0)  executing action  D  :  0.1\n",
      "Reward for non terminal states:  -0.04\n",
      "Reward for state : (1, 3)  (state type:  P ) :  -5.0\n",
      "Reward for state : (2, 3)  (state type:  G ) :  1.0\n",
      "\n",
      "EXECUTION TIME: \n",
      "0.0002\n",
      "\n",
      "\u001b[96m##################################################################\u001b[0m\n",
      "\u001b[96m#######  Environment: LavaFloor-v0 \tPolicy Iteration  ########\u001b[0m\n",
      "\u001b[96m##################################################################\u001b[0m\n",
      "\n",
      "\u001b[91m> Your policy\n",
      " [['L' 'L' 'L' 'L']\n",
      " ['L' 'L' 'L' 'L']\n",
      " ['L' 'L' 'L' 'L']] is not optimal!\n",
      "\n",
      "Our policy is:\n",
      " [['D' 'L' 'L' 'U']\n",
      " ['D' 'L' 'L' 'L']\n",
      " ['R' 'R' 'R' 'L']]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env_name = \"LavaFloor-v0\"\n",
    "#env_name = \"HugeLavaFloor-v0\"\n",
    "#env_name = \"NiceLavaFloor-v0\"\n",
    "#env_name = \"VeryBadLavaFloor-v0\"\n",
    "\n",
    "env = gym.make(env_name)\n",
    "print(\"\\nENV RENDER:\")\n",
    "env.render()\n",
    "\n",
    "print(\"Transition model for \", env_name, \" : \") #assume transition functions is the same for all states\n",
    "state=0\n",
    "next_state=1\n",
    "for i in range(0,env.action_space.n):\n",
    "    print(\"probability of reaching \", env.state_to_pos(next_state), \"from \", env.state_to_pos(state), \" executing action \", env.actions[i], \" : \", env.T[state, i, next_state])\n",
    "print(\"Reward for non terminal states: \",env.RS[env.pos_to_state(0,0)]) #assume all states have same reward\n",
    "for state in range(0,env.observation_space.n):\n",
    "    if env.grid[state] == \"P\" or env.grid[state] == \"G\":\n",
    "                    print(\"Reward for state :\", env.state_to_pos(state) ,\" (state type: \", env.grid[state],\") : \",env.RS[state])\n",
    "\n",
    "t = timer()\n",
    "policy = policy_iteration(env)\n",
    "\n",
    "print(\"\\nEXECUTION TIME: \\n{}\".format(round(timer() - t, 4)))\n",
    "policy_render = np.vectorize(env.actions.get)(policy.reshape(env.rows, env.cols))\n",
    "results = CheckResult_L3(env_name, policy_render)\n",
    "results.check_policy_iteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "\n",
    "The following code performs a comparison between Value Iteration and Policy Iteration, by plotting the accumulated rewards of each episode with iterations in range $[1, 50]$ (might take a long time if not optimizied via numpy). You can perform all the test on a different versions of the environment, but with the same structure: *HugeLavaFloor*.\n",
    "\n",
    "The function **run_episode(envirnonment, policy, max_iteration)** run an episode on the given environment using the input policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition model for  LavaFloor-v0  : \n",
      "probability of reaching  (0, 1) from  (0, 0)  executing action  L  :  0.0\n",
      "probability of reaching  (0, 1) from  (0, 0)  executing action  R  :  0.8\n",
      "probability of reaching  (0, 1) from  (0, 0)  executing action  U  :  0.1\n",
      "probability of reaching  (0, 1) from  (0, 0)  executing action  D  :  0.1\n",
      "Reward for non terminal states:  -0.04\n",
      "Reward for state : (1, 3)  (state type:  P ) :  -5.0\n",
      "Reward for state : (2, 3)  (state type:  G ) :  1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Value Iteration: 100%|âââââââââââââââââââââââââââââââââ| 50/50 [00:02<00:00, 17.53it/s]\n",
      "Policy Iteration: 100%|ââââââââââââââââââââââââââââââââ| 50/50 [00:02<00:00, 19.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 5.4134s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env_name = \"LavaFloor-v0\"\n",
    "#env_name = \"HugeLavaFloor-v0\"\n",
    "\n",
    "print(\"Transition model for \", env_name, \" : \") #assume transition functions is the same for all states\n",
    "state=0\n",
    "next_state=1\n",
    "for i in range(0,env.action_space.n):\n",
    "    print(\"probability of reaching \", env.state_to_pos(next_state), \"from \", env.state_to_pos(state), \" executing action \", env.actions[i], \" : \", env.T[state, i, next_state])\n",
    "print(\"Reward for non terminal states: \",env.RS[env.pos_to_state(0,0)]) #assume all states have same reward\n",
    "for state in range(0,env.observation_space.n):\n",
    "    if env.grid[state] == \"P\" or env.grid[state] == \"G\":\n",
    "                    print(\"Reward for state :\", env.state_to_pos(state) ,\" (state type: \", env.grid[state],\") : \",env.RS[state])\n",
    "\n",
    "\n",
    "\n",
    "maxiters = 49\n",
    "\n",
    "env = gym.make(env_name)\n",
    "\n",
    "series = []  # Series of learning rates to plot\n",
    "liters = np.arange(maxiters + 1)  # Learning iteration values\n",
    "liters[0] = 1\n",
    "elimit = 100  # Limit of steps per episode\n",
    "rep = 10  # Number of repetitions per iteration value\n",
    "virewards = np.zeros(len(liters))  # Rewards array\n",
    "c = 0\n",
    "\n",
    "t = timer()\n",
    "\n",
    "# Value iteration\n",
    "for i in tqdm(liters, desc=\"Value Iteration\", leave=True):\n",
    "    reprew = 0\n",
    "    policy = value_iteration(env, maxiters=i)  # Compute policy\n",
    "        \n",
    "    # Repeat multiple times and compute mean reward\n",
    "    for _ in range(rep):\n",
    "        reprew += run_episode(env, policy, elimit)  # Execute policy\n",
    "    virewards[c] = reprew / rep\n",
    "    c += 1\n",
    "series.append({\"x\": liters, \"y\": virewards, \"ls\": \"-\", \"label\": \"Value Iteration\"})\n",
    "\n",
    "\n",
    "vmaxiters = 5  # Max number of iterations to perform while evaluating a policy\n",
    "pirewards = np.zeros(len(liters))  # Rewards array\n",
    "c = 0\n",
    "\n",
    "# Policy iteration\n",
    "for i in tqdm(liters, desc=\"Policy Iteration\", leave=True):\n",
    "    reprew = 0\n",
    "    policy = policy_iteration(env, maxiters=i)  # Compute policy\n",
    "    # Repeat multiple times and compute mean reward\n",
    "    for _ in range(rep):\n",
    "        reprew += run_episode(env, policy, elimit)  # Execute policy\n",
    "    pirewards[c] = reprew / rep\n",
    "    c += 1\n",
    "series.append({\"x\": liters, \"y\": pirewards, \"ls\": \"-\", \"label\": \"Policy Iteration\"})\n",
    "\n",
    "print(\"Execution time: {0}s\".format(round(timer() - t, 4)))\n",
    "np.set_printoptions(linewidth=10000)\n",
    "\n",
    "plot(series, \"Learning Rate\", \"Iterations\", \"Reward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct results for comparison can be found here below. Notice that since the executions are stochastic the charts could differ: the important thing is the global trend and the final convergence to an optimal solution.\n",
    "\n",
    "**Standard Lava floor results comparison**\n",
    "<img src=\"images/results-standard.png\" width=\"600\">\n",
    "\n",
    "**Huge Lava floor results comparison** \n",
    "<img src=\"images/results-huge.png\" width=\"600\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
