{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c707f35e",
   "metadata": {},
   "source": [
    "# AI Exam (Ex 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233976dd",
   "metadata": {},
   "source": [
    "An advanced aquatic drone is deployed to collect critical data on marine biodiversity in a coastal region. The drone starts at point $S$, located near the shore, and must navigate to point $G$, a designated marine research site rich in coral reefs and sea life. Along the way, the drone must carefully maneuver through dynamic underwater environments, avoiding hazards and optimizing energy usage.\n",
    "\n",
    "The environment includes:  \n",
    "1. **(O) Open Water:** Normal movement; no additional challenges.  \n",
    "2. **(C) Currents:** Areas where ocean currents influence the drone's movement, potentially pushing it off course.  \n",
    "3. **(F) Seaweed Forests:** Dense vegetation that slows the drone, incurring extra energy costs per move.    \n",
    "4. **(R) Rocky Areas:** Dangerous zones that the drone must avoid, we do not know what happens to the agent if it enters in the cell.  \n",
    "5. **(E) Energy Stations:** Specific points where the drone can recharge its battery, adding a small reward to encourage efficient navigation.  \n",
    "\n",
    "\n",
    "<img src=\"images/env_ex3.png\" style=\"zoom: 20%;\"/>\n",
    "\n",
    "\n",
    "\n",
    "### Environment Details:\n",
    "The partial information on the enviroment you have are:\n",
    "- **Grid Representation:** The above image represents the environment (a grid 10x10).  \n",
    "- $S$ - Start state: The drone's starting point at (0, 0).  \n",
    "- $G$ - Goal state: The marine research site at (9, 7) providing a large positive reward +20.0 and ending the episode.  \n",
    "- **Movement Costs:** Each move has a default energy cost of -0.01.  \n",
    "- **Hazards:**  \n",
    "  - **Strong Currents:** Entering a current zone imposes a stochastic movement:  \n",
    "    - 80% chance to move as intended.  \n",
    "    - 10% chance to be pushed one cell in the left direction of the current.  \n",
    "    - 10% chance to be pushed one cell in the right direction of the current. \n",
    "  - **Seaweed Forests:** Entering these zones incurs an additional -0.1 reward penalty.\n",
    "  - **Rocky areas:** For these areas the model for the dynamics of the drones and the reward is not available\n",
    "- **Energy Stations:** Provide a 0.1 reward when visited, (however reaching these cells may require the agent to move far from the goal).   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92438f5",
   "metadata": {},
   "source": [
    "Consider the problem of computing the optimal solution for the environment reported above. You can interact with the environment using the code reported here below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be1e677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys \n",
    "import tqdm\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('tools'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import gym, envs\n",
    "from utils.ai_lab_functions import *\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "env_name = 'AquaticEnv-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "env.render()\n",
    "\n",
    "# you can visualize the policy of a solution by calling the plot_policy function\n",
    "policy = np.random.choice(list(env.actions.keys()), env.observation_space.n)\n",
    "visual_policy = np.vectorize(env.actions.get)(policy.reshape(env.rows, env.cols)) \n",
    "plot_policy(visual_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87a4617",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nActions encoding: \", env.actions)\n",
    "\n",
    "# Remember that you can know the type of a cell whenever you need by accessing the grid element of the environment:\n",
    "print(\"Cell type of start state: \",env.grid[env.startstate])\n",
    "print(\"Cell type of goal state: {} Reward: {}\".format(env.grid[env.goalstate],env.RS[env.goalstate]))\n",
    "\n",
    "state = 15 # normal state\n",
    "print(f\"Cell type of cell {env.state_to_pos(state)}: \",env.grid[state])\n",
    "print(f\"Probability of effectivelty performing action {env.actions[0]} from cell {env.state_to_pos(state)} to cell {env.state_to_pos(state+1)}: {env.T[state, list(env.actions.keys())[0], state+1]}\")\n",
    "print(f\"Probability of effectivelty performing action {env.actions[1]} from cell {env.state_to_pos(state)} to cell {env.state_to_pos(state+1)}: {env.T[state, list(env.actions.keys())[1], state+1]}\")\n",
    "print(f\"Probability of effectivelty performing action {env.actions[2]} from cell {env.state_to_pos(state)} to cell {env.state_to_pos(state+1)}: {env.T[state, list(env.actions.keys())[2], state+1]}\")\n",
    "print(f\"Probability of effectivelty performing action {env.actions[3]} from cell {env.state_to_pos(state)} to cell {env.state_to_pos(state+1)}: {env.T[state, list(env.actions.keys())[3], state+1]}\")\n",
    "\n",
    "state = 10 # state with stochastic transitions\n",
    "print(f\"\\nCell type of cell {env.state_to_pos(state)}: \",env.grid[state])\n",
    "print(f\"Probability of effectivelty performing action {env.actions[0]} from cell {env.state_to_pos(state)} to cell {env.state_to_pos(state+1)}: {env.T[state, list(env.actions.keys())[0], state+1]}\")\n",
    "print(f\"Probability of effectivelty performing action {env.actions[1]} from cell {env.state_to_pos(state)} to cell {env.state_to_pos(state+1)}: {env.T[state, list(env.actions.keys())[1], state+1]}\")\n",
    "print(f\"Probability of effectivelty performing action {env.actions[2]} from cell {env.state_to_pos(state)} to cell {env.state_to_pos(state+1)}: {env.T[state, list(env.actions.keys())[2], state+1]}\")\n",
    "print(f\"Probability of effectivelty performing action {env.actions[3]} from cell {env.state_to_pos(state)} to cell {env.state_to_pos(state+1)}: {env.T[state, list(env.actions.keys())[3], state+1]}\")\n",
    "\n",
    "state = 10 # state with stochastic transitions\n",
    "print(f\"\\nCell type of cell {env.state_to_pos(state)}: \",env.grid[state])\n",
    "print(f\"Probability of effectivelty performing action {env.actions[0]} from cell {env.state_to_pos(state)} to cell {env.state_to_pos(state+1)}: {env.T[state, list(env.actions.keys())[0], state+1]}\")\n",
    "print(f\"Probability of effectivelty performing action {env.actions[1]} from cell {env.state_to_pos(state)} to cell {env.state_to_pos(state+1)}: {env.T[state, list(env.actions.keys())[1], state+1]}\")\n",
    "print(f\"Probability of effectivelty performing action {env.actions[2]} from cell {env.state_to_pos(state)} to cell {env.state_to_pos(state+1)}: {env.T[state, list(env.actions.keys())[2], state+1]}\")\n",
    "print(f\"Probability of effectivelty performing action {env.actions[3]} from cell {env.state_to_pos(state)} to cell {env.state_to_pos(state+1)}: {env.T[state, list(env.actions.keys())[3], state+1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f146690",
   "metadata": {},
   "source": [
    "#### Q1: Provide an algorithm that finds a solution that reaches the goal without touching any 'R' state and passing through at least one charging station in this environment. Use the approach that you think is most appropriate motivating your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d99a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'AquaticEnv-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "# for reproducibility purposes you can set a seed to your algorithm. For instance:\n",
    "seed = 0\n",
    "#sol, _, _ = your_method(env, your_hyperparams, seed=0)\n",
    "# inside the function then\n",
    "def your_method(env, hyperparams, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    # your code here\n",
    "    return sol, _, _\n",
    "\n",
    "# you can visualize the policy of a solution by calling the plot_policy function\n",
    "#using a random policy for now, you should replace this with your solution \n",
    "sol = np.random.choice(list(env.actions.keys()), env.observation_space.n)\n",
    "visual_policy = np.vectorize(env.actions.get)(sol.reshape(env.rows, env.cols)) \n",
    "plot_policy(visual_policy)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
