\documentclass[a4paper]{article}
\usepackage{import}
\input{../../preamble.sty}

\begin{document}

\input{title.tex}

\tableofcontents
\pagebreak

\section{Introduzione}
\subsection{Cos'è l'informatica?}
È una scienza che studia la calcolabilità, cioè cerca di capire che problemi si possono
risolvere con un programma. Nasce dall'unione di matematica, ingegneria e logica. Il
computer è solo uno strumento, mentre la matematica è il linguaggio con cui si
creano algoritmi che permettono di risolvere i problemi.

\subsection{Origini dell'informatica}
Hilbert, nel 1900, si pose l'obiettivo di formalizzare tutta la matematica con un insieme
finito e non contraddittorio di assiomi. Nel 1931, invece, Gödel dimostrò che l'informatica
non potrà mai rappresentare tutta la matematica, perché ci saranno sempre proposizioni
vere ma non dimostrabili tramite il calcolo. Ci si iniziò a chiedere se esistessero
modelli di calcolo meccanici in grado di risolvere tutti i problemi. Nel 1936, Turing
propose la macchina di Turing, una \textbf{sola} macchina programmabile in grado di
risolvere tutti i problemi risolvibili:
\[
  Int(P,x) = \begin{cases}
    P(x) & \text{se } P(x) \text{ termina}\\
    \uparrow & \text{se } P(x) \text{ non termina}
  \end{cases}
\] 
dove $P$ è un programma e $x$ è un input. La macchina di Turing è un modello teorico
di calcolatore, che non esiste fisicamente, ma è in grado di simulare qualsiasi altro
calcolatore. Da questo modello deriva la concezione di calcolabilità, cioè se un problema
è intuitivamente calcolabile, allora esiste un programma in grado di risolverlo.

Altri modelli di calcolo che sono stati proposti sono:
\begin{itemize}
  \item Lambda-calcolo
  \item Funzioni ricorsive
  \item Linguaggi di programmazione (Turing-completi)
\end{itemize}

\begin{define}
  La Turing-completezza è la proprietà di un linguaggio di programmazione di essere
  in grado di simulare una macchina di Turing, cioè di poter risolvere qualsiasi problema
  risolvibile.
\end{define}

\subsubsection{Calcolabilità}
Un programma è calcolabile se termina, ma non è detto che termini in un tempo ragionevole.
Non esistono algoritmi che possono dire se un programma termina o meno. Questo è un esempio
di problema non calcolabile.

I problemi non calcolabili sono infinitamente più numerosi di quelli calcolabili

\subsection{Nozioni di base}
\subsubsection{Basi di logica}

Alcune nozioni di logica che ci serviranno in seguito:

\begin{itemize}
    \item \textbf{Linguaggio del primo ordine:} 
    \begin{itemize}
        \item Simboli relazionali $(p,q, ...)$
        \item Simboli di funzione $(f,g, ...)$
        \item Simboli di costante $(c,d, ...)$
    \end{itemize}
    \item \textbf{Simboli logici:}
    \begin{itemize}
        \item Parentesi (,) e virgola
        \item Insieme numerabile di variabili $(v,x,...)$
        \item Connettivi logici ($\neg, \land, \lor, \rightarrow, \leftrightarrow$)
        \item Quantificatori ($\forall, \exists$)
    \end{itemize}
    \item \textbf{Termini:}
        \begin{itemize}
            \item Variabili
            \item Costanti
            \item $f$ simbolo di funzione m-ario $t_1, t_2, \dots, t_m$ termini, allora

              $f(t_1, t_2, \dots, t_m)$ è un termine.
        \end{itemize}
    \item \textbf{Formula atomica:} $p$ simbolo di relazione n-ario, 
    $t_1,t_2,\dots,t_n$ 
    termini, allora $p(t_1,t_2,\dots,t_n)$ è una formula atomica.
    \item \textbf{Formula:}
    \begin{itemize}
        \item Formula atomica
        \item $\phi$ formula, allora $\neg \phi$ è una formula
        \item $\phi$ e $\psi$ formule, allora $(\phi \land \psi)$, $(\phi \lor \psi)$, $(\phi \rightarrow \psi)$, $(\phi \leftrightarrow \psi)$ sono formule.
        \item $\phi$ formula e $v$ variabile, alloar e $\forall v . \phi$ e $\exists v . \phi$ sono formule.
    \end{itemize}
\end{itemize}

\subsubsection{Nozioni sugli insiemi}

\begin{itemize}
    \item $x \in A$ signfiica che $x$ è un elemento dell'insieme $A$
    \item $\{x | P(x)\}$ si identifica insieme costuito dagli $x$ che soddisfano la proprietà (o predicato) $P(x)$
    \item $A \subseteq B$ significa che $A$ è un sottoinsieme di $B$ se ogni elemento di $A$ è anche in $B$
    \item $\mathcal{P}(S)$ denota l'insieme delle parti di $S$, ovvero l'insieme di tutti i sottoinsiemi di $S$ ($\mathcal{P}(S) = \{X | X \subseteq S\}$)
    \item $A \backslash B = \{x | x \in A \land x \notin B\}, A \cup B = \{x | x \in A \lor x \in B\}, A \cap B = \{x | x \in A \land x \in B\}$   
    \item $|A|$ denota la cardinalità di $A$, ovvero il numero di elementi in $A$.
    \item $\bar{A}$ denota il complemento di $A$, ovvero $x \bar{\in} A \leftrightarrow x \notin A$
\end{itemize}

\subsubsection{Nozioni sulle relazioni}

\begin{itemize}
    \item Prodotto cartesiano:
      \[
        A_1 \times A_2 \times \dots \times A_n = \{\langle a_1,a_2,\dots,a_n \rangle | a_1 \in A_1,\dots, a_n \in A_n\}
      \]
    \item Una \textbf{relazione} (binaria) è un sottoinsieme del prodotto cartesiano di (due) insiemi; dati $A$ e $B$, $R \subseteq A \times B$ 
    è una relazione su $A$ e $B$ 
    \begin{itemize}
        \item \textbf{Riflessiva:} $\forall a \in S$ si ha che $aRa$
        \item \textbf{Simmetrica:} $\forall a,b \in S$ se $aRb$ allora $bRa$
        \item \textbf{Antisimmetrica:} $\forall a,b \in S$ se $aRb$ e $bRa$ allora $a=b$
        \item \textbf{Transitiva:} $\forall a,b,c \in S$ se $aRb$ e $bRc$ allora $aRc$
    \end{itemize}  
    \item Per ogni relazione $R \subseteq S \times S$ la chiusura transitiva di $R$ è il più piccolo 
    insieme $R^*$ tale che $\langle a,b \rangle \in R \land \langle b,c \rangle \in R \rightarrow \langle a,c \rangle \in R^*$  
    \item Una relazione è detta \textbf{totale} su $S$ se $\forall a,b \in S$ si ha che $aRb \lor bRa$
    \item Una relazione $R$ di \textit{di equivalenza} è una relazione binaria riflessiva, simmetrica e transitiva.
    \item Una relazione binaria $R \subseteq S \times S$ è un \textbf{pre-ordine} se è riflessiva e transitiva.
    \item $R$ è un ordine parziale se è un pre-ordine antisimmetrico.
    \item $x \in S$ è \textbf{minimale} rispetto a $R$ se $\forall y \in S . y \not{R} x$ (ovvero $\neg(yRx))$
    \item $x \in S$ è \textbf{minimo} rispetto a $R$ se $\forall y \in S . xRy$
    \item $x \in S$ è \textbf{massimale} rispetto a $R$ se $\forall y \in S . x \not{R} y$ (ovvero $\neg(xRy)$)
    \item $x \in S$ è \textbf{massimo} rispetto a $R$ se $\forall y \in S . yRx$
\end{itemize}

\subsubsection{Nozioni sulle funzioni}

\begin{itemize}
    \item Una relazione $f$ è una \textbf{funzione} se $\forall a \in A$ esiste uno ed un solo $b \in B$ tale che $(a,b) \in f$
    \item $A$ dominio e $B$ codominio di $f$. Il range di $f$ è l'insieme di tutti i valori che $f$ può assumere.
    \item f è \textbf{iniettiva} se $\forall a_1,a_2 \in A$ se $a_1 \neq a_2$ allora $f(a_1) \neq f(a_2)$
    \item Se $f : A \mapsto B$ è sia iniettiva che suriettiva allora è \textbf{biiettiva} e quindi esiste $f^{-1} : B \mapsto A$ 
\end{itemize}

\section{Funzioni calcolabili}
Un insieme è una proprietà ed è rappresentato da una funzione che indica se un elemento
appartiene o meno all'insieme. I problemi da risolvere (in questo corso) hanno come
soluzione una funzione sui naturali:
\[
  f : \mathbb{N} \to \mathbb{N}
\] 
Questo tupo di funzione è un \textbf{insieme} di associazioni input-output. Un esempio
è la funzione quadrato:
\[
  f = \text{quadrato} = \{(0,0), (1,1), (2,4), (3,9), \ldots\} = \{(n,n^2) | n \in \mathbb{N}\}
\] 
Quindi \( f \) è un insieme di coppie in \( \mathbb{N} \subseteq \mathbb{N} \times \mathbb{N} \) la
cui cardinalità è: \( \left| \mathbb{N} \times \mathbb{N} \right| =
\left| \mathbb{N} \right| \). Di conseguenza la funzione è un sottoinsieme di \( \mathbb{N}
\times \mathbb{N}\):
\[
  f \subseteq \mathbb{N} \times \mathbb{N} \quad f \in \mathcal{P}(\mathbb{N} \times \mathbb{N})
\] 
Dove \( \mathcal{P}(\mathbb{N} \times \mathbb{N}) \) è l'insieme delle parti di \( \mathbb{N}
\times \mathbb{N} \), cioè l'insieme di tutti i sottoinsiemi di \( \mathbb{N} \times
\mathbb{N} \).

Il numero di funzioni è:
\[
  f: \mathbb{N} \to \mathbb{N} = \left| \mathcal{P}(\mathbb{N} \times \mathbb{N}) \right| 
  = \left| \mathcal{P}(\mathbb{N}) \right| 
\] 
\begin{example}
  Un esempio di insieme delle parti per l'insieme \( A = \{1,2,3\} \) è:
  \[
    \mathcal{P}(A) = \{\varnothing, \{1\}, \{2\}, \{3\}, \{1,2\}, \{1,3\}, \{2,3\}, \{1,2,3\}\}
  \] 
  E le cardinalità sono:
  \[
    \left| A \right| = 3 \quad \left| \mathcal{P}(A) \right| = 8 = 2^3
  \] 
  \[
    \downarrow
  \] 
  \[
    \left| \mathbb{N} \right| = \omega < 
    \underbrace{\left| \mathcal{P}(\mathbb{N}) \right| = 2^{\omega}}_{%
      \substack{\text{Insieme delle funzioni,}\\ 
      \text{non numerabile}}
    }
    = \left| \mathbb{R} \right| 
  \] 
\end{example}
Si ha quindi che \textbf{l'insieme delle funzioni non è numerabile}

\vspace{1em}
\noindent
Ci si chiede se queste funzioni sono tutte calcolabili:
\begin{definition}
  Una funzione \textbf{intuitivamente calcolabile} è una funzione descrivibile attraverso
  un algoritmo, cioè una sequenza finita di passi discreti elementari.
\end{definition}

\subsection{Quante funzioni numerabili ci sono?}
Consideriamo \( \Sigma \) come un alfabeto finito, cioè una seguenza di simboli
utilizzabili per scrivere un programma o algoritmo.
\[
  \Sigma = \{s_1,s_2,s_3, \ldots, s_n\}
\] 
\[
  \downarrow
\] 
\[
  \text{Programma} \subseteq \text{sqeuenze di simboli in } \Sigma
\] 
Con \( \Sigma^* \) si descrive l'insieme di tutte le sequenze finite di simboli in \( \Sigma \),
quindi l'insieme di tutti i possibili programmi è:
\[
  \text{Programmi} \subseteq \Sigma^*
\]

\begin{example}
  Se \( \Sigma = \left\{ a, b, c \right\} \) allora la sequenza di tutti i possibili
  simboli è:
  \[
    \Sigma^* = \{\epsilon, a, b, c, aa, ab, ac, ba, bb, bc, ca, cb, cc, aaa, aab, \ldots\}
  \]
  dove \( \epsilon \) è la stringa vuota.

  La cardinalità di \( \Sigma^* \) è infinita numerabile (anche se \( \Sigma  \) è finito).
  \[
    \left| \Sigma^* \right| = \left| \mathbb{N} \right| 
  \] 
\end{example}

Si ha quindi che l'insieme dei programmi è numerabile:
\[
  \left| \text{Programmi in } \Sigma \right| \leq \left| \Sigma^* \right| = \left| \mathbb{N} \right|
\] 
e questo implica che l'insieme delle \textbf{funzioni calcolabili è numerabile}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{cardinalita_funzioni_calcolabili}
  \caption{Cardinalità delle funzioni}
\end{figure}

\begin{example}
  Prendiamo ad esempio la seguente funzione (serie di fibonacci):
  \[
    f(n) \subseteq \mathbb{N} \times \mathbb{N}
  \] 

  \vspace{1em}
  \noindent
  Dove:
  \[
    f(0) = 1, f(1) = 1, f(2) = 2
  \] 
  \[
    f(3) = 3, f(4) = 5, f(5) = 8
  \] 
  \[
    f(6) = 13, f(7) = 21, \ldots
  \] 
  Definiamo un algoritmo ricorsivo:
  \[
    \begin{cases}
      f(0) = 1 = f(1)\\
      f(x+2) = f(x+1) + f(x)
    \end{cases}
  \] 

  \vspace{1em}
  \noindent
  Trovare un algoritmo non è possibile per tutte le funzioni, ma solo per quelle calcolabili.
\end{example}
Nell'insieme delle funzioni calcolabili ci sono:
\begin{itemize}
  \item Funzioni totali, cioè definite per ogni input \( n \in \mathbb{N} \) e terminano
    sempre
  \item Funzioni parziali, cioè non definite per ogni \( n \in \mathbb{N} \)
\end{itemize}

\subsection{Funzioni vs Insiemi}
Una funzione può essere vista come un linguaggio \( \mathcal{L}_f \) tale che:
\[
  f: \mathbb{N} \to \mathbb{N} \quad \leftrightarrow \quad
  \mathcal{L}_f = \left\{ 1^{f(x)} \;\left|\; x \in \mathbb{N} \right.\right\} \quad
  \Sigma = \{1\}
\] 
Questo linguaggio permette di dire se un input appartiene o meno al linguaggio:
\[
  \sigma  \in \Sigma^*
\] 
\[
  \downarrow
\] 
\[
  \begin{cases}
    \sigma  \in \mathcal{L}_f \quad \text{se appartiene al linguaggio}\\
    \sigma  \notin \mathcal{L}_f \quad \text{se non appartiene al linguaggio}
  \end{cases}
\] 
Parliamo di insiemi invece che di funzioni dove gli elementi dell'insieme dipendono dal
calcolo della funzione.
\begin{example}
  Prendiamo ad esempio le seguenti funzioni:
  \begin{itemize}
    \item Funzione costante (Finite)
      \[
        f(x) = 2 \to \mathcal{L}_f \text{ è finito}
      \] 
    \item Funzione lineare (Regolari)
      \[
        f(x) = 2x \to \mathcal{L}_f \text{ è infinito numerabile}
      \]
      C'è bisogno di una memoria finita per determinare se la stringa appartiene al linguaggio
    \item (Context free)
      \[
        f(\sigma) = \sigma \sigma^{\text{reverse}}
      \] 
      \[
        \sigma = abc \quad \sigma^\text{reverse} = cba
      \] 
      Per calcolare questa funzione c'è bisogno di una memoria illimitata, cioè non
      si può sapere a priori quanta ce n'è bisogno, è sufficiente uno stack.

    \item Decidibile
      \[
        f(x) = x^2
      \] 
      Per calcolare questa funzione c'è bisogno di una memoria illimitata
  \end{itemize}
\end{example}

\vspace{1em}
\noindent
Le funzioni calcolabili sono divise in classi secondo la gerarchia di Chomsky:
\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{gerarchia_chomsky}
  \caption{Gerarchia di Chomsky}
\end{figure}

\section{Linguaggi regolari}
Il principio di induzione è un meccanismo di definizione e dimostrazione che funziona
\textbf{solo su insiemi infiniti}. Esistono due metodi di induzione:
\begin{itemize}
  \item Induzione matematica
  \item Induzione strutturale
\end{itemize}
In questo corso tratteremo solo l'induzione matematica.

\vspace{1em}
\noindent
Un insieme \( A \) infinito con una relazione di ordine non riflessiva (senza l'uguale
perchè l'elemento non è in relazione con sè stesso): \( < \;: (A, <) \).
\( A = \mathbb{N} \) e \( < \) è l'ordinamento
stretto tra numeri naturali. La relazione di ordine deve essere \textbf{ben fondata},
quindi non devono esserci catene discendenti infinite, cioè una sequenza di elementi in
ordine decrescente infinita:
\[
  a_0 > a_1 > a_2 > a_3 > \ldots \to \text{non ben fondata}
\] 
Una relazione di ordine riflessiva non è ben fondata, perchè esistono catene
infinite:
\[
  a_0 \geq a_1 \geq a_2 \geq a_3 \geq a_3 \geq a_3 \geq \ldots \to \text{non ben fondata}
\]

\( b \) minimale in \( A : b \in A \) \( b \) è minimale se \( \forall b' < b \;.\; b' \notin A \) 
Ad esempio: \( \left\{ 1, 2, 3 \right\} \) ha come minimali (di contenimento) \( \left\{ 1, 2 \right\} \) 
e \( \left\{ 2,3 \right\} \).

\begin{definition}[Principio di induzione]
  Se \( A \) è un insisme ben fondato (con ordinamento \( < \)), e \( \Pi  \) è una
  proprietà definita sugli elementi di \( A: \Pi \subseteq A \), allora:
  \[
    \forall  a \in A \;.\; \underbrace{\Pi(a) }_{a\text{ soddisfa } \Pi} \iff
    \underbrace{
      \forall a \in A \;.\; \left[ \left[ \forall b < a \;.\; \Pi(b) \right] \Rightarrow
      \Pi(a)\right]
    }_{
      \substack{
        \text{Se dimostriamo } \Pi \text{ per ogni }\\
        \text{elemento più piccolo di } a, \\
        \text{allora } \Pi \text{ vale anche per } a
      }
    }
  \] 
  Consideriamo come caso base gli elementi minimali di \( A \):
  \[
    \text{Base}_A = \left\{ a \in A \;\left|\; a \text{ minimale}  \right.\right\}
  \] 
  Se si dimostra che \( \Pi \) vale per tutti gli elementi minimali di \( A \) (la base):
  \[
    \overbrace{
      \underbrace{
        \forall a \in \text{Base}_A \;.\; \Pi(a)
      }_{
        \substack{
          \text{Dimostriamo } \Pi \text{ per ogni }\\
          \text{elemento minimale di } A
        }
      }
    }^{\text{Base}}
    \wedge 
    \underbrace{
      \forall a \in A \setminus \text{Base}_A
    }_{
      \text{Passo induttivo}
    }
    \;.\;
    \underbrace{
      \forall b < a \;.\; \Pi(b)
    }_{
      \text{Ipotesi induttiva}
    }
    \underbrace{
      \Rightarrow \Pi(a)
    }_{
      \text{Tesi da dimostrare}
    }
  \] 
\end{definition}
\begin{example}
  Dimostriamo che:
  \[
    \forall n \in \mathbb{N} \sum_{i=1}^{n} i = \frac{n(n+1)}{2}
  \] 
  L'insieme è:
  \[
    A = \mathbb{N} \setminus \{0\} = \{1,2,3, \ldots\}
  \] 
  \begin{itemize}
    \item \textbf{Base}
      \[
        \text{Base}_A = \{1\}
      \] 
      \begin{itemize}
        \item dimostriamo la base
          \[
            \sum_{i=1}^{1} i = n(n+1)/2 = 1(1+1)/2 = 1
          \] 
      \end{itemize}

    \item \textbf{Passo induttivo}:
      \vspace{1em}
      \noindent
      Prendo \( n \in \mathbb{N} \) 
      \begin{itemize}
        \item Ipotesi induttiva, cioè per ogni \( m < n \) vale la proprietà:
          \[
            \forall \underset{\in \mathbb{N}}{m} < n \;.\; \sum_{i=1}^{m} i =
            \frac{m(m+1)}{2}
          \] 
          Dobbiamo dimostrare la proprietà per \( n \):
          \[
            \sum_{i=1}^{n} i = \sum_{i=1}^{n-1} i + n
          \] 
          \[
            n - 1 < n \quad \text{quindi vale l'ipotesi induttiva}
          \] 
          \[
            \sum_{i=1}^{n-1} i = \frac{(n-1)(n-1+1)}{2} = \frac{(n-1)n}{2}
          \] 
          \[
            \downarrow
          \] 
          \[
            \begin{aligned}
              \sum_{i=1}^{n} i &= \sum_{i=1}^{n-1} i + n\\
                               &= \frac{(n-1)n}{2} + n\\
                               &= \frac{(n-1)n + 2n}{2}\\
                               &= \frac{n^2 - n + 2n}{2}\\
                               &= \frac{n(n+1)}{2} \quad \square
            \end{aligned}
          \] 
      \end{itemize}
  \end{itemize}
  È quindi dimostrato che:
  \[
    \forall n \in \mathbb{N} \sum_{i=1}^{n} i = \frac{n(n+1)}{2}
  \]
\end{example}

\subsection{Linguaggi formali}
\begin{definition}
  Un linguaggio formale è un insieme di stringhe costruite su un alfabeto finito \( \Sigma  \) .
\end{definition}
Solitamente un linguaggio formale \( \mathcal{L} \) è un sottoinsieme di \( \Sigma^* \),
tipicamente infiniti, ma non necessariamente:
\[
  \mathcal{L} \subseteq \Sigma^*
\] 
I linguaggi sono divisi in:
\begin{itemize}
  \item Linguaggi finiti
  \item Linguaggi regolari, il modello utilizzato è l'automa a stati finiti.
\end{itemize}
\begin{figure}[H]
  \centering
  \begin{tikzpicture}
    \draw[thick] (0,0) circle(3);
    \draw[thick] (0,1.9) circle(1);
    \node at (4,1.9) {Linguaggi formali};
    \draw[<-, thick] (0,1.9) -- (2.6,1.9);

    \node at (5,0) {Linguaggi regolari};
    \draw[<-, thick] (0,0) -- (3.6,0);
  \end{tikzpicture}
  \caption{Linguaggi formali e linguaggi regolari}
\end{figure}

\subsection{Linguaggi regolari (automi a stati finiti, DFA)}
Il meccanismo più semplice per una memoria finita è l'automa a stati finiti

\vspace{1em}
\noindent
Consideriamo il linguaggio:
\[
\mathcal{L}_f = \left\{ 1^{2n} \;\left|\; n \in \mathbb{N} \right. \right\}
\] 
ha bisogno di due stati \( q_0 \) e \( q_1 \). Lo stato \( q_0 \) rappresenta
l'informazione di essere di lunghezza pari, mentre lo stato \( q_1 \)
rappresenta l'informazione di essere di lunghezza dispari.
\begin{figure}[H]
  \centering
  \begin{tikzpicture}[node distance=3cm, on grid, auto]
    \node[state, initial, accepting] (q0) {$q_0$};
    \node[state, right of=q0] (q1) {$q_1$};

    \draw (q0) edge[bend left, above] node{1} (q1);
    \draw (q1) edge[bend left, below] node{1} (q0);
  \end{tikzpicture}
  \caption{Automa a stati finiti per il linguaggio \( \mathcal{L}_f = \left\{ 1^{2n} \;\left|\; n
            \in \mathbb{N} \right. \right\} \)}
\end{figure}
L'automa a stati finiti \textbf{deterministico} è definito come una quintupla: 
\[
  M = \left( Q, \Sigma , \delta, q_0, F \right) 
\] 
dove:
\begin{itemize}
  \item \( Q \) è un insieme \textbf{finito} di stati. Ogni stato rappresenta
    un'informazione

  \item \( \Sigma  \) è un insieme \textbf{finito} di simboli (alfabeto).
    Ogni simbolo è un elemento atomico che posso leggere e che compone le
    stringhe da riconoscere
    
  \item \( q_0 \in Q \) è uno stato e identifica lo stato iniziale. Lo stato finale
    viene indicato con un doppio cerchio

  \item \( F \subseteq Q \) è l'insieme degli stati finali (di accettazione)

  \item \( \delta: Q \times \Sigma \to Q \) È una \textbf{funzione di transizione}
    che dato uno stato e un simbolo, restituisce lo stato successivo ed è come se
    fosse una tabella che associa ad ogni coppia (stato, simbolo) uno stato:
    \begin{table}[H]
      \centering
      \begin{tabular}{c|c|c}
        \( \Sigma \setminus Q\) & \( q_0 \) & \( q_1 \)\\
        \hline
        1 & \( q_1 \) & \( q_0 \)\\
      \end{tabular}
    \end{table}
    La funzione deve essere \textbf{totale}, cioè deve essere definita per ogni
    coppia \( (q,a) \in Q \times \Sigma  \), quindi la tabella deve essere completa.

  \item \( \hat{\delta}: Q \times \Sigma^* \to Q \) Descrive lo stato che raggiungono
    leggendo una sequenza di simboli
    \[
      \begin{cases}
        \hat{\delta}(q, \epsilon) = q\\
        \hat{\delta}(q, wa) = \delta(\hat{\delta}(q,w), a)
      \end{cases}
      \quad
      w \in \Sigma^*, \quad a \in \Sigma 
    \] 
    È quindi la \textbf{chiusura transitiva} di \( \delta \)
\end{itemize}

\subsubsection{Come si dimostra che un linguaggio è regolare?}
\begin{example}
  Prendiamo in considerazione il seguente linguaggio:
  \[
    L = \left\{ \sigma  \;\left|\; \sigma \text{ contiene almeno due } 1 \right. \right\}
  \] 
  \[
    \Sigma = \{0,1\}
  \] 
  Con le seguenti stringhe si ha:
  \begin{itemize}
    \item \( \sigma = 011 \in \mathcal{L} \) 
    \item \( \sigma = 1000100 \in \mathcal{L} \) 
    \item \( \sigma = 00010 \notin \mathcal{L} \) 
  \end{itemize}
  L'informazione che codifica lo stato iniziale deve essere coerente con \( \varepsilon  \) 
  (stringa vuota)
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}[->, node distance=2cm, on grid, auto]
      \node[state, initial] (q0) {$q_0$};
      \node[state, right of=q0] (q1) {$q_1$};
      \node[state, right of=q1, accepting] (q2) {$q_2$};

      \draw (q0) edge[above] node{1} (q1);
      \draw (q0) edge[loop above] node{0} (q0);
      \draw (q1) edge[loop above] node{0} (q2);
      \draw (q1) edge[above] node{1} (q2);
      \draw (q2) edge[loop above] node{0,1} (q2);
    \end{tikzpicture}
    \caption{Automa a stati finiti per il linguaggio \( \mathcal{L} \)}
  \end{figure}
\end{example}
Un lingauggio \( L \) è riconosciuto da \( M = \left( Q, \Sigma, \delta, q_0, f \right)  \) (DFA, Deterministic Finite Automaton) se:
\( L = L(M) \) dove \( L(M) \) è il linguaggio di \( M \) definito come:
\[
  L(M) = \left\{ \sigma  \in \Sigma^* \;\left|\; \hat{\delta}(q_0, \sigma) \in F \right. \right\}
\] 
Cioè sono tutte le stringhe che partendo da \( q_0 \) fanno raggiungere uno stato finale.

\vspace{1em}
\noindent
\begin{definition}
  Per dimostrare che \( L \) è regolare dobbiamo costruire \( M \) (almeno un \( M \)) e
  \textbf{dimostrare che} \( L = L(M) \) 
\end{definition}
\( L = L(M) \) è un uguaglianza insiemistica e si dimostra con due contenimenti:
\[
  L = L(M) \equiv L \subset L(M) \wedge L(M) \subset L
\]  
\begin{itemize}
  \item Se un elemento si trova nel primo insieme, allora si trova anche nel secondo
    \[
      L \subseteq L(M) \equiv \sigma \in L \Rightarrow \sigma \in L(M) \equiv
      \sigma \in L \Rightarrow \hat{\delta}(q_0, \sigma) \in F
    \] 
  \item Se un elemento si trova nel secondo insieme, allora si trova anche nel primo
    \[
      L(M) \subseteq L \equiv \sigma \in L(M) \Rightarrow \sigma \in L \equiv
      \hat{\delta}(q_0, \sigma) \in F \Rightarrow \sigma \in L
    \]
    o per contrapposizione:
    \[
      \sigma \notin L \Rightarrow \hat{\delta}(q_0, \sigma) \notin F
    \] 
\end{itemize}
Questo dimostra che il linguaggio è regolare perchè è riconosciuto da un automa.

\begin{example}
  Riprendendo l'esempio precedente:
  \[
    L = \left\{ \sigma \in \Sigma^* \;\left|\; \sigma \text{ contiene almeno due } 1 \right. \right\}
  \] 
  \[
    \Sigma = \{0,1\}
  \]
  \( M = \) 
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}[->, node distance=2cm, on grid, auto]
      \node[state, initial] (q0) {$q_0$};
      \node[state, right of=q0] (q1) {$q_1$};
      \node[state, right of=q1, accepting] (q2) {$q_2$};

      \draw (q0) edge[above] node{1} (q1);
      \draw (q0) edge[loop above] node{0} (q0);
      \draw (q1) edge[loop above] node{0} (q2);
      \draw (q1) edge[above] node{1} (q2);
      \draw (q2) edge[loop above] node{0,1} (q2);
    \end{tikzpicture}
    \caption{Automa a stati finiti per il linguaggio \( \mathcal{L} \)}
  \end{figure}
  \noindent
  Dimostriamo per induzione sulla lunghezza delle stringhe \( \sigma \in \Sigma^* \) 
  che se \( x \in L \) allora \( \hat{\delta}(q_0, x) \in F \) e se \( x \notin L \) allora
  \( \hat{\delta}(q_0, x) \notin F \).

  \vspace{1em}
  \noindent
  \( \left| \sigma  \right| = 0 \) non è \textbf{mai} sufficiente come base, ma
  è eventualmente la base \textbf{solo} per una delle due dimostrazioni. Bisogna
  quindi prendere la lunghezza più piccola che permette di avere sia \( \sigma \in L \) 
  che  \( \sigma \notin L \), in questo caso è \( \left| \sigma  \right| = 2 \).
  Per ogni \( \sigma  \) tale che \( \left| \sigma  \right| < 2 \quad \sigma \notin L \)
  perchè non può contenere due 1 e non è riconosciuta da \( M \) dove il primo stato finale
  è raggiunto leggendo almeno due simboli.
  \[
    \varepsilon \in L \quad \varepsilon \notin L
  \] 
  \begin{itemize}
    \item \textbf{Base}: Controlliamo ogni stringa di lungheezza minima nel linguaggio per
      provare il caso base. In questo caso la lunghezza minima è
      \( \left| \sigma  \right| = 2 \) 
      \[
        \begin{cases}
          \sigma &= 11 \in L \; \text{ e } \; \hat{\delta}(q_0, 11) = q_2 \in F \\
          \sigma &= 10 \notin L \; \text{ e } \; \hat{\delta}(q_0, 10) = q_1 \notin F \\
          \sigma &= 01 \notin L \; \text{ e } \; \hat{\delta}(q_0, 01) = q_1 \notin F \\
          \sigma &= 00 \notin L \; \text{ e } \; \hat{\delta}(q_0, 00) = q_0 \notin F
        \end{cases}
      \] 

    \item \textbf{Passo induttivo}: Assumiamo che valga l'\textbf{ipotesi induttiva}, cioè
      la tesi con un limite fissato:
      \[
        \forall \sigma \in \Sigma^* \;.\; \left| \sigma  \right| \le n \;.\;
        \begin{cases}
          \sigma \in L &\Rightarrow \hat{\delta}(q_0, \sigma) \in F\\
          \sigma \notin L &\Rightarrow \hat{\delta}(q_0, \sigma) \notin F
        \end{cases}
      \] 
      Vogliamo dimostrare che la tesi vale per \( \left| \sigma  \right| = n + 1 \)
      (la successiva stringa che posso considerare).

      \noindent
      \textbf{Tesi}:
      \( 
        \begin{cases}
          \sigma \in L \Rightarrow \hat{\delta}(q_0,\sigma) = q_2
          \; \sigma \text{ contiene almeno due 1}\\
          \sigma \notin L \Rightarrow \hat{\delta}(q_0,\sigma) = q_0
          \; \sigma \text{ non contiene 1}\\
          \sigma \notin L \Rightarrow \hat{\delta}(q_0,\sigma) = q_1
          \; \sigma \text{ contiene esattamente un 1}
        \end{cases}
      \) 

      \vspace{1em}
      \noindent
      \textbf{Ipotesi induttiva}:
      \[
        \forall \sigma \in \Sigma^* \;.\; \left| \sigma  \right| \le n \;.\;
        \text{ allora la tesi vale su } \sigma
      \] 

      \vspace{1em}
      \noindent
      Dimostrazione della tesi per \( \sigma  \) tale che \( \left| \sigma  \right| = n + 1 \).
      (\( \left| \sigma' \right| = n \) quindi su \( \sigma' \) possiamo applicare l'ipotesi
      induttiva):
      \[
        \left| \sigma  \right| = n + 1 \to 
          \sigma = \sigma'1 \; \vee \; \sigma = \sigma'0
      \] 
      \begin{itemize}
        \item Supponiamo che \( \sigma  \) appartenga al linguaggio e termini con 1:
          \[
            \sigma \in L \wedge \sigma  = \sigma'1
          \] 
          \[
            \downarrow
          \] 
          \begin{itemize}
            \item Se \( \sigma' \in L \) applico l'ipotesi induttiva:
              \[
                  \hat{\delta}(q_0, \sigma') = q_2\\
              \] 
              \[
                \begin{aligned}
                  \hat{\delta}(q_0, \sigma) &\stackrel{\sigma = \sigma'1}{=}
                  \hat{\delta}(q_0, \sigma'1)\\
                                            &= \delta(\hat{\delta}(q_0, \sigma'), 1)\\
                                            &= \delta(q_2, 1) = q_2 \in F
                \end{aligned}
              \] 

            \item Se \( \sigma' \notin L \) allora \( \sigma' \) contiene esattamente un 1:
              \[
                \hat{\delta}(q_0, \sigma') = q_1
              \] 
              \[
                \hat{\delta}(q_0, \sigma'1) = \delta(q_1, 1) = q_2
              \] 
          \end{itemize}

        \item Supponiamo che \( \sigma  \) appartenga al linguaggio e termini con 0:
          \[
            \sigma \in L \wedge \sigma = \sigma'0
          \] 
          Per definizione di \( L \) abbiamo che
          \[
            \sigma \in L \wedge \sigma = \sigma'0 \Rightarrow \sigma' \in L
          \] 
          Dimostriamo l'ipotesi induttiva:
          \[
            \hat{\delta}(q_0, \sigma') = q_2
          \] 
          allora
          \[
            \begin{aligned}
              \hat{\delta}(q_0, \sigma) &= \hat{\delta}(q_0, \sigma'0)\\
                                        &= \delta(\hat{\delta}(q_0, \sigma'), 0)\\
                                        &= \delta(q_2, 0) = q_2 \in F
            \end{aligned}
          \] 

        \item Supponiamo che \( \sigma  \) non appartenga al linguaggio e contiene
          esatatmente un 1:
          \begin{itemize}
            \item \( \sigma  = \sigma'0 \Rightarrow \sigma' \notin L \) e contiene esattamente un 1\\
              \noindent
              Ipotesi induttiva:
              \[
                \hat{\delta}(q_0, \sigma') = q_1
              \] 
              \[
                \Downarrow
              \] 
              \[
                \begin{aligned}
                  \hat{\delta}(q_0, \sigma) &= \hat{\delta}(q_0, \sigma'0)\\
                                            &= \delta(\hat{\delta}(q_0, \sigma'), 0)\\
                                            &= \delta(q_1, 0) = q_1 \notin F
                \end{aligned}
              \] 

            \item \( \sigma = \sigma'1 \Rightarrow \sigma' \notin L \) e non contiene 1\\
              \noindent
              Ipotesi induttiva:
              \[
                \hat{\delta}(q_0, \sigma') = q_0
              \] 
              \[
                \Downarrow
              \] 
              \[
                \begin{aligned}
                  \hat{\delta}(q_0, \sigma) &= \hat{\delta}(q_0, \sigma'1)\\
                                            &= \delta(\hat{\delta}(q_0, \sigma'), 1)\\
                                            &= \delta(q_0, 1) = q_1 \notin F
                \end{aligned}
              \] 
          \end{itemize}

        \item Supponiamo che \( \sigma  \) non appartenga al linguaggio e non contiene 1
          \[
            \sigma \notin L \wedge  \sigma  = \sigma'0
          \] 
          \( \sigma = \sigma'1 \) non è possibile per l'ipotesi che \( \sigma  \) non contiene 1\\
          \noindent
          Ipotesi induttiva:
          \[
              \hat{\delta}(q_0, \sigma') = q_0\\
          \] 
          \[
            \Downarrow
          \] 
          \[
            \begin{aligned}
              \hat{\delta}(q_0, \sigma) &= \hat{\delta}(q_0, \sigma'0)\\
                                        &= \delta(\hat{\delta}(q_0, \sigma'), 0)\\
                                        &= \delta(q_0, 0) = q_0 \notin F
            \end{aligned}
          \] 
      \end{itemize}
  \end{itemize}
  Tutti i casi sono dimostrati, quindi abbiamo dimostrato che:
  \[
    L = L(M) \quad \Rightarrow \quad L \text{ è regolare}
  \]
\end{example}
\begin{exercise}
  Consideriamo il seguente linguaggio:
  \[
    \begin{aligned}
      L &= \left\{ \sigma \in \Sigma^* \;\left|\; \text{ogni sequenza di } 0 \text{ è di lunghezza pari} \right.\right\}\\
    \end{aligned}
  \] 
  \[
    \Sigma = \{0,1\}
  \] 
  Si può accettare anche sequenze di lunghezza 0. Alcuni esempi sono:
  \[
    \begin{aligned}
      101 \notin L\\
      1111 \in L\\
      10010000 \in L\\
      00101 \notin L\\
    \end{aligned}
  \] 
  L'automa a stati finiti \( M \) è il seguente:
  \begin{itemize}
    \item \( q_0 \): Non sono stati letti 0
    \item \( q_1 \): Sequenza di 0 consecutiva di lunghezza dispari
    \item \( q_2 \): Sequenza di 0 consecutiva di lunghezza pari
  \end{itemize}
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}[->, node distance=2cm, on grid, auto]
      \node[state, initial, accepting] (q0) {$q_0$};

      \node[state, right of=q0] (q1) {$q_1$};

      \node[state, right of=q1, accepting] (q2) {$q_2$};

      \node[state, above of=q1] (q3) {$q_{\bot}$};

      \draw (q0) edge[loop above] node{1} (q0);
      \draw (q0) edge[above] node{0} (q1);
      \draw (q1) edge[bend left, above] node{0} (q2);
      \draw (q2) edge[bend left, above] node{0} (q1);
      \draw (q2) edge[loop above] node{1} (q2);
      \draw (q1) edge[left] node{1} (q3);
      \draw (q3) edge[loop above] node{0,1} (q3);
    \end{tikzpicture}
    \caption{Automa a stati finiti per il linguaggio \( L \)}
  \end{figure}
  \noindent
  La tesi è:
  \begin{itemize}
    \item \( \sigma \in L \) e non contiene 0:
      \(
        \Rightarrow \hat{\delta}(q_0, \sigma) = q_0
      \) 

    \item \( \sigma \in L \) e contiene una sequenza pari di 0:
      \(
        \Rightarrow \hat{\delta}(q_0, \sigma) = q_2
      \)

    \item \( \sigma \notin L \) e contiene una sequenza \textbf{finale} dispari di 0:
      \(
        \Rightarrow \hat{\delta}(q_0, \sigma) = q_1
      \)

    \item \( \sigma \notin L \) e contiene una sequenza dispari di 0 seguita da 1:
      \(
        \Rightarrow \hat{\delta}(q_0, \sigma) = q_{\bot}
      \)
  \end{itemize}
\end{exercise}
\begin{exercise}
  Consideriamo il seguente linguaggio (ogni sequenza di 0 è di lunghezza almeno 2):
  \[
    L = \left\{ \sigma \in \Sigma^* \;\left|\; \exists n \ge 1 \;.\; \sigma = 0^n \Rightarrow n \ge 2 \right.\right\}\\
  \] 
  \[
    \Sigma = \{0,1\}
  \]
  L'automa a stati finiti \( M \) è il seguente:
  \begin{itemize}
    \item \( q_0 \): Non contiene 0, oppure \textbf{tutte} le sequenze di 0
      sono lunghe almeno 2

    \item \( q_1 \): Esattamente uno 0
      
    \item \( q_2 \): Almeno due 0
  \end{itemize}
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}[->, node distance=2cm, on grid, auto]
      \node[state, initial, accepting] (q0) {$q_0$};

      \node[state, right of=q0] (q1) {$q_1$};

      \node[state, right of=q1, accepting] (q2) {$q_2$};

      \node[state, below of=q1] (q3) {$q_{\bot}$};

      \draw (q0) edge[loop above] node{1} (q0);
      \draw (q0) edge[above] node{0} (q1);
      \draw (q1) edge[above] node{0} (q2);
      \draw (q0) edge[bend left, above] node{1} (q2);
      \draw (q2) edge[loop above] node{0} (q2);
      \draw (q1) edge[right] node{1} (q3);
      \draw (q3) edge[loop below] node{0,1} (q3);

    \end{tikzpicture}
    \caption{Automa a stati finiti per il linguaggio \( L \)}
  \end{figure}
  \noindent
  La tesi è:
  \begin{itemize}
    \item \( \sigma \in L \) e \( \sigma = \sigma'1 \)
      \(
        \Rightarrow \hat{\delta}(q_0, \sigma) = q_0
      \) 

    \item \( \sigma \in L \) e \( \sigma = \sigma'0 \) 
      \(
        \Rightarrow \hat{\delta}(q_0, \sigma) = q_2
      \) 

    \item \( \sigma \notin L \) e \( \sigma = \sigma'0 \) dove l'ultima sequenza di 0
      è esattamente lunga 1:
      \(
        \Rightarrow \hat{\delta}(q_0, \sigma) = q_1
      \) 

    \item \( \sigma \notin L \) e \( \sigma  \) contiene una sequenza lunga 1 di 0:
      \(
        \Rightarrow \hat{\delta}(q_0, \sigma) = q_{\bot}
      \)
  \end{itemize}
\end{exercise}

\subsection{Automi a stati finiti non deterministici (NFA)}
Un automa a stati finiti non deterministico si crea quando ad un solo simbolo sono
associate più transizioni. Quando questo succede gli stati vengono considerati in
parallelo. Un NFA è definito come una quintupla:
\[
  N = \left< Q, \Sigma, \delta, q_0, F \right>
\] 
\begin{itemize}
  \item \( Q \) è un insieme finito di stati
  \item \( \Sigma  \) è un insieme finito di simboli (alfabeto)
  \item \( q_0 \in Q \) è uno stato e identifica lo stato iniziale
  \item \( F \subseteq Q \) è l'insieme degli stati finali
  \item \( \delta: Q \times \Sigma \to \mathcal{P}(Q) \) è una funzione di transizione che
    dato uno stato e un simbolo restituisce un insieme di stati \textbf{potenzialmente}
    raggiungibili. È possibile che esistano coppie associate all'insieme vuoto:
    \[
      \varnothing \in \mathcal{P}(Q)
    \] 
    Inoltre non è obbligatorio avere un arco uscente per ogni simbolo di \( \Sigma \).
  \item \( \hat{\delta}: Q \times \Sigma^* \to \mathcal{P}(Q) \) Descrive gli stati che si possono
    raggiungere leggendo una sequenza di simboli:
    \[
      \begin{cases}
        \hat{\delta}(q, \epsilon) = \{q\}\\
        \hat{\delta}(q, wa) = \bigcup_{p \in \hat{\delta}(q,w)} \delta(p, a)
      \end{cases}
      \quad
      w \in \Sigma^*, \quad a \in \Sigma 
    \] 
    È quindi la chiusura transitiva di \( \delta \)
\end{itemize}
\begin{example}
  Un esempio di NFA è il seguente:
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}[->, node distance=2cm, on grid, auto]
      \node[state, initial] (q0) {$q_0$};

      \node[state, above right of=q0] (q1) {$q_1$};

      \node[state, below right of=q1] (q2) {$q_2$};

      \draw (q0) edge[above] node{a} (q1);
      \draw (q0) edge[below] node{a} (q2);

    \end{tikzpicture}
    \caption{Esempio di NFA}
  \end{figure}
  \[
    \delta(q_0, a) = \{q_1, q_2\} \subseteq Q
  \] 
\end{example}

\subsubsection{Linguaggio riconosciuto da un NFA}
Un linguaggio \( L \) è riconosciuto da un NFA \( N \) se:
\[
  L(N) = \left\{ \sigma \in \Sigma^* \;\left|\;
  \hat{\delta}(q_0, a) \cap F \neq \varnothing \right.\right\}
\] 
\begin{example}
  Consideriamo il seguente linguaggio:
  \[
  L(N) = \left\{ \sigma \in \Sigma^* \;\left|\; \sigma \text{ contiene almeno due } 1 \right. \right\}
  \] 
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}[->, node distance=2cm, on grid, auto]
      \node[state, initial] (q0) {$q_0$};

      \node[state, right of=q0] (q1) {$q_1$};

      \node[state, above of=q0] (q2) {$q_2$};

      \draw (q2) edge[right] node{1} (q0);
      \draw (q0) edge[bend left, below] node{1} (q1);
      \draw (q1) edge[bend left, below] node{1} (q0);

      \draw (q1) edge[below] node{1} (q2);
      \draw (q2) edge[bend left, above] node{0,1} (q1);

      \draw (q2) edge[loop above] node{0,1} (q2);

    \end{tikzpicture}
    \caption{Esempio di NFA per il linguaggio \( L \)}
  \end{figure}
\end{example}

\begin{theorem}[Teorema di Rabin-Scott]
  Ogni linguaggio riconosciuto da un NFA è riconosciuto da un DFA.
  \[
    \forall N = (Q, \Sigma, \delta, q_0, F) \; \exists M = (Q', \Sigma, \delta', q_0', F') \;.\;
    L(N) = L(M)
  \] 

  \vspace{1em}
  \noindent
  \textbf{Dimostrazione}: Consideriamo un NFA \( N = (Q, \Sigma, \delta, q_0, F) \) e
  costruiamo un DFA \( M = (Q', \Sigma, \delta', q_0', F') \).
  Gli insiemi degli stati sono:
  \[
    Q' = \mathcal{P}(Q)
  \] 
  \[
   Q = \{q_0, q_1, q_2\}
  \] 
  \[
    \Downarrow
  \] 
  \[
    Q' = \left\{ 
      \stackrel{q'_1}{\varnothing}, \stackrel{q'_0}{\{q_0\}}, \stackrel{q'_2}{\{q_1\}},
      \stackrel{q'_3}{\{q_2\}}, \stackrel{q'_4}{\{q_0, q_1\}}, \stackrel{q'_5}{\{q_0, q_2\}},
      \stackrel{q'_6}{\{q_1, q_2\}}, \stackrel{q'_7}{\{q_0, q_1, q_2\}}
    \right\}
  \] 
  Lo stato iniziale rimane uguale per entrambi gli insiemi: \( q'_0 = \{q_0\}  \).

  Gli insiemi degli stati finali sono:
  \[
    F' = \left\{ P \subseteq Q \;\left|\; P \cap F \neq \varnothing \right. \right\} \quad
    P \in \mathcal{P}(Q)
  \] 
  Quindi:
  \[
    F = \{q_2\} 
  \] 
  \[
    F' = \left\{ \stackrel{q'_3}{\{q_2\}}, \stackrel{q'_6}{\{q_1, q_2\}},
      \stackrel{q'_5}{\{q_0, q_2\}}, \stackrel{q'_7}{\{q_0, q_1, q_2\}}
    \right\}
  \] 
  La funzione di transizione è definita come:
  \[
    \delta'(P, a) = \bigcup_{q \in P} \delta(q, a) \in \mathcal{P}(Q) \quad P \in Q' = \mathcal{P}(Q),\; a \in \Sigma 
  \]
  Quindi:
  \[
    \begin{aligned}
      \underbrace{\delta'(q'_5, 1)}_{= \{q_1, q_2\} }
      &= \delta(q_1, 1) \cup \delta(q_2, 1)\\
                       &= \{q_0, q_2\} \cup \{q_0, q_1, q_2\}\\
                       &= \{q_0, q_1, q_2\} = q'_7
    \end{aligned}
  \] 

  \vspace{1em}
  \noindent
  Dimostriamo: 
  \begin{enumerate}
    \item 
      \( \hat{\delta}(q_0, \sigma) = \hat{\delta}'(q'_0, \sigma) = \{q_0\}  \) 

      \vspace{1em}
      \noindent
      Dimostriamo per induzione su \( \left| \sigma  \right|  \):

      \begin{itemize}
        \item 
          Se \( \sigma = \varepsilon \) allora:
          \[
            \hat{\delta}'(q'_0, \varepsilon) = q'_0 = \{q_0\} = \hat{\delta}(q_0, \varepsilon)
          \] 
          per le definizioni

        \item Se \( \sigma = \sigma'a \):
          \[
            \begin{aligned}
              \hat{\delta}'(q'_0, \sigma'a) &= \delta'(\hat{\delta}'(q_0, \sigma'), a)\\
                                            &= \delta'(\hat{\delta}(q_0, \sigma'), a)\\
                                            &= \bigcup_{p \in \hat{\delta}(q_0, \sigma')} \delta(p, a)\\
                                            &= \hat{\delta}(q_0, \sigma'a)
            \end{aligned}
          \] 
          Definizione di \( \hat{\delta}' \) non deterministica
      \end{itemize}
    \item
      \(
        \sigma \in L(N) \iff \sigma \in L(M)
      \) 

      \vspace{1em}
      \noindent
      Dimostriamo la definizione di linguaggio riconosciuto in NFA:
      \[
        \begin{aligned}
          \sigma \in L(N) &\iff \hat{\delta}(q_0, \sigma) \cap F \neq \varnothing\\
                          &\iff \hat{\delta}'(q'_0, \sigma) \cap F' \neq \varnothing \;(\text{(1.)})\\
                          &\iff \hat{\delta}'(q'_0, \sigma) \in F'\\
                          &\iff \sigma \in L(M) \; \text{(def. linguaggio accettato in DFA)}
        \end{aligned}
      \] 
  \end{enumerate}

\end{theorem}

\subsubsection{Conversione da NFA a DFA}
Prendiamo in considerazione il seguente NFA:
  \[
  L(N) = \left\{ \sigma \in \Sigma^* \;\left|\; \sigma \text{ contiene almeno due } 1 \right. \right\}
  \] 
\begin{figure}[H]
  \centering
  \begin{tikzpicture}[->, node distance=2cm, on grid, auto]
    \node[state, initial] (q0) {$q_0$};

    \node[state, right of=q0] (q1) {$q_1$};

    \node[state, above of=q0] (q2) {$q_2$};

    \draw (q0) edge[loop below] node{0,1} (q0);

    \draw (q2) edge[right] node{1} (q0);
    \draw (q0) edge[bend left, below] node{1} (q1);
    \draw (q1) edge[bend left, below] node{1} (q0);

    \draw (q1) edge[below] node{1} (q2);
    \draw (q1) edge[loop below] node{0} (q1);
    \draw (q2) edge[bend left, above] node{0,1} (q1);

    \draw (q2) edge[loop above] node{0,1} (q2);

  \end{tikzpicture}
  \caption{Esempio di NFA per il linguaggio \( L \)}
\end{figure}
\noindent
Per trasformare un NFA in un DFA bisogna creare dei nuovi stati che raggruppano
gli stati non deterministici. In questo caso:
\begin{itemize}
  \item \textbf{Tabella degli stati della NFA}:
    \begin{table}[H]
      \centering
      \begin{tabular}{|c|c|c|}
        \hline
        Stato & Input 0 & Input 1\\
        \hline
        \( q_0 \) & \( \{q_0\} \) & \( \{q_0, q_1\} \)\\
        \( q_1 \) & \( \{q_1\} \) & \( \{q_0, q_2\} \)\\
        \( q_2 \) & \( \{q_1, q_2\} \) & \( \{q_0, q_1, q_2\} \)\\
        \hline
      \end{tabular}
      \caption{Tabella di transizione della NFA}
    \end{table}

  \item \textbf{Traduzione degli stati della NFA in stati del DFA}:
    \begin{table}[H]
      \centering
      \begin{tabular}{|c|c|c|c|}
        \hline
        & & 0 & 1 \\
        \hline
        & \( \varnothing \) & \( \varnothing \) & \( \varnothing \) \\

      \end{tabular}
      \caption{Tabella di traduzione degli stati della NFA in stati del DFA}
    \end{table}
\end{itemize}


\subsection{Automi non deterministici con \texorpdfstring{\( \varepsilon  \)}{epsilon}-transizioni (\texorpdfstring{\( \varepsilon  \)-NFA}{epsilon-NFA})}
Questo tipo di NFA permette di cambiare stato anche senza leggere simboli:
\[
  q_1 \stackrel{\varepsilon}{\to} q_2
\] 
Un \( \varepsilon \)-NFA è definito come un NFA con la differenza che la funzione di transizione
è definita come:
\[
  \delta: Q \times (\Sigma \cup \{\varepsilon\}) \to \underbrace{\mathcal{P}(Q)}_{\text{Non determinismo}}
\] 
\begin{example}
  Prendiamo ad esempio il seguente \( \varepsilon \)-NFA in cui leggendo solo \( a \) 
  si può raggiungere sia \( q' \) che \( q" \):
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}[->, node distance=2cm, on grid, auto]
      \node[state, initial] (q0) {$q_0$};

      \node[state, right of=q0] (q1) {$q'$};

      \node[state, right of=q1] (q2) {$q"$};

      \draw (q0) edge[above] node{a} (q1);

      \draw (q1) edge[above] node{\( \varepsilon  \) } (q2);

    \end{tikzpicture}
    \caption{Esempio di \( \varepsilon \)-NFA}
  \end{figure}
\end{example}

\subsubsection{\texorpdfstring{\( \varepsilon  \)}{epsilon}-closure}
Per definire \( \hat{\delta} \) bisogna prima definire la \( \varepsilon \)-closure.

Una \( \varepsilon \)-closure di uno stato \( q \) è l'insieme di tutti gli stati
che si possono raggiungere da \( q \) seguendo archi etichettati con \( \varepsilon  \):
\[
  \varepsilon\text{-closure}: Q \to \mathcal{P}(Q)
\] 
O definito per insiemi:
\[
  \varepsilon\text{-closure}: \mathcal{P}(Q) \to \mathcal{P}(Q)
\] 
\[
  \varepsilon\text{-closure}(P) = \bigcup_{p \in P} \varepsilon\text{-closure}(p)
\] 

La funzione \( \hat{\delta} \) è definita come:
\[
  \begin{cases}
    \hat{\delta}(q, \varepsilon) &= \varepsilon\text{-closure}(q)\\
    \hat{\delta}(q, wa) &= \bigcup_{p \in \hat{\delta}(q,w)} \varepsilon\text{-closure}(\delta(p, a))
  \end{cases}
\] 
\begin{example}
  La \( \varepsilon \)-closure dell'esempio precedente è:
  \[
    \varepsilon\text{-closure}(q') = \{q', q"\}
  \] 
\end{example}

\vspace{1em}
\noindent
Il riconoscimento di un linguaggio è analogo a quello di un NFA:
\[
  L(N) = \left\{ \sigma \in \Sigma^*
  \;\left|\; \hat{\delta}(q_0, \sigma) \cap F \neq \varnothing \right.\right\}
\] 
\begin{theorem}
  Sia \( M = \left<Q, \Sigma, \delta, q_0, F \right> \) una \( \varepsilon \)-NFA, allora
  esiste una NFA \( M' \) tale che \( L(M) = L(M') \).

  Quindi l'insieme dei linguaggi riconosciuti da \( \varepsilon  \)-NFA coincide
  con quello degli NFA, che a sua volta coincide con i linguaggi regolari.

  \vspace{1em}
  \noindent
  \[
    M = \left<Q, \Sigma, \delta, q_0, F \right> \; \varepsilon\text{-NFA}
  \] 
  Costruiamo una NFA
  \[
    M' = \left<Q', \Sigma', \delta', q_0', F' \right>
  \]
  Dove:
  \[
    Q'=Q, \quad \Sigma'=\Sigma, \quad q_0'=q_0 
  \] 
  \[
   \quad \delta'(q,a)=\hat{\delta}(q,a) \quad
    F'= \begin{cases}
      F \cup \{q_0\} \; \text{se } \varepsilon\text{-closure}(q_0) \cap F \neq \varnothing\\
      F
    \end{cases}
  \] 
  in cui il numero degli stati rimane lo stesso, l'alfabeto non cambia e neanche lo stato
  iniziale.
\end{theorem}

\section{Espressioni regolari}
Le espressioni regolari sono operazioni algebriche sui linguaggi regolari.
Le operazioni che si possono fare sono:
\begin{itemize}
  \item \textbf{Unione}: Dati i linguaggi \( L_1, L_2 \subseteq \Sigma^* \) 
    \[
      L_1 \cup L_2 = \left\{ \sigma \;\left|\; \sigma \in L_1 \vee \sigma \in L_2 \right. \right\}
    \] 

  \item \textbf{Concatenazione}: Dati i linguaggi \( L_1, L_2 \subseteq \Sigma^* \) 
    \[
      L_1 \cdot L_2 = \left\{ \sigma_1\sigma_2 \;\left|\; \sigma_1 \in L_1 \wedge \sigma_2 \in L_2 \right. \right\}
      \equiv L_1 L_2
    \]
    Ad esempio:
    \[
      \color{blue}{L_1 = \{0101, 010101\}}
      \quad 
      \color{red}{L_2 = \{000, 111\}}
    \] 
    \[
      L_1 \cdot L_2 = \{\color{blue}{0101}\color{red}{000},
        \color{blue}{0101}\color{red}{111},
        \color{blue}{010101}\color{red}{000},
        \color{blue}{010101}\color{red}{111}
      \}
    \] 

  \item \textbf{Stella di Kleene}: Dato un linguaggio \( L \subseteq \Sigma^* \)
    \[
      L^* = \bigcup_{n \in \mathbb{N}} L^n
    \] 
    Cioè la concatenazione di \( L \) con se stesso \( n \) volte, con:
    \[
      \begin{cases}
        L^0 = \{\varepsilon\}\\
        L^{n+1} = L \cdot L^n
      \end{cases}
    \] 
    Ad esempio:
    \[
      L = \{000, 111\} 
    \] 
    \[
      \begin{aligned}
        L^0 &= \{\varepsilon\}\\
        L^1 &= L \cdot L^0 = \{000, 111\}\\
        L^2 &= L \cdot L^1 = \{000000, 000111, 111000, 111111\}\\
        L^3 &= L \cdot L^2 = \left\{\begin{array}{l}
          000000000, 000000111, 000111000, 000111111,\\
          111000000, 111000111, 111111000, 111111111
        \end{array}\right\}\\
        \vdots\\
        L^* &= \{L^0, L^1, L^2, L^3, \ldots\}\\
      \end{aligned}
    \] 

    \vspace{1em}
    \noindent
    \begin{itemize}
      \item \( L^* \) è l'insieme di tutte le possibili concatenazioni di stringhe
        appartenenti a \( L \), compresa la stringa vuota \( \varepsilon \)
      \item \( L^+ = \bigcup_{n \ge 0} L^n \) è definito come \( L^* \) senza la
        stringa vuota \( \varepsilon \):
        \[
          L^+ = L \cdot L^* 
        \]
    \end{itemize}
\end{itemize}

\begin{definition}
  Definiamo per induzione le espressioni regolari su un alfabeto \( \Sigma \):
  \begin{itemize}
    \item \textbf{Caso base}:
      \begin{itemize}
        \item \( \varnothing \subseteq \Sigma^* \) è un'espressione regolare che rappresenta
          il linguaggio vuoto

        \item \( \varepsilon \) è un'espressione regolare che rappresenta il linguaggio:
          \[
            \{\varepsilon\} \subseteq \Sigma^*
          \] 

        \item \( a \in \Sigma  \) è un'espressione regolare che rappresenta il linguaggio:
          \[
            \{a\} \subseteq \Sigma^*
          \]
      \end{itemize}

    \item \textbf{Passo induttivo}:

      \vspace{1em}
      \noindent
      Siamo \( r, s \) sono espressioni regolari che rappresentano il linguaggio
      \[
        R \subseteq \Sigma^* \wedge S \subseteq \Sigma^*
      \] 
      \begin{itemize}
        \item \( r + s \) è un'espressione regolare che rappresenta il linguaggio \( R \cup S \) 
        \item \( r \cdot s \) è un'espressione regolare che rappresenta il linguaggio \( R \cdot S \)
        \item \( r^* \) è un'espressione regolare che rappresenta il linguaggio:
          \[
            R^*
          \]
      \end{itemize}
  \end{itemize}
\end{definition}

\begin{example}
  Prendiamo ad esempio l'espressione regolare:
  \[
    1^* + 0^* + (10)^*
  \] 
  che equivale a
  \[
    \left\{ 1^n \;\left|\; n \in \mathbb{N} \right. \right\} \cup 
    \left\{ 0^n \;\left|\; n \in \mathbb{N} \right. \right\} \cup
    \left\{ (10)^n \;\left|\; n \in \mathbb{N} \right. \right\}
  \] 
\end{example}

\begin{theorem}[Teorema di equivalenza]
  Dato un DFA \( M = \left< Q, \Sigma, \delta, q_0, F \right> \) allora esiste
  un'espressione regolare \( r \) tale che \( L(M) = L(r) \).
  \[
    L \text{ Regolare} \stackrel{\text{def}}{\iff}
    \underbrace{\exists M \text{ DFA }.\; L}_{L(M) = L}
    \stackrel{\text{Th}}{\Rightarrow} \exists r \in \underbrace{ER}{\text{Espressioni Regolari}} \;.\; L(r) = L
  \] 
\end{theorem}
\begin{theorem}
  Data un'espressione regolare (ER) \( r \) esiste una \( \varepsilon \)-NFA \( M \)
  tale che: \( L(r) = L(M) \) 

  \vspace{1em}
  \noindent
  Quindi:
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}[node distance=2cm, on grid, auto]
      \node[] (s) {$\varepsilon \leadsto$};
      \node[state, accepting, right of=s] (q0) {$q_0$};
      \node[right of=q0] (f) {$L(M) = \{\varepsilon \} $};

    \end{tikzpicture}
  \end{figure}
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}[node distance=1.5cm, on grid, auto]
      \node[] (s) {$\varnothing \leadsto$};
      \node[state, right of=s] (q0) {$q_0$};
      \node[state, accepting, right of=q0] (q1) {$q_1$};
      \node[right of=q1] (f) {$L(M) = \{\varnothing \} $};

      \draw (q0) edge[above] node{a} (q1);
    \end{tikzpicture}
  \end{figure}

  \[
    \begin{aligned}
      r \text{ ER} \Rightarrow M \; \underbrace{\varepsilon\text{-NFA}}_{L(r) = L(M)} \iff\\
      M' \; \underbrace{\text{DFA}}_{L(M) = L(M') = L(r)} \iff\\
      L(r) = L(M') \text{ è regolare}
    \end{aligned}
  \] 
\end{theorem}

\begin{example}
  Consideriamo il seguente linguaggio:
  \[
    L = \left\{ \sigma \in \Sigma^* \;\left|\; \sigma \text{ contiene almeno due } 1 \right. \right\}
  \] 
  Per dimostrare che è regolare si costruisce il DFA \( M \) e si dimostra \( L = L(M) \).
  Però se si ha un'espressione regolare \( r = 0^*1 0^*1 0^* \) \textbf{non} dimostra
  che \( L \) è regolare. Bisognerebbe dimostrare \( L = L(r) \) 
\end{example}

\subsection{Proprietà dei linguaggi regolari}

\subsubsection{Proprietà di chiusura}
Indica se l'insieme dei linguaggi regolari è chiuso rispetto ad alcune operazioni,
cioè se applicando queste operazioni a linguaggi regolari si ottengono sempre
linguaggi regolari.

Operazioni:
\begin{itemize}
  \item \( * \) 
  \item \( \cup \) 
  \item \( \cdot  \) 
\item \( \cap \quad \left(L_1 \cap L_2 = \left\{ \sigma \;\left|\; \sigma \in L_1 \wedge \sigma \in L_2 \right. \right\}\right) \)
\item \( \bar{} \quad \left(\bar{L} = \left\{ \sigma \;\left|\; \sigma \\notin L \right. \right\}\right) \)
\end{itemize}

\begin{theorem}
  I linguaggi regolari sono chiusi rispetto alle operazioni di:
  \begin{itemize}
    \item Stella di Kleene
    \item Unione (finita)
    \item Concatenazione
  \end{itemize}
  Consideriamo i linguaggi regolari \( L_1, L_2 \). Allora:
  \begin{itemize}
    \item \( L_1^* \) è regolare
    \item \( L_1 \cup L_2 \) è regolare
    \item \( L_1 \cdot L_2 \) è regolare
  \end{itemize}
\end{theorem}

\begin{theorem}
  I linguaggi regolari sono chiusi rispetto alla complementazione. Dato un automa
  a stati finiti, il linguaggio complementare è riconosciuto dall'automa complementare,
  cioè quello in cui gli stati finali diventano non finali e viceversa.

  \vspace{1em}
  \noindent
  Per le leggi di De Morgan, i linguaggi regolari sono chiusi per intersezione finita:
  \[
    L_1 \cap L_2 = \overline{\overline{L_1} \cup \overline{L_2}}
  \] 
\end{theorem}


\subsubsection{Proprietà di decidibilità}
Indica se esistono algoritmi che risolvono alcuni problemi sui linguaggi regolari.
Consideriamo un insieme di stringhe accettate da un DFA (linguaggiio regolare) \( M \) 
con \( n \) stati.
\begin{itemize}
  \item \( L(M) \neq \varnothing \) se e solo se accetta almeno una stringa di lunghezza \( \le n \) 

  \item \( L(M) \) è infinito se e solo se accetta almeno una stringa di lunghezza
    \( l \) con \( n \le l < 2n \). Cioè se esiste un ciclo. Questo fornisce un estremo
    superiore per verificare se un linguaggio è infinito.

  \item \( L(M_1) = L(M_2) \) 
\end{itemize}


\subsubsection{Esistenza dell'automa minimo}
Forniamo strategie per costruire un automa minimo.

\begin{definition}[Relazione di equivalenza e partizione]
  Data una relazione \( R \) su \( \Sigma \).
  \( R \) è una relazione di:
  \begin{itemize}
    \item Equivalenza: \( E \subset \Sigma \times \Sigma  \) 
    \item Riflessiva: \( \forall a \;.\; aRa \) 
    \item Simmetrica: \( \forall a,b \;.\; aRb \Rightarrow bRa \)
    \item Transitiva: \( \forall a,b,c \;.\; aRb \wedge bRc \Rightarrow aRc \)
  \end{itemize}
  La \textbf{relazione di equivalenza} \( R \) \textbf{induce una partizione} (unione di insiemi)
  di \( S \):
  \[
    R \subseteq S \times S
  \] 
  ovvero:
  \[
    S = S_1 \cup S_2 \cup \ldots \cup S_k
  \] 
  dove \( S_i \) sono una partizione di \( S \).
  Inoltre:
  \begin{itemize}
    \item \( \forall i,j \;.\; S_i \cap S_j = \varnothing \)
    \item \( \forall i \;.\; \forall a,b \in S_i \;.\; aRb \)
    \item \( \forall a \in S_i, b \in S_j, i \neq j \;.\; a \not R b \)
  \end{itemize}

  \vspace{1em}
  \noindent
  Quindi \( R \) è come se dividesse \( S \) in insiemi disgiunti:
  \begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{partizione}
    \caption{Esempio di partizione su S}
  \end{figure}
  \noindent
  \( S_i \) sono detti \textbf{classi di equivalenza} di \( R \) e si indica con:
  \[
    a \in R \Rightarrow [a]_R = \left\{ b \;\left|\; aRb \right. \right\}
  \] 
  Ad esempio:
  \[
    cRa \Rightarrow [a]_R \equiv [c]_R
  \] 
\end{definition}

\vspace{1em}
\noindent
Definiamo di seguito due relazioni di equivalenza:
\begin{definition}[Classe di equivalenza definita per tutti i linguaggi]
  Consideriamo un linguaggio \( L \subseteq \Sigma^* \), possiamo definire una relazione
  \( R_L \) come:
  \[
    R_L \subseteq \Sigma^* \times \Sigma^*
  \] 
  \[
    x,y \in \Sigma^* \quad x R_L y \iff \forall z \in \Sigma^* \;.\; xz \in L \iff
    yz \in L
  \] 
  Quindi o entrambi appartengono a \( L \) o entrambi non appartengono a \( L \).
\end{definition}
\begin{definition}[Classe di equivalenza definita per gli automi]
  Consideriamo un automa \( M \) DFA \( \left< Q, \Sigma, \delta, q_0, F \right> \).
  Possiamo definire una relazione \( R_M \) come:
  \[
    R_M \subseteq \Sigma^* \times \Sigma^*
  \] 
  \[
    x,y \in \Sigma^* \quad x R_M y \iff \delta(q_0, x) = \delta(q_0, y)
  \] 
  Due stringhe sono in relazione se raggiungono lo stesso stato.
\end{definition}


\vspace{1em}
\noindent
\begin{definition}[Relazione invariante destra]
  Una relazione \( R \) su \( \Sigma^* \): \( R \subseteq \Sigma^* \times \Sigma^* \) 
  è \textbf{invariante destra} se e solo se:
  \[
    x,y \in \Sigma^* \quad xRy \implies \forall z \in \Sigma^* \;.\; xz R yz
  \] 
  Se due stringhe sono in relazione tra di loro, allora in qualunque modo vengano
  estese, rimarranno in relazione tra di loro.
  Quindi essere in relazione è invariante rispetto all'estensione della stringa
  \textbf{verso destra}

  \vspace{1em}
  \noindent
  \( R_L \) e \( R_M \) sono relazioni invarianti destre.
\end{definition}

\begin{definition}[Raffinamento]
  La relazione \( R_2 \) è \textbf{raffinamento} di \( R_1 \) se:
  \[
    \begin{array}{ll}
      R_2 \subseteq S \times S \\
      R_1 \subseteq S \times S
    \end{array}
    \text{ di equivalenza}
  \] 
  e \( R_1 \) è più grossa di \( R_2 \), cioè ogni classe di equivalenza di \( R_2 \)
  è contenuta in una classe di equivalenza di \( R_1 \) e quindi è come se fosse
  più dettagliata:
  \[
    \forall x \;.\; [x]_{R_2} \subseteq [x]_{R_1}
  \] 
  Il numero di classi di equivalenza di \( R_2 \) è maggiore del numero di classi
  di equivalenza di \( R_1 \).
\end{definition}

\begin{definition}
  Dato un automa DFA \( M = \left< Q, \Sigma, \delta, q_0, F \right> \),
  ogni stato definisce un linguaggio \( L_q \) come:
  \[
    q \in Q \quad : \quad L_q = \left\{ \sigma \in \Sigma^* \;\left|\; \hat{\delta}(q, \sigma) = q \right. \right\}
  \] 
\end{definition}

\begin{theorem}[Teorema di Myhill-Nerode]
  I seguenti enunciati sono equivalenti:
  \begin{enumerate}
    \item \( L \subseteq \Sigma^* \) è un linguaggio regolare, ovvero esiste un
      DFA \( M \) tale che \( L = L(M) \) 

    \item \( L \) è unione di classi di equivalenza (cioè è partizionato) indotte da
      una relazione di equivalenza \( R \) invariante destra e di indice finito,
      cioè se il numero di classi di equivalenza indotte è finito. 

    \item \( R_L \) è di indice finito
  \end{enumerate}
  Dimostriamo che:
  \[
    1. \implies 2. \underset{R \text{ raffina } R_L}{\implies} 3.
    \underset{\text{Costruisce } M}{\implies} 1.
  \] 
  \begin{itemize}
    \item \( 1. \implies 2. \)

      \textbf{Ipotesi}: \( L \) è un linguaggio riconosciuto da
      un DFA \( M = \left< Q, \Sigma, \delta, q_0, F \right> \) \( L = L(M) \).

      \textbf{Tesi}: Esisrte una relazione di equivalenza \( R \) invariante destra
      e di indice finito tale che \( L \) è unione di classi di equivalenza di \( R \).

      \vspace{1em}
      \noindent
      Prendiamo \( R = R_M \quad x R_M y \iff \delta(q_0, x) = \delta(q_0, y) \).
      \begin{enumerate}
        \item Il numero delle classi di equivalenza di \( R_M \) è uguale al numero
          di stati di \( M \): \( \left| Q \right| \)
          e \( \left| Q \right| \) è finito, quindi \( R_M \) è di indice finito

        \item \( R_M \) è invariante destra
      \end{enumerate}

      \[
        \begin{aligned}
        L = L(M) &= \left\{ x \in \Sigma^* \;\left|\; \hat{\delta}(q_0, x) \in F \right. \right\}\\
                 &= \left\{ x \in \Sigma^* \;\left|\; \hat{\delta}(q_0, x) = q \wedge q \in F \right. \right\}\\
                 &= \bigcup_{q \in F} \left\{ x \in \Sigma^* \;\left|\; \hat{\delta}(q_0, x) =
                 q \right. \right\}\\
                 &= \bigcup_{q \in F} L_q
        \end{aligned}
      \] 
      Quindi \( L \) è unione di classi di equivalenza di \( R_M \).

    \item \( 2. \implies 3. \)

      \textbf{Ipotesi}: \( L \) è unione di classi di equivalenza di
      una relazione di equivalenza \( R \) invariante destra e di indice finito.

      \textbf{Tesi}: \( R_L \) è di indice finito (numero di classi finito).

      \vspace{1em}
      \noindent
      Dimostriamo che \( R \) è raffinamento di \( R_L \) perchè allora il numero
      di classi di equivalenza di \( R \) (finito per ipotesi) sarebbe maggiore del numero di classi
      di \( R_L \) (che quindi sarebbe finito).

      Per dimostrare \( R \) raffinamento di \( R_L \) bisogna dobbiamo dimostrare
      che se due oggetti sono in relazione secondo la relazione più fine \( R \), lo sono
      anche secondo la relazione più grossa \( R_L \):
      \[
        \begin{aligned}
          \forall x,y &\;.\; xRy \implies x R_L y\\
                      &\equiv \left[ y \in [x]_R \implies y \in [x]_{R_L} \right]\\
                      &\equiv [x]_R \subseteq [x]_{R_L} \text{ per raffinamento}
        \end{aligned}
      \] 

      Prendiamo \( xRy \) sapendo che \( R \) è invariante destra, cioè:
      \[
        \forall z \in \Sigma^* \;.\; xRy \implies xzRyz
      \] 
      \[
        L = \cup \text{ classi di equivalenza}
      \] 
      Questo implica che o entrambe \( x \) e \( y \) appartengono a \( L \) o entrambe
      non appartengono a \( L \):
      \[
        xRy \implies x \in L \iff y \in L
      \] 
      \[
        [x]_R \subseteq L \quad \vee \quad [x]_R \text{ fuori da } L
      \] 

      \[
        \begin{aligned}
          xRy &\implies x \in L \iff y \in L\\
              &\underset{\text{Invariante destra}}{\implies}
              \forall z \;.\; xz R yz
              \underset{L = \cup \text{ classi}}{\implies} xz \in L \iff yz \in L\\
              &\underset{\text{Def. di } R_L}{\implies} x R_L y
        \end{aligned}
      \] 
      Questo dimostra che dato un \( M \) generico \( R_M \) è un raffinamento di \( R_L \).

    \item \( 3. \implies 1. \)
      \textbf{Ipotesi}: \( R_L \) è di indice finito.

      \textbf{Tesi}: \( L \) è regolare, cioè è riconisciuto da un DFA \( M \): \( L = L(M) \).

      \vspace{1em}
      \noindent
      Costriuiamo \( M \):
      \begin{itemize}
        \item \( Q = \left\{ [x]_{R_L} \;\left|\; x \in \Sigma^* \right. \right\} \) 
          è l'insieme delle classi di equivalenza di \( R_L \) (sono finite per ipotesi)

        \item \( \Sigma \) è l'alfabeto del linguaggio \( L \)

        \item \( q_0 = [\varepsilon]_{R_L} \) è la classe di equivalenza della stringa vuota

        \item \( F = \left\{ [x]_{R_L} \;\left|\; x \in L \right. \right\} \) è l'insieme
          delle classi di equivalenza che contengono almeno una stringa appartenente a \( L \)

        \item \( \delta(q, a) = \delta\left( [x]_{R_L}, a \right) = [xa]_{R_L} \). Si
          potrebbe anche prendere un elemento qualsiasi \( y \in [x]_{R_L} \) e definire:
          \[
            yRx \implies [x]_{R_L} = [y]_{R_L} \quad \implies \quad [ya]_{R_L} = [xa]_{R_L}
          \] 
          e questo vale perchè \( R_L \) è invariante destra. Quindi la definizione
          di \( \delta \) è una buona definizione perchè è indipendente dall'elemento
          che rappresenta la classe di equivalenza.
      \end{itemize}
      Bisogna dimostrare che \( L = L(M) \):
      \begin{itemize}
        \item Dimostrazione per induzione che \( \hat{\delta}([x],y) = [xy] \) 

          \vspace{1em}
          \noindent
          \[
            \hat{\delta}(q_0, x) = \hat{\delta}([\varepsilon]_{R_L}, x) = [x]_{R_L}
          \] 
          \[
            \begin{aligned}
              \implies x \in L(M) &\iff \hat{\delta}(q_0, x) \in F\\
                                  &\iff \hat{\delta}([\varepsilon]_{R_L}, x) \in F\\
                                  &\iff [x]_{R_L} \in F\\
                                  &\iff x \in L \implies L(M) = L
            \end{aligned}
          \] 

      \end{itemize}
      Dato \( L \) esiste un DFA \( M \) tale che \( L = L(M) \) ed \( M \) ha il numero
      minimo di stati. (Quello costruito con le classi di equivalenza di \( R_L \))
  \end{itemize}
\end{theorem}


\subsubsection{Condizione necessaria perchè un linguaggio sia regolare}
Un linguaggio \( L \) è regolare se esiste un automa, quindi per dimostrare che
un linguaggio non è regolare si può dimostrare che non esiste un automa. Per fare ciò
si usa il Pumping lemma che fornisce una condizione \( \Pi_L \) \textbf{necessaria}
alla regolarità di un linguaggio:
\[
  L \text{ regolare} \Rightarrow \Pi_L \quad \equiv \quad \neg \Pi_L \Rightarrow L \text{ non regolare}
\] 

\begin{theorem}[Pumping lemma per linguaggi regolari]
  Consideriamo un linguaggio regolare \( L \), allora esiste una costante
  \( k \in \mathbb{N} \) tale che per ogni stringa di lunghezza maggiore di \( k \) nel
  linguaggio, esiste una suddivisione della stringa in tre parti \( u,v,w \) tale che:
  \[
    \exists k \in \mathbb{N} \;.\; \forall z \in L \;:\; |z| \ge k
  \] 
  \[
    \Downarrow
  \] 
  \[
    \exists\;
    u,v,w \in \Sigma^* \;.\;
    z = uvw \wedge \begin{cases}
      |uv| \le k\\
      |v| > 0\\
      \forall i \in \mathbb{N} \;.\; uv^iw \in L
    \end{cases}
  \] 
  cioè la parte \( v \) può essere "pompata" (ripetuta \( i \) volte) e la stringa
  risultante appartiene ancora a \( L \).
  \begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{insieme_pumping_lemma}
    \caption{Rappresentazione grafica dell'insieme per il Pumping lemma}
  \end{figure}

  \vspace{1em}
  \noindent
  \textbf{Dimostrazione}:

  \noindent
  Consideriamo \( L \) regolare, allora esiste un automa DFA \( M \) tale che \( L = L(M) \) 
  \[
    M = \left< Q, \Sigma, \delta, q_0, F \right> \quad |Q| = n \in \mathbb{N} \text{ stati finiti}
  \] 
  Vogliamo dimostrare che il lemma vale per \( k = n \) (k è \( |Q| \)). Prendiamo
  una stringa nel linguaggio \( z \in L \) tal che la sua lunghezza sia maggiore o
  uguale di \( k \): \( |z| \ge k \). Scriviamo \( z \) come insieme di caratteri:
  \[
    z = a_1 a_2 a_3 \ldots a_{m} \quad m \ge k
  \] 
  Rappresentiamo l'elaborazione di \( z \) come una sequenza di stati:
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}[->,node distance=1.5cm, on grid, auto]
      \node[state] (q0) {$q_0$};
      \node[state, right of=q0] (q1) {$q_1$};
      \node[state, right of=q1] (q2) {$q_2$};
      \node[state, right of=q2] (q3) {$q_3$};
      \node[right of=q3] (dots) {$\ldots$};
      \node[state, right of=dots] (qm) {$q_m$};

      \draw (q0) edge[above] node{\( a_1 \)} (q1);
      \draw (q1) edge[above] node{\( a_2 \)} (q2);
      \draw (q2) edge[above] node{\( a_3 \)} (q3);
      \draw (q3) edge[above] node{\( a_4 \)} (dots);
      \draw (dots) edge[above] node{\( a_m \)} (qm);

      \draw[-] (0,0.5) -- ++(0,0.2) -- ++(7.5,0) node[midway, above] {z} -- ++(0,-0.2);
    \end{tikzpicture}
  \end{figure}
  \noindent
  Per leggere \( m \) simboli si attraversano \( m+1 \) stati e quindi per
  \textbf{riconoscere} \( z \) si usano \( m+1 \) stati, ma \( m \ge n \),
  quindi si attraversano almeno \( n+1 \) stati \( \Rightarrow m+1 \ge n+1 \).
  Questo implica che si attraversano più stati di quelli in \( Q \), ovvero \textbf{almeno}
  uno stato è ripetuto nel riconoscimento di \( z \).
  
  \vspace{1em}
  \noindent
  Supponiamo che \( \bar{q} \in Q \) sia il primo stato (leggendo \( z \)) che viene
  \textbf{ripetuto}, in cui si torna per riconoscere \( z \):
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}[->,node distance=1.5cm, on grid, auto]
      \node[state] (q0) {$q_0$};
      \node[right of=q0] (dots1) {$\ldots$};
      \node[state, right of=dots1] (qbar) {$\bar{q}$};
      \node[right of=qbar] (dots2) {$\ldots$};
      \node[state, right of=dots2] (qm) {$q_m$};

      \draw (q0) edge[above] node{\( a_1 \)} (dots1);
      \draw (dots1) edge[above] node{} (qbar);

      \draw (qbar) edge[loop below, dashed] node{} (qbar);
      \draw (qbar) edge[above] node{} (dots2);

      \draw (dots2) edge[above] node{\( a_m \)} (qm);

      \draw[-] (0,0.5) -- ++(0,0.2) -- ++(2.5,0) node[midway, above] {u} -- ++(0,-0.2);
      \draw[-] (2.5,0.5) -- ++(0,0.2) -- ++(1,0) node[midway, above] {v} -- ++(0,-0.2);
      \draw[-] (3.5,0.5) -- ++(0,0.2) -- ++(2.5,0) node[midway, above] {w} -- ++(0,-0.2);
    \end{tikzpicture}
  \end{figure}
  \noindent
  \begin{itemize}
    \item \( u \) va da \( q_0 \) alla prima occorrenza di \( \bar{q} \) (il primo
      stato ripetuto)

    \item \( v \) va da \( \bar{q} \) a \( \bar{q} \) (percorso che porta a ritrovare
      lo stato ripetuto)

    \item \( w \) va da \( \bar{q} \) allo stato finale \( q_m \)
  \end{itemize}
  Questa suddivisione è accettabile per il teorema?
  \begin{enumerate}
    \item \( \left| uv \right| \le k = n\) 
      \begin{figure}[H]
        \centering
        \begin{tikzpicture}[->,node distance=1.5cm, on grid, auto]
          \node[state] (q0) {$q_0$};
          \node[state, right of=q0] (q1) {$q_1$};
          \node[state, right of=q1] (q2) {$q_2$};
          \node[right of=q2] (dots1) {$\ldots$};
          \node[state, right of=dots1] (qn) {$q_{n-1}$};

          \draw (q0) edge[above] node{\( a_1 \)} (q1);
          \draw (q1) edge[above] node{\( a_2 \)} (q2);
          \draw (q2) edge[above] node{} (dots1);
          \draw (dots1) edge[above] node{\( a_{n-1} \) } (qn);
          \draw (qn) edge[loop below] node{\( a_n \) } (qn);
        \end{tikzpicture}
      \end{figure}
      \noindent
      Per arrivare allo stato finale \( q_n \) si attraversano \( n-1 \) simboli
      \[
        Q = \left\{ q_0, q_1, \ldots, q_{n-1} \right\} \to \left| Q \right| = n
      \] 
      \begin{figure}[H]
        \centering
        \begin{tikzpicture}[->,node distance=1.5cm, on grid, auto]
          \node[state] (q0) {$q_0$};
          \node[state, right of=q0] (q1) {$q_1$};
          \node[state, right of=q1] (q2) {$q_2$};
          \node[state, right of=q2] (q3) {$q_3$};
          \node[state, below right=2cm of q3] (q4) {$q_4$};
          \node[state, below left=2cm of q3] (qn) {$q_{n-1}$};
          \node[right=3.5cm of q3] (dots) {$\ldots$};

          \draw (q0) edge[above] node{\( a_1 \)} (q1);
          \draw (q1) edge[above] node{\( a_2 \)} (q2);
          \draw (q2) edge[above] node{\( a_3 \)} (q3);
          \draw (q3) edge[right] node{\( a_4 \)} (q4);
          \draw (q4) edge[above, dashed] node{\( a_{n-1} \)} (qn);
          \draw (qn) edge[left] node{\( a_n \)} (q3);
          \draw (q3) edge[above] node{} (dots);

          \draw[-] (7,0.5) -- ++(0,0.2) -- ++(1.2,0) node[midway, above] {w};
          \draw[-] (-0.5,-2) -- ++(0,-0.2) -- ++(7,0) node[midway,below]
            {$\le n \text{ num. di stati}$} -- ++(0,0.2);
        \end{tikzpicture}
      \end{figure}

    \item \( \left| v \right| > 0 \) almeno un arco deve essere uscire dallo stato
      e ritornare nello stesso stato

    \item \( uw \) arrivano in \( q_m \), quindi:
      \[
        \forall i \;.\; uv^iw \in L
      \] 
      e raggiunge lo stato finale \( q_m \)
  \end{enumerate}
\end{theorem}

\subsubsection{Dimostrazione che un linguaggio non è regolare}
Per dimostrare che un linguaggio non è regolare si può usare il pumping lemma:
\[
  L \text{ regolare } \Rightarrow \exists k \;.\; \forall z \in L \;:\; |z| \ge k
\] 
\[
  \Downarrow
\] 
\[
  \exists z = uvw \;.\; \begin{cases}
    |uv| \le k\\
    |v| > 0\\
    \forall i \in \mathbb{N} \;.\; uv^iw \in L
  \end{cases}
\]
Per fare ciò si nega il pumping lemma:
\[
  A \leadsto B
\] 
\[
  \neg B \leadsto \neg A
\] 
\[
  \exists \leadsto \forall 
\] 
Il pumping lemma si nega come segue:
\begin{itemize}
  \item \( \exists k \leadsto \forall K \):
    La dimostrazione \textbf{non} deve imporre vincoli su \( k \)

  \item \( \forall z \leadsto \exists z \in L \;.\; \left| z \right| \ge k \):
    costruiamo noi la \( z \in L \) di lunghezza \( \ge k \) 

  \item \( \exists uvw = z \leadsto \forall uvw = z \):
    \( |v| > 0 \; |uv \le k \) 
  \item \( \forall i \in \mathbb{N} \leadsto \exists i \in \mathbb{N} \;.\; uv^iw \notin L \):
    troviamo un \( i \) che "rompe" la stringa
\end{itemize}
Quindi il pumping lemma negato diventa:
\[
  \forall k \in \mathbb{N} \;.\; \exists z \in L \;:\; |z| \ge k
\] 
\[
  \Downarrow
\] 
\[
  \forall  uvw = z \;.\; \begin{cases}
    |uv| \le k\\
    |v| > 0\\
    \exists i \in \mathbb{N} \;.\; uv^iw \notin L
  \end{cases}
\] 
\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{insieme_pumping_lemma_negato}
  \caption{Rappresentazione grafica dell'insieme per il Pumping lemma negato}
\end{figure}

\begin{example}
  Consideriamo il linguaggio:
  \[
    L = \left\{ 0^n 1^n \;\left|\; n \in \mathbb{N} \right. \right\}
  \] 
  Bisogna creare un automa che riconosca tutte le stringhe, ad esempio:
  \[
    \varepsilon, 01, 0011, 000111, 00001111, \ldots
  \] 
  Scriviamo le condizioni di appartenenza a \( L \):
  \[
    0^a 1^b \in L \iff a = b
  \] 
  (gli 0 devono essere uguali agli 1).

  \noindent
  Ogni volta che si legge un 1 bisogna ricordarsi quanti 0 sono stati letti prima,
  però si possono leggere infiniti 0 e quindi servirebbero infiniti stati.
  Vogliamo quindi dimostrare che \( L \) non è regolare usando il pumping lemma negato:
  \[
    \forall k \in \mathbb{N} \;.\; \exists z \in L \;:\; |z| \ge k
  \] 
  \[
    \Downarrow
  \] 
  \[
    \forall  uvw = z \;.\; \begin{cases}
      |uv| \le k\\
      |v| > 0\\
      \exists i \in \mathbb{N} \;.\; uv^iw \notin L
    \end{cases}
  \] 

  \vspace{1em}
  \noindent
  Fissiamo \( k \in \mathbb{N} \) (\( k \) non deve avere nessun vincolo). Qualunque sia
  \( k \) prendiamo la stringa:
  \[
    z = 0^k 1^k \in L \quad |z| \ge k
  \] 
  Le uniche suddivisioni che vanno bene sono quelle in cui \( uv \) stanno nella
  parte degli 0 (perchè per ipotesi \( |uv| \le k \).
  Si può concludere che la sottostringa \( uv \) è composta da soli 0:
  \[
    uv \in 0^k
  \] 

  \vspace{1em}
  \noindent
  Consideriamo la stringa:
  \[
    z_i = uv^iw = 0^{k + (i-1)|v|} 1^k
  \] 
  (ripetere \( v \) \( i \) volte equivale ad aggiungere \( (i-1)|v| \) volte \( v \) a quella
  già esistente). Ad esempio:
  Supponendo di avere 10 zeri un esempio di suddivisione è il seguente
  \[
    \underbrace{000000}_{u}
    \underbrace{000}_{v}
    \underbrace{01111111111}_{w}
  \] 
  \begin{itemize}
    \item Con \( i = 0 \) (togliere \( v \))
      \[
        0^{k + (0-1)|v|} 1^k = 0^{k - |v|} 1^k
      \] 

    \item Con \( i = 1 \) (lasciare \( v \) una volta)
      \[
        0^{k + (1-1)|v|} 1^k = 0^{k} 1^k
      \] 

    \item Con \( i = 3 \) (ripetere \( v \) tre volte)
      \[
        0^{k + (3-1)|v|} 1^k = 0^{k + 2|v|} 1^k
      \]
  \end{itemize}

  \vspace{1em}
  \noindent
  Troviamo un \( i \) tale che \( z_i \notin L \):
  \[
    z_i = 0^{k + (i-1)|v|} 1^k
  \] 
  Scegliamo ad esempio \( i = 2 \):
  \[
    \begin{aligned}
      z_2 = 0^{k + |v|} 1^k \in L &\iff \underbrace{k + |v|}_a = \underbrace{k}_b\\
                                  &\iff |v| = 0 \text{ che è assurdo perchè } |v| > 0
                                  &\implies z_2 \notin L
    \end{aligned}
  \] 
\end{example}

\section{Linguaggi context free}
I linguaggi context free sono più potenti dei linguaggi regolari, mantenendo comunque
la decidibilità di molti problemi. Lo strumento che si utilizza per \textbf{definire} i linguaggi
context free sono le \textbf{grammatiche}. Rispetto ai linguaggi regolari in cui l'automa
era un riconoscitore, per i linguaggi context free la grammatica è un generatore.
Un esempio di linguaggi context free sono i linguaggi di programmazione.

\subsection{Grammatiche context free}
Una grammatica context free è una quadrupla \( E = \left< V, T, P, S \right> \) dove:
\begin{itemize}
  \item \( V \) è un insieme \textbf{finito} di simboli \textbf{non terminali} (variabili).
    Il loro ruolo è quello di rappresentare una categoria sintattica (ad esempio: espressione,
    istruzione, comando, ecc.)

  \item \( T \) è un insieme finito di simboli \textbf{terminali}. Sono i simboli effettivi
    che possono essere sostituiti a quelli non terminali. Ad esempio:
    \[
      \underbrace{x}_{\text{terminale}} = \underbrace{\text{espressione}}_{\text{non terminale}}
    \] 

  \item \( P \) è un insieme finito di \textbf{produzioni} (regole di derivazione).
    Indicano come sostituire i simboli non terminali con stringhe di simboli.

  \item \( S \in V \) è il simbolo iniziale

  \item il \textbf{tipo di produzioni} (come possono essere fatte) determina il tipo
    di grammatica. In una \textbf{grammatica context free} le produzioni sono della forma:
    \[
      A \to \alpha
    \] 
    dove \( A \in V \) (non terminali) e \( \alpha \in \left( V \cup T \right)^* \)
    (sequenza di simboli terminali e non terminali). Il vincolo principale è il fatto
    che a sinistra del'implicazione ci sia esattamente \textbf{una sola variabile}.
    Questa implicazione significa che \( A \) può essere sostituito con \( \alpha \),
    indipendentemente dal contesto in cui \( A \) si trova,
    cioè dai simboli presenti prima e dopo \( A \).
\end{itemize}

\begin{example}
  Un esempio di grammatica sono le \textbf{espressioni booleane}.
  Consideriamo la grammatica: \( G = \left< V, T, P, S \right> \) dove:
  \[
    \begin{aligned}
      V &= \left\{ E \right\}\\
      T &= \left\{ 0, 1, \text{and}, \text{or}, \text{not}, (, ) \right\}\\
      P &= \left\{
        \begin{aligned}
          &E \to 0 \;|\; 1 \equiv E \to 0 \; E \to 1\\
          &E \to (E \text{ and } E)\\
          &E \to (E \text{ or } E)\\
          &E \to \text{not } E
        \end{aligned}
      \right. = 
      E \to 0 \;|\; 1 \;|\; (E \text{ and } E) \;|\; (E \text{ or } E) \;|\; (\text{not } E)
      \\
        S &= E
    \end{aligned}
  \] 
  (\( | \) è l'operatore or). Una grammatica può essere scritta anche solamente con
  l'insieme delle sue produzioni.

  \vspace{1em}
  \noindent
  Il \textbf{linguaggio generato} dalla grammatica \( G \) è una combinazione di
  produzioni. Ad esempio:
  \[
    A \to \beta \in P \quad \alpha, \gamma \in (V \cup T)^*
  \] 
  dove:
  \[
  \begin{aligned}
    A &\in V\\
    \beta &\in (V \cup T)^*
  \end{aligned}
  \] 
  Si deriva la stringa a destra applicando la produzione \( A \to \beta \) 
  \[
    \alpha \color{red}A\color{black} \gamma \Rightarrow \alpha \color{red}\beta\color{black} \gamma
  \] 
  Si è generata una nuova stringa sostituendo \( A \) con \( \beta \).
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}
      \coordinate (A) at (0,0);
      \draw[red,fill] (A) circle (0.1cm) node[above=0.2cm] {\( A \)};
      \node[below left=1.5cm of A] (A1) {};
      \node[below right=1.5cm of A] (A2) {};
      \draw[red,|-|] (A1) -- (A2) node[midway, below] {\( \beta \in P \)};
      \draw[red] (A) -- (A1);
      \draw[red] (A) -- (A2);
      \draw[red,->] (A) -- ++(0,-0.5);
    \end{tikzpicture}
    \caption{Rappresentazione grafica della produzione \( A \to \beta \)}
  \end{figure}
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}
      \coordinate (A) at (0,0);
      \coordinate[below left=1.5cm of A] (Al) {};
      \coordinate[below right=1.5cm of A] (Ar) {};
      \draw[red,|-|] (Al) -- (Ar) node[midway, below] {\( \beta \)};
      \draw[red] (A) -- (Al);
      \draw[red] (A) -- (Ar);

      \coordinate (alpha) at A + (-2,0);
      \draw[|-] (alpha) -- (A) node[midway, above] {\( \alpha \)};

      \coordinate (gamma) at A + (+3,0);
      \draw[|-] (gamma) -- (A) node[midway, above] {\( \gamma \)};

      \draw[red,fill] (A) circle (0.1cm) node[above=0.2cm] {\( A \)};

      \coordinate[xshift=-2cm] (alpha2) at (Al);
      \draw[-] (alpha) -- (alpha2);
      \draw[|-] (alpha2) -- (Al) node[midway, below] {\( \alpha \)};

      \coordinate[xshift=+3cm] (gamma2) at (Ar);
      \draw[-] (gamma) -- (gamma2);
      \draw[|-] (gamma2) -- (Ar) node[midway, below] {\( \gamma \)};
    \end{tikzpicture}
    \caption{Rappresentazione grafica della derivazione \( \alpha A \gamma \Rightarrow
      \alpha \beta \gamma \)}
  \end{figure}
  \noindent
  La singola freccia (\( \to  \)) indica una produzione, mentre la doppia freccia
  (\( \Rightarrow \)) indica una derivazione (applicazione di una produzione).
  \noindent
  Altri esempi di derivazioni:
  \begin{itemize}
    \item \( E \to (E \text{ or } E) \equiv E \Rightarrow (E \text{ or } E) \) 
    \item \( E \to (E \text{ and } E) \equiv
      \underbrace{(}_{\alpha}\underbrace{E}_{A} \underbrace{\text{ or } E)}_{\gamma } \Rightarrow
      \underbrace{(}_{\alpha}\underbrace{(E \text{ and } E)}_{\beta} \underbrace{\text{ or } E)}_{\gamma} \)
  \end{itemize}
\end{example}
\begin{definition}[Derivazione]
  Partendo da una sequenza \( \alpha \in V \cup T \) una derivazione consiste nell'applicazione
  di una produzione ad uno dei simboli non terminali presenti nella sequenza \( \alpha \).
  Consideramdo la stringa:
  \[
    \alpha_1, \alpha_2, \alpha_3, \ldots, \alpha_n \in (V \cup T)^*
  \] 
  e
  \[
    \underset{j \in \left[ 1, n-1 \right]}{\forall j} \;.\; \alpha_j \Rightarrow \alpha_{j+1}
  \] 
  allora si può scrivere l'applicazione di \( n \) produzioni come:
  \[
   \alpha_1 \Rightarrow_n \alpha_n
  \] 
  Ad esempio:
  \[
    \alpha_i \Rightarrow_0 \alpha_j \quad \text{ Nessuna produzione applicata}
  \] 
  \vspace{1em}
  \noindent
  Si può scrivere la chiusura transitiva di \( \Rightarrow_n \) come:
  \[
    \alpha \Rightarrow^* \beta \text{ se } \exists i \;.\; \alpha \Rightarrow_i \beta
  \] 
  Cioè esiste un numero di passi \( i \) tale che applicando \( i \) produzioni si passa
  da \( \alpha \) a \( \beta \). (Da \( \alpha \) si deriva \( \beta \))
\end{definition}

\begin{definition}[Linguaggio generato]
  Il linguaggio \( L \) è generato dalla grammatica \( G: L(G) \) con \( G = \left<V,T,P,S\right> \)
  è l'insieme delle sequenze di terminali generate da \( S \), ovvero per le quali
  esiste una derivazione a partire da \( S \):
  \[
    L(G) = \left\{ \sigma \in T^* \;\left|\; S \stackrel{G}{\Rightarrow^*}
    \sigma \right. \right\}
  \] 
  Il linguaggio \( L \subseteq T^* \) è \textbf{context free} se esiste una grammatica
  \( G \) context free che lo genera, ovvero tale che \( L = L(G) \). Quindi per dimostrare
  che un linguaggio è context free basta costruire una grammatica che lo genera.
\end{definition}

\subsubsection{Dimostrazione canonica che un linguaggio è context free}
\begin{definition}[Dimostrazione che un linguaggio è context free]
  Possiamo dimostrare che \( L = L(G) \), quindi \( L \) è context free. Dimostriamo:
  \begin{enumerate}
    \item \( L \subseteq L(G) \)

      \vspace{1em}
      \noindent
      Se \( \sigma  \) appartiene al linguaggio, allora da \( S \) si può derivare \( \sigma \):
      \[
        L \subseteq L(G) \equiv \sigma \in L \implies \sigma \in L(G) \equiv S \Rightarrow^* \sigma
      \] 

      \vspace{1em}
      \noindent
      Dimostriamo per induzione sulla lunghezza di \( \sigma  \).
      \begin{itemize}
        \item \textbf{Caso base}: \( |\sigma| = \text{ lunghezza minima in } L \)
        \item \textbf{Passo induttivo}: Si suppone che per ogni \( \sigma  \) di lunghezza
          \( n \) valga l'ipotesi induttiva, e si dimostra che vale anche per
          \( \sigma  \) di lunghezza maggiore di \( n \).
          \[
            \forall \sigma  \;.\; |\sigma| = n \text{ vale l'ipotesi induttiva 1.}
          \] 
      \end{itemize}
    \item \( L(G) \subseteq L \) 

      \vspace{1em}
      \noindent
      Se \( \sigma \) appartiene al linguaggio generato dalla grammatica, allora \( \sigma \)
      appartiene al linguaggio:
      \[
        L(G) \subseteq L \equiv \sigma \in L(G) \implies \sigma \in L \equiv S \Rightarrow^* \sigma
      \] 

      \vspace{1em}
      \noindent
      Dimostriamo per induzione sulla lunghezza della derivazione:
      \[
        S \Rightarrow^* \sigma \equiv \exists i \;.\; S \Rightarrow_i \sigma
      \] 
      quindi l'induzione è su \( i \).
      \begin{itemize}
        \item \textbf{Caso base}: La derivazione più corta possibile di simboli terminali
        \item \textbf{Passo induttivo}: Si suppone che per ogni \( i \le n \) se
          la \( S \) deriva in \( i \) passi in una stringa di soli terminali \( \sigma  \) 
          allora \( \sigma  \in L \):
          \[
            \forall i \le n \;.\; S \Rightarrow_i \sigma \text{ allora } \sigma \in L
          \] 
          Dimostriamo che \( S \Rightarrow_{n+1} \sigma \text{ allora } \sigma \in L \).
      \end{itemize}
  \end{enumerate}
\end{definition}

\begin{example}
  Consideriamo il seguente linguaggio:
  \[
  L = \left\{ 0^n 1^n \;\left|\; n \in \mathbb{N} \right. \right\}
  \] 
  Vogliamo dimostrare che \( L \) è context free. Per farlo bisogna costruire una grammatica
  \( G \) che genera \( L \):
  \begin{itemize}
    \item \( S \): Bisogna chiedersi se la stringa vuota appartiene a \( L \).
      \begin{enumerate}
        \item \( \varepsilon \in L? \) Si, per \( n = 0 \) \( 0^0 1^0 = \varepsilon \).
          \( \varepsilon \) va sempre generata dalla grammatica.
      \end{enumerate}
      Quindi \( S \) diventa:
      \[
        S \to \varepsilon \;|\; 0 S 1
      \] 
  \end{itemize}
  La grammatica di questo linguaggio quindi è:
  \[
    G: S \to \varepsilon \;|\; 0 S 1 \text{ (grammatica context free)}
  \] 

  \vspace{1em}
  \noindent
  Dimostriamo: \( L = L(G) \) 
  \begin{enumerate}
    \item \( L \subseteq L(G) \) per induzione sulla lunghezza di \( \sigma \in L \)
      \begin{itemize}
        \item \text{Tesi}: \( \sigma \in L \) allora \( S \Rightarrow^* \sigma \)
        \item \textbf{Base}: \( n = 0 \) \( |\sigma| = 0 \) è tale che \( \sigma = \varepsilon  \) 
          e \( \sigma  \to \varepsilon \; (S \Rightarrow \varepsilon ) \) 
        \item \textbf{Passo induttivo}: L'ipotesi induttiva è la tesi, ma limitata a n:
          \[
            \forall i \le n \;.\; |\sigma| = i \; \sigma \in L \text{ allora } S \Rightarrow^* \sigma
          \] 
          Dimostriamo che \( |\sigma| = n \) e \( \sigma \in L \) allora \( S \Rightarrow^* \sigma \).
          Prendiamo \( \sigma \in L \), ovvero:
          \[
            \exists j \;.\; \sigma = 0^j 1^j \quad 2j = n
          \] 
          Bisogna trovare la derivazione. Possiamo osservare che:
          \[ \sigma = 0 \underbrace{0^{j-1} 1^{j-1}}_{2j-2 < n} 1 \]
          (abbiamo separato il primo 0 e l'ultimo 1 dalla
          sequenza centrale). \( \sigma  \) può essere scritta come:
          \[
            \sigma = 0 \underbrace{0^{j-1} 1^{j-1}}_{2j-2 < n} 1
            = 0 \sigma' 1 \quad \text{ con } \sigma' = 0^{j-1} 1^{j-1} \in L
          \]
          Siccome \( \left| \sigma' \right| < n \) e \( \sigma' \in L \) allora vale
          l'ipotesi induttiva e quindi:
          \[
            \implies \exists S \Rightarrow^* \sigma' \equiv 0^{j-1} 1^{j-1}
          \] 
          usiamo questa derivazione per costruire la derivazione di \( \sigma \).
          Questa derivazione deve essere fatta in modo da avere come ultimo passo
          \( S \Rightarrow \varepsilon \) per ottenere \( \sigma' \):
          \[
            S \Rightarrow^* 0^{j-1} 1^{j-1} \equiv
            S \Rightarrow 0^{j-1} S 1^{j-1} \Rightarrow 0^{j-1} \varepsilon 1^{j-1}
            = 0^{j-1} 1^{j-1}
          \] 
          Per arrivare a \(\sigma = 0^j 1^j \) bisogna sostituire la \( S \) con \( 0 S 1 \):
          \[
            S \Rightarrow^* 0^j S 1^j \Rightarrow 0^{j-1} 0 S 1 1^{j-1} \Rightarrow
            0^{j-1} 0 \varepsilon 1 1^{j-1} = 0^j 1^j = \sigma \quad \square
          \] 
          Questo conclude la dimostrazione del primo punto, cioè trovare una derivazione
          per \( \sigma \). Quindi abbiamo dimostrato che \( L \subseteq L(G) \).
      \end{itemize}

    \item \( L(G) \subseteq L \) Dimostriamo per induzione sulla lunghezza delle derivazioni
      \begin{itemize}
        \item \textbf{Tesi:} \( S \Rightarrow^* \sigma  \) allora \( \sigma  \in L \) 
        \item \textbf{Base:} \( S \Rightarrow \varepsilon \; (S \to \varepsilon ) \) e
          \( \varepsilon \in L \) 
        \item \textbf{Passo induttivo:} L'ipotesi induttiva è la tesi limitata a \( i \):
          \[
            \forall i \le n \;.\; S \Rightarrow_i \sigma \text{ allora } \sigma \in L
          \] 
          Prendiamo una stringa generata in \( n \) passi:
          \[
            S \Rightarrow_n \sigma \quad\equiv\quad S \Rightarrow_{n-1} \sigma_1 S \sigma_2
            \Rightarrow \sigma_1 \sigma_2 = \sigma
          \] 
          Per come è fatta la grammatica \( \sigma_1 \) contiene solo 0 e \( \sigma_2 \)
          contiene solo 1. Ovvero:
          \[
            \sigma_1 = 0^j \quad \sigma_2 = 1^k
          \] 
          Bisogna riuscire ad ottenere una derivazione più corta di \( n \) simboli terminali
          in modo tale da poter applicare l'ipotesi induttiva. Un solo passo indietro
          non basta perchè non si ha una derivazione di tutti simboli terminali, quindi
          si può andare ancora più indietro:
          \[
            S \Rightarrow_{n-1} \sigma_1 S \sigma_2 \quad\equiv\quad
            S \Rightarrow_{n-2} \sigma_1' \color{red}S\color{black} \sigma_2'
            \Rightarrow \underbrace{\sigma_1
            \color{red}0\color{black}}_{\sigma_1} \color{red}S\color{black}
            \underbrace{\color{red}1\color{black} \sigma_2}_{\sigma_2}
          \] 
          Scorporiamo la derivazione di \( n-2 \) passi in:
          \[
            S \Rightarrow_{n-2} \sigma_1' S \sigma_2' \Rightarrow \sigma_1' \sigma_2'
          \] 
          abbiamo una derivazione lunga \( n-1 \) di simboli terminali dove:
          \[
          \begin{aligned}
            \sigma_1 = \sigma_1' 0 &\implies \sigma_1' = 0^{h-1}\\
            \sigma_2 = 1 \sigma_2' &\implies \sigma_2' = 1^{k-1}
          \end{aligned}
          \] 
          Per ipotesi induttiva:
          \[
            \stackrel{I.I.}{\implies} \sigma_1' \sigma_2' \in L
          \] 
          Quindi:
          \[
            \sigma_1' \sigma_2' = 0^{h-1} 1^{k-1} \in L \implies h-1 = k-1 \implies h = k
          \] 
          ma allora \( \sigma = \sigma_1 \sigma_2 = 0^h 1^k \) è tale che \( h=k \) 
          e quindi \( \sigma \in L \) 

          \vspace{1em}
          \noindent
          Ricapitolando:
          \begin{itemize}
            \item Si parte da \( S \Rightarrow_n 0^h 1^k \), ma non si può applicare
              l'ipotesi induttiva, quindi si va un passo indietro
            \item  \( S \Rightarrow_{n-1} 0^h S 1^k \), ma ancora non si può applicare l'ipotesi induttiva
            \item Si va ancora un passo indietro:
              \[
                S \Rightarrow_{n-2} 0^{h-1} S 1^{k-1} \Rightarrow 0^{h-1} 0 S 1 1^{k-1}
                \Rightarrow 0^h 1^k
              \] 
              questo si può usare per generare \( 0^{h-1} 1^{k-1} \):
              \[
                S \Rightarrow_{n-2} 0^{h-1} S 1^{k-1} \Rightarrow 0^{h-1} 1^{k-1}
              \] 
              che è lunga \( n-1 \) e quindi si può applicare l'ipotesi induttiva.
            \item Quindi \( 0^{h-1} 1^{k-1} \in L \implies h-1 = k-1 \implies h = k \)
              e quindi \( 0^h 1^k \in L \quad \square \)
          \end{itemize}
          Siamo andati indietro tanto quanto serviva per generare una stringa di soli
          simboli terminali.
      \end{itemize}
  \end{enumerate}
\end{example}

\subsubsection{Dimostrazione alternativa}
\begin{example}
  Considerando l'esempio precedente, una dimostrazione alternativa che un linguaggio è context free consiste
  nel partire dall'inizio e fare passi avanti piuttosto che partire dalla fine e fare passi indietro.
  Questo non funziona sempre, ma in questo caso sì:
  \[
    L(G) \subseteq L
  \] 
  \[
    \forall i < n \;.\; S \Rightarrow_i \sigma \text{ allora } \sigma \in L
  \] 
  \begin{itemize}
    \item \textbf{Passo induttivo}:
      \[
        S \Rightarrow_n \sigma \quad\equiv\quad S \Rightarrow 0 S 1 \Rightarrow_{n-1} \sigma
        \quad\equiv\quad 0 \sigma' 1
      \] 
      dove \( S \Rightarrow_{n-1} \sigma' \). Ma allora si può applicare l'ipotesi induttiva
      e quindi \( \sigma' \in L \) e questo significa che:
      \[
        \exists j \;.\; \sigma' = 0^{j} 1^{j}
      \] 
      questo implica che:
      \[
        \sigma = 0 \sigma' 1 = 0 0^{j} 1^{j} 1 = 0^{j+1} 1^{j+1} \in L
      \] 
  \end{itemize}
\end{example}

\subsection{Alberi di derivazione}
Data una grammatica context free \( G = \left< V, T, P, S \right> \), un albero di
derivazione (parse tree) per \( G \) ha le seguenti caratteristiche:
\begin{enumerate}
  \item Ogni nodo ha un'etichetta che è un simbolo di \( V \cup T \cup \left\{ \varepsilon \right\} \)
  \item L'etichetta della radice è un simbolo in \( V \) 
  \item Ogni nodo interno interno (non foglia) ha etichetta in \( V \)
  \item Se un nodo \( n \) è etichettato con \( A \in V \) e i nodi figli
    \( n_1, n_2, \ldots, n_k \) sono etichettati con \( X_1, X_2, \ldots, X_k \in V \cup T \cup
    \left\{ \varepsilon \right\} \) allora esiste una produzione che genera un arco per
    ogni simbolo figlio:
    \[
      A \to X_1 X_2 \ldots X_k \in P
    \]
    \label{03-11-D1}

  \item Se un nodo ha etichetta \( \varepsilon \) allora è una foglia ed è l'unico figlio
    del padre
    \label{03-11-D2}
\end{enumerate}

\begin{example}
  Consideriamo la grammatica
  \[
    E \to 0 \;|\; 1 \;|\; (E \text{ and } E) \;|\; (E \text{ or } E) \;|\; (\text{not } E)
  \]
  Consideriamo la derivazione:
  \[
    E \Rightarrow^* \left( \left( 0 \text{ or } 1 \right)  \text{ and } \left( \text{not } 0 \right)  \right) 
  \] 
  L'albero di derivazione è il seguente:
  \label{03-11-D3}
\end{example}

\begin{theorem}
  Sia \( G = \left< V, T, P, S \right> \) una grammatica context free, allora:
  \[
    S \Rightarrow^* \alpha \in T^*
  \] 
  se e solo se esiste un albero di derivazione con radice etichettata \( S \) e
  foglie etichettate (leggendo da sinistra verso destra) con i simboli di \( \alpha \).
\end{theorem}

\subsection{Ambiguità delle grammatiche}
\begin{example}
  Consideriamo una grammatica per le espressioni aritmetiche senza parentesi:
  \[
    E \to E + E \;|\; E * E \;|\; 0 \;|\; 1 \;|\; 2
  \] 
  Consideriamo la stringa:
  \[
    2 * 0 + 1
  \]
  Si hanno due casi:
  \begin{itemize}
    \item Si esegue prima la moltiplicazione:
      \[
        2 * 0 + 1 = (2 * 0) + 1 = 1
      \] 
    \item Si esegue prima l'addizione:
      \[
        2 * 0 + 1 = 2 * (0 + 1) = 2
      \]
  \end{itemize}
  Notiamo che si ottengono due risultati diversi, quindi la grammatica è ambigua
  perchè la stessa stringa può essere generata da due alberi di derivazione distinti:
  \begin{itemize}
    \item Albero di derivazione con precedenza alla moltiplicazione
      \label{03-11-D4} 
    \item Albero di derivazione con precedenza all'addizione
      \label{03-11-D5}
  \end{itemize}
\end{example}

\begin{definition}[Ambiguità]
  Una grammatica è \textbf{ambigua} se esiste almeno una parola \( \alpha \) con più
  di un albero di derivazione con radice \( S \).

  Il linguaggio generato da una grammatica ambigua è detto \textbf{inerentemente ambiguo}.
\end{definition}

\subsection{Forma normale}
Le forme normali sono grammatiche le cui produzioni rispettano vincoli di "forma"
specifici. Ci sono due forme normali importanti per le grammatiche context free:
\begin{itemize}
  \item \textbf{Forma normale di Chomsky}: Tutte le produzioni hanno la forma:
    \[
      A \to a \quad \vee A \to BC
    \] 
    dove:
    \[
      A, B, C \in V \quad a \in T
    \] 
    Questa forma genera sempre un albero binario (ogni nodo ha al massimo due figli).

  \item \textbf{Forma normale di Greibach}: Tutte le produzioni hanno la forma:
    \[
      A \to a \alpha
    \] 
    dove:
    \[
      A \in V \quad a \in T \quad \alpha \in V^*
    \]
    Rappresenta l'automa a pila in cui \( \alpha \) è il contenuto della pila e
    \( a \) è il simbolo da aggiungere.
\end{itemize}


\subsection{Minimizzazione delle grammatiche context free}
Ogni grammatica context free può essere riscritta in modo tale che:
\begin{enumerate}
  \item \textbf{Eliminazione dei simboli inutili}: Ogni simbolo non terminale genera
    simboli terminali e ogni simbolo terminale è generato da almeno un simbolo non
    terminale

  \item \textbf{Eliminazione delle produzioni unitarie}: Nessuna produzione è della forma:
    \[
      A \to B \quad A, B \in V
    \] 
    perchè \( A \) è generabile da \( B \) e \( B \) è generabile da \( A \).

  \item \textbf{Eliminazione della \( \varepsilon \)-produzione}: Se
    \( \varepsilon \notin L \) allora \textbf{non} ci deve essere la produzione:
    \[
      A \to \varepsilon \quad A \in V
    \] 
\end{enumerate}

\begin{definition}
  Quando la grammatica è senza simboli inutili, senza produzioni unitarie e senza
  \( \varepsilon \)-produzioni, si può trasformare in forma normale di Chomsky.
\end{definition}

\subsubsection{Eliminazione dei simboli inutili}
\( x \in V \cup T \) è utile se esiste una derivazione
\[
  S \Rightarrow^* \underbrace{\alpha x \beta \Rightarrow^* w \in T^*}_{1.a}
\] 
\begin{theorem}
  Per ogni grammatica context free \( \forall G = \left< V, T, P, S \right> \) esiste
  una grammatica equivalente \( G' = \left< V', T, P', S \right> \) senza simboli
  inutili:
  \[
    \forall G \;.\; \exists G' \;.\; L(G) = L(G') \text{ e } G' \text{ non ha simboli inutili}
  \] 
  \( G' \) si ottiene applicando in sequenza le due trasformazioni viste
  (\ref{eliminazione_simboli_inutili_1}, \ref{eliminazione_simboli_inutili_2}) (nell'ordine visto)
\end{theorem}

\begin{enumerate}
  \item \label{eliminazione_simboli_inutili_1}
    Eliminiamo i simboli che non generano terminali:
    \begin{example}
      Consideriamo le produzioni \( S \to \alpha A B \) e \( A \to A \).
      Il simbolo \( A \) è inutile perchè non genera mai simboli terminali.
    \end{example}

    \textbf{Metodo algoritmico per eliminare i simboli che non generano terminali}:
    \begin{enumerate}
      \item Dopo la trasformazione 
        \[
          \forall A \in V \;.\; \exists w \in T^* \;.\;
          A \Rightarrow^* w
        \] 
        Allora \( L(G) = \varnothing \) tutti i simboli sono inutili

      \item Prendiamo per ipotesi che \( L(G) \neq 0 \), definiamo:
        \[
          \Gamma(W) = \left\{ A \in V \;\left|\; \exists \alpha \in (T \cup W)^* \;.\;
        A \to \alpha \in P \right. \right\}
        \] 
        \[
          \begin{cases}
            \Gamma^0(W) = W\\
            \Gamma^{i+1}(W) = \Gamma \left( \Gamma^i(W) \right) 
          \end{cases}
        \] 
        \begin{example}
          Consideriamo le seguenti produzioni:
          \[
          \begin{aligned}
            A &\to A\\
            S &\to aS \;|\; B\\
            B &\to b
          \end{aligned}
          \] 
          Si parte con \( W = \varnothing \) :
          \begin{itemize}
            \item \( \Gamma^0(\varnothing) = \left\{ A \in V \;|\; \exists
              \alpha \in T^* \;.\; A \to \alpha \in P \right\} = \varnothing \)

            \item \( \Gamma^1(\varnothing) = \Gamma(\varnothing) =
              \left\{ B \right\} \) perchè \( B \to b \in P \)

            \item \( \Gamma^2(\varnothing) = \Gamma(\left\{ B \right\}) =
              \left\{ A \in V \;\left|\; \exists \alpha \in (T \cup \left\{ B \right\})^* \;.\;
            A \to \alpha \right. \right\}\)\\ \( = \left\{ B, S \right\} \)

          \item \( \Gamma^3(\varnothing) = \Gamma(\left\{ B, S \right\}) \)\\
            \( = \left\{ A \in V \;\left|\; \exists \alpha \in (T \cup \left\{ B, S \right\})^* \;.\;
          A \to \alpha \right. \right\}\) \\ \( = \left\{ B, S \right\} \)
          \\
          Si è raggiunto il \textbf{punto fisso}, cioè non si ottengono più simboli
          nuovi.
          \end{itemize}
          Alla fine si ottiene l'insieme di tutti i simboli utili, cioè quelli che generano
          simboli terminali. Tutti gli altri (e le corrispondenti produzioni) vengono
          eliminati.

          \vspace{1em}
          \noindent
          Quindi \( S \to aS \;|\; B \;\; B \to b \) è equivalente ma senza simboli
          inutili che non generano nulla
        \end{example}
    \end{enumerate}

    \item \label{eliminazione_simboli_inutili_2}
      Eliminiamo i simboli non raggiungibili da \( S \):
      \[
        \Gamma(W) = \left\{ X \in V \cup T \;\left|\; \exists A \in W \;.\;
      A \to \alpha X \beta \in P \right. \right\} \cup S
      \] 
      Quindi ad esempio:
      \[
        \Gamma(\varnothing) = \left\{ S \right\}
      \] 
      Cioè si aggiunge ciò che possiamo raggiungere da \( S \) in un certo numero di passi.

      \begin{example}
        Consideriamo le seguenti produzioni:
        \[
        \begin{aligned}
          S \to a \;|\; bC\\
          C \to d \;|\;dE\\
          E \to \varepsilon\\
          F \to f
        \end{aligned}
        \] 
        Si parte con \( W = \varnothing \) :
        \begin{itemize}
          \item \( \Gamma(\varnothing) = \left\{ S \right\} \)

          \item \( \Gamma(\left\{ S \right\}) = \left\{ X \in V \cup T \;\left|\;
          S \to \alpha X \beta \in P \right. \right\} \cup \left\{ S \right\} =
          \left\{ S, a, b, C \right\} \)

          \item \( \Gamma(\left\{ S, a, b, C \right\}) = \left\{ X \in V \cup T \;\left|\;
          C \to \alpha X \beta \in P \vee S \to \alpha X \beta \in P \right. \right\}
          = \left\{ S, a, b, C, d, E \right\} \)

        \item \( \Gamma(\left\{ S, a, b, C, d, E \right\}) = \left\{ S, a, b, C, d, E, \varepsilon \right\} \)
        \end{itemize}
        Quindi \( F \) è un simbolo non raggiungibile da \( S \) e quindi è inutile.
      \end{example}
\end{enumerate}

\subsubsection{Eliminazione delle produzioni unitarie}
L'idea è quella di sostituire le produzioni unitarie con le produzioni
dello stesso simbolo non terminale a cui puntano. Ad esempio se abbiamo
\( S \to A \to 0A0 \) si può compattare \( S \) come:
\(
  S \to 0A0
\) 
\begin{example}
  Consideriamo la seguente grammatica:
  \[
    \begin{aligned}
      S &\to \varepsilon  \;|\; A \;|\; B\\
      A &\to  0\;|\; 0A0 \;|\; B \;|\; 00 \\
      B &\to 1 \;|\; 1B1 \;|\; A\;|\; 11
    \end{aligned}
  \] 
  Sostituiamo in tutte le produzioni quello che è producibile dal non terminale a destra:
  \[
    \begin{aligned}
      S \to \varepsilon \;|\; \underbrace{0 \;|\; 0A0 \;|\; 00}_{A} \;|\; \underbrace{1 \;|\; 1B1 \;|\; 11}_{B}\\
      A \to 0 \;|\; 0A0 \;|\; 00 \;|\; \underbrace{1 \;|\; 1B1 \;|\; 11}_{B}\\
      B \to 1 \;|\; 1B1 \;|\; 11 \;|\; \underbrace{0 \;|\; 0A0 \;|\; 00}_{A}
    \end{aligned}
  \] 
\end{example}

\subsubsection{Eliminazione delle \texorpdfstring{\( \varepsilon \)}{epsilon}-produzioni}
L'idea è quella di sostituire la transizione con tutto quello che può generare
\( \varepsilon  \) ad esempio:
\begin{example}
  Consideriamo le produzioni:
  \[
    \begin{aligned}
      S &\to \varepsilon  \;|\; A \;|\; B\\
      A &\to  0\;|\; 0A0 \;|\; B \\
      B &\to 1 \;|\; 1B1 \;|\; A\;|\; \varepsilon 
    \end{aligned}
  \] 
  Siccome sia \( S \) che \( B \) vanno in \( \varepsilon  \) si possono sostituire con:
  \[
    B \to S
  \] 
  Anche \( A \) con un certo numero di derivazioni arriva in \( \varepsilon  \), quindi
  tutti i simboli non terminali che in un certo numero di passi generano \( \varepsilon  \)
  sono:
  \[
    \left\{ S, A, B \right\}
  \] 
  Le nuove produzioni diventano:
  \[
    \begin{aligned}
      S &\to \varepsilon \;|\; A \;|\; B \\
      A &\to 0 \;|\; 0A0 \;|\; 00 \;|\; B \\
      B &\to 1 \;|\; 1B1 \;|\; 11 \;|\; A
    \end{aligned}
  \] 
  Eliminando \( B \to \varepsilon  \) le stringhe \( 00 \) e \( 11 \) non sarebbero più
  generabili e quindi si aggiungono

  \vspace{1em}
  \noindent
  In questo modo si sono eliminate tutte le \( \varepsilon\)-produzioni, tranne
  \( S \to \varepsilon  \) se \( \varepsilon \in L \).
\end{example}

\subsection{Trasformazione di una grammatica in forma normale di Chomsky}
Quando una grammatica non ha simboli inutili, non ha produzioni unitarie e
non ha \( \varepsilon \)-produzioni, si può trasformare in forma normale di
Chomsky, cioè tutte le produzioni hanno la forma:
\[
  A \to a \in T \quad \vee \quad A \to BC \quad B, C \in V
\] 
\begin{example}
  Consideriamo la seguente grammatica:
  \[
  \begin{aligned}
    S &\to \varepsilon  \\ 
    S &\to 0 \;|\; 1 \\
    S &\to 00 \;|\; 0S0 \;|\; 11 \;|\; 1S1\\
  \end{aligned}
  \] 
  Trasformiamo le sequenze più lunghe di 1 simbolo terminale in simboli non terminali:
  \[
  \begin{aligned}
    S &\to \varepsilon  \\ 
    S &\to 0 \;|\; 1 \\
    S &\to V_1V_1 \;|\; V_1 S V_1 \;|\; V_2 V_2 \;|\; V_2 S V_2\\
    V_1 &\to 0\\
    V_2 &\to 1
  \end{aligned}
  \] 
  Trasformiamo le sequenze più lunghe di 2 simboli non terminali in un singolo simbolo
  non terminale:
  \[
  \begin{aligned}
    S &\to \varepsilon  \\ 
    S &\to 0 \;|\; 1 \\
    S &\to V_1V_1 \;|\; V_3 V_1 \;|\; V_2 V_2 \;|\; V_4 V_2\\
    V_1 &\to 0\\
    V_2 &\to 1\\
    V_3 &\to V_1 S\\
    V_4 &\to V_2 S
  \end{aligned}
  \] 
  Ora tutte le produzioni sono in forma normale di Chomsky.
\end{example}
Questa forma è semplicemente una forma canonica facilmente rappresentaible,
non una forma più semplice.

\subsection{Pumping lemma per i linguaggi context free}
Il pumping lemma è una \textbf{condizione necessaria} per essere un linguaggio context free:
\[
  L \text{ è CF } \implies \text{ vale } \Pi \text{ condizione del pumping lemma}
\] 
\begin{theorem}[Pumping lemma per i linguaggi context free]
  Sia \( L \) un linguaggio context free. Allora esiste una costante
  \( k \) tale che per ogni stringa \( z \in L \) con
  \( \left| \sigma \right| \ge k \) esiste una suddivisione di \( z \)
  in cinque sottostringhe:
  \[
    \exists k \in \mathbb{N} \;.\; \forall z \in L \;.\; \left| z \right| \ge k \implies
    \exists u, v, w, x, y = z \vee
    \begin{cases}
      \left| vwx \right| \le  k \\
      \left| vx \right| \ge 0 \\
    \end{cases}
  \] 
  \[
    \;.\; \forall i \in \mathbb{N} \;.\; u v^i w x^i y \in L
  \] 
  \label{04-11-D1}
\end{theorem}

\subsubsection{Dimostrazione (grafica)}
Consideriamo un linguaggio context free \( L \subseteq T^* \), allora esiste
una grammatica tale che \( L = L(G) \):
\[
  L \subseteq T^* \implies \exists G = \left< V, T, P, S \right> \text{ CF } \;.\; L = L(G)
\] 
Supponiamo, senza perdere generalità, che \( G \) sia in forma normale di Chomsky.
Definiamo \( n \) come la dimensione di \( V \), cioè il numero di simboli non terminali in
\( G \):
\[
  n = \left| V \right|
\] 
Poichè la grammatica è in forma normale di Chomsky questo implica che tutti gli alberi
di derivazione in \( G \) sono binari, cioè ogni nodi ha due figli non terminali oppure una foglia
terminale.
\label{04-11-D2}
L'altezza dell'albero è \( h \) e di conseguenza il numero di elementi ad una certa
altezza è \( 2^{h-1} \).

\vspace{1em}
\noindent
Definiamo \( k = 2^n \) e prendiamo una stringa \( z \in L \) di lunghezza maggiore o
uguale a \( k \):
\[
  z \in L \;.\; \left| z \right| \ge k
\] 
Se le foglie sono maggiori o uguali a \( 2^n \) allora l'altezza dell'albero
è \( \ge n+1 \). Il numero di \textbf{archi} che collegano simboli non terminali sono \( \ge n \).
Di conseguenza il numero di nodi interni (non terminali) nel cammino sono \( n+1 \) 
\label{04-11-D3}
Siccome in questa grammatica il numero di nodi non terminali è \( n \), allora almeno
una etichetta non terminale è ripetuta nel cammino due volte. (Ci sono almeno due nodi
interni che hanno la stessa etichetta)
\label{04-11-D4}
Supponiamo di prendere il nodo \( A \), più vicino alle foglie, che si ripete (implica che
dalla prima occorrenza di \( A \) fino alle foglie non ci sono ripetizioni). Consideriamo
il sottoalbero radicato in questo nodo e quello radicato nel secondo nodo con la stessa etichetta.
Osserviamo che la stringa è stata divisa in cinque parti \( u, v, w, x, y \):
\label{04-11-D5}
Valgono i seguenti vincoli:
\begin{itemize}
  \item \( \left| vx \right| \ge 0 \): almeno un simbolo deve essere generato in \( v \) 
    o \( x \)

  \item \( \left| vwx \right| \le k \): se dalla prima occorrenza di \( A \) fino
    alle foglie non ci sono ripetizioni, allora attraversiamo massimo \( n = \left| V \right|  \) 
    nodi e quindi non si generano più di \( 2^n = k \) simboli terminali
\end{itemize}
\label{04-11-D6}

\subsubsection{Dimostrare che un linguaggio non è context free}
Il pumping lemma può essere usato per dimostrare che un linguaggio è context free:
\[
  L \text{ è CF } \implies \text{ vale } \Pi \text{ condizione del pumping lemma}
\] 
Si può negare il pumping lemma per ottenere una condizione \textbf{sufficiente} a
dimostrare che un linguaggio non è context free:
\[
  \neg \Pi \text{ condizione del pumping lemma } \implies L \text{ non è CF}
\]
dove \( \Pi  \) è:
\[
  \Pi: \exists k \in \mathbb{N} \;.\; \forall z \in L \;.\; \left| z \right| \ge k
  \;.\; \exists u, v, w, x, y = z \vee
  \begin{cases}
    \left| vwx \right| \le  k \\
    \left| vx \right| \ge 0 \\
  \end{cases}
\] 
\[
    \;.\; \forall i \in \mathbb{N} \;.\; u v^i w x^i y \in L
\] 
La negazione di \( \Pi  \) è:
\[
  \neg \Pi: \forall k \in \mathbb{N} \;.\; \exists z \in L \;.\; \left| z \right| \ge k
  \;.\; \forall u, v, w, x, y = z \vee
  \begin{cases}
    \left| vwx \right| \le  k \\
    \left| vx \right| \ge 0 \\
  \end{cases}
\] 
\[
    \;.\; \exists i \in \mathbb{N} \;.\; u v^i w x^i y \notin L
\]
\label{04-11-D7}

\begin{example}
  Consideriamo il seguente linguaggio:
  \[
    L = \left\{ a^n b^n c^n \;\left|\; n \in \mathbb{N} \right. \right\}
  \] 
  Dimostriamo che \( L \) non è context free usando la negazione del pumping lemma.

  Prendiamo \( n = k \) siccome \( n \) non ha alcun vincolo e scegliamo la stringa:
  \[
    z = a^k b^k c^k \in L
  \] 
  Tutte le possibili suddivisioni della stringa sono le seguenti:
  \label{04-11-D8}
  Le suddivisioni a cavallo di almeno due gtuppi sono tali che \( \forall i > 1 \) cambiamo
  la struttura della stringa e quindi esce da \( L \) di conseguenza non le consideriamo.
  Bisognerebbe dimostrare tutte le suddivisioni, ma siccome le dimostrazioni per alcune
  sarebbero uguali non si ripetono.

  \vspace{1em}
  \noindent
  Le condizioni di appartenenza a \( L \) sono:
  \[
    a^i b^j c^l \in L \iff i = j = l
  \] 

  \begin{enumerate}
    \item Se la suddivisione è \( z = a^k b^k c^k \quad v,x \in a^k \):
      \[
        z_i = a^{k + (i-1) \left| v x \right|} b^k c^k
      \] 
      \begin{itemize}
        \item Consideriamo \( i = 2 \):
          \[
            z_2 = a^{k + \left| v x \right|} b^k c^k
          \]
          verifichiamo se appartiene a \( L \):
          \[
            z_2 \in L \iff
            \begin{cases}
              k + \left| v x \right| = k \iff \left| v x \right| = 0 \text{ (falso)}\\
              k = k \text{ (ovvio)}
            \end{cases}
          \] 
          Poichè per costruzione \( \left| v x \right| \ge 0 \) allora \( z_2 \notin L \)
      \end{itemize}

    \item Se la suddivisione è \( v \in a^k \quad x \in b^k \):
      \[
        z_i = a^{k + (i-1) \left| v \right|} b^{k + (i-1) \left| x \right|} c^k
      \]

      \begin{itemize}
        \item Consideriamo \( i = 2 \):
          \[
            z_2 = a^{k + \left| v \right|} b^{k + \left| x \right|} c^k
          \] 
          verifichiamo se appartiene a \( L \):
          \[
            \begin{aligned}
              z_2 \in L &\iff k + \left| v \right| = k + \left| x \right| = k\\
                        &\iff \left| v \right| = \left| x \right| = 0 \implies \left| vx \right| = 0
              \text{ (falso)}
            \end{aligned}
          \]
          poiche \( \left| vx \right| > 0 \) allora \( z_2 \notin L \)
      \end{itemize}
  \end{enumerate}
  Abbiamo dimostrato che per ogni suddivisione di \( z \) esiste un \( i \)
  tale che \( z_i \notin L \), quindi \( L \) non è context free.
\end{example}

\end{document}
