\documentclass[a4paper]{article}
\usepackage{import}
\input{../../../preamble.sty}

\begin{document}

\input{title.tex}

\tableofcontents
\pagebreak

\section{Introduzione}
Un'algoritmo è una sequenza \textbf{finita} di \textbf{istruzioni} volta a risolvere un problema.
Per implementarlo nel pratico si scrive un \textbf{programma}, cioè l'applicazione di
un linguaggio di programmazione, oppure si può descrivere in modo informale
attraverso del \textbf{pseudocodice} che non lo implementa in modo preciso,
ma spiega i passi per farlo.
\\
Ogni algoritmo può essere implementato in modi diversi, sta al programmatore
capire qual'è l'opzione migliore e scegliere in base alle proprie necessità.

\subsection{Confronto tra algoritmi}
Ogni algoritmo si può confrontare con gli altri in base a tanti fattori, come:
\begin{itemize}
  \item \textbf{Complessità}: quanto ci vuole ad eseguire l'algoritmo
  \item \textbf{Memoria}: quanto spazio in memoria occupa l'algoritmo
\end{itemize}

\subsection{Rappresentazione dei dati}
Per implementare un algoritmo bisogna riuscire a strutturare i dati in maniera tale
da riuscire a manipolarli in modo efficiente.

\section{Calcolo della complessità}
La complessità di un algoritmo mette in relazione il numero di istruzioni da eseguire
con la dimensione del problema, e quindi è una funzione che dipende dalla dimensione
del problema.

\vspace{1em}
\noindent
La \textbf{dimensione del problema} è un insieme di oggetti adeguato a dare un idea
chiara di quanto è grande il problema da risolvere, ma sta a noi decidere come
misurare il problema.

\noindent
Ad esempio una matrice è più comoda da misurare come il numero di righe e il numero
di colonne, al posto di misurarla come il numero di elementi totali.

\vspace{1em}
\noindent
La complessità di solito si calcola come il \textbf{caso peggiore}, cioè il
limite superiore di esecuzione dell'algoritmo.

\subsection{Linguaggi di programmazione}
Ogni linguaggio di programmazione è formato da diversi blocchi:
\begin{enumerate}
  \item \textbf{Blocco iterativo}: un tipico blocco di codice eseguito sequenzialmente
    e tipicamente finisce con un punto e virgola.
  \item \textbf{Blocco condizionale}: un blocco di codice che viene eseguito solo
    se una condizione è vera.
  \item \textbf{Blocco iterativo}: un blocco di codice che viene eseguito
    ripetutamente finché una condizione è vera.
\end{enumerate}

\noindent
Questi sono i blocchi base della programmazione e se riusciamo a calcolare
la complessità di ognuno di questi blocchi possiamo calcolare più facilmente
la complessità di un intero algoritmo.

\subsubsection{Blocchi iterativi}
\[
  I_1 \;\; c_1(n)
\] 
\[
  I_2 \;\; c_2(n)
\] 
\[
  \vdots \;\;\;\;\;\; \vdots
\] 
\[
  I_l \;\; c_l(n)
\]
Se ogni blocco ha complessità \( c_1(n) \), allora la complessità totale è data
da:
\[
\sum_{i=1}^{l} c_i(n)
\] 

\subsubsection{Blocchi condizionali}
\[
  \text{IF cond} \;\; c_{cond}(n)
\] 
\[
  I_1 \quad \quad c_1(n)
\] 
\[
  \hspace{-1.75cm} \text{ELSE}
\] 
\[
  I_2 \quad \quad c_2(n)
\] 
La complessità totale è data da:
\[
  c(n) = c_{cond}(n) + \max(c_1(n), c_2(n))
\] 
A volte la condizione è un test sulla dimensione del problema e in quel caso si
può scrivere una complessità più precisa.

\subsubsection{Blocchi iterativi}
\[
  \text{WHILE cond} \;\; c_{cond}(n)
\] 
\[
  I \hspace{1.6cm} c_0(n)
\] 
Si cerca di trovare un limite superiore \( m \) al limite di iterazioni.

\vspace{1em}
\noindent
Di conseguenza la complessità totale è data da:
\[
  c_{cond}(n) + m(c_{cond}(n) + c_0(n))
\]

\subsection{Esempio}
\begin{figure}[H]
  \begin{example}
    Calcoliamo la complessità della moltiplicazione tra 2 matrici:
    \[
      A_{n \times m} \cdot B_{m \times l} = C_{n \times l}
    \] 
    L'algoritmo è il seguente:
    \begin{lstlisting}[language=Scala]

  for i <- 1 to n // n ( 5 ml + 4l + 2) + n + 1
    for j <- 1 to l // l (5m + 2 + 1) + 1 + l 
      c[i][j] <- 0
      for k <- 1 to m // (m + 1 + m(4))
        // 3 (moltiplicazione, somma e assegnamento)
        // 1 (incremento for) 
        c[i][j] += a[i][k] * b[k][j]
    \end{lstlisting}

    \noindent
    Partiamo calcolando la complessità del ciclo for più interno. Non ha
    senso tenere in considerazione tutti i dati, ma solo quelli rilevanti. In
    questo caso avremo:
    \[
      (m + 1 + m(4)) = 5m + 1
    \] 
    Questa complessità contiene informazioni poco rilevanti perchè possono far
    riferimento alla velocità della cpu e un millisecondo in più o in meno non cambia
    nulla se teniamo in considerazione solo l'incognita abbiamo:
    \[
      m
    \]
    Questo semplifica molto i calcoli, rendendo meno probabili gli errori. Siccome
    la complessità si calcola su numeri molto grandi, le costanti piccole prima o poi
    verranno tolte perchè poco influenti.

    \vspace{1em}
    \noindent
    La complessità totale alla fine sarebbe stata:
    \[
      5nml+4ml+2n+n+1
    \] 
    Ma ciò che ci interessa veramente è:
    \[
      5\color{red}nml\color{black}+4ml+2n+n+1
    \] 
    Se non consideriamo le costanti inutili, la complessità finale è:
    \[
      nml
    \]
    Nella maggior perte dei casi ci si concentra soltanto sull'ordine di grandezza
    della complessità, e non sulle costanti.
  \end{example}
\end{figure}

\subsection{Ordine di grandezza}
L'ordine di grandezza è una funzione che approssima la complessità di un algoritmo:
\[
f \in O(g)
\] 
\[
  \exists c > 0\; \exists \bar{n}\;\; \forall n \ge \bar{n}\;\; f(n) \le c g(n)
\] 
\begin{figure}[H]
  \centering
  \begin{tikzpicture}
    % axis
    \draw[->] (-0.2,0) -- (6,0) node[right] {$t$};
    \draw[->] (0,-0.2) -- (0,5) node[above] {$y$};

    \draw[red, domain=0:5, samples=100, smooth] plot ({\x},{exp(\x/3)}) node[right] {$cg$};
    \draw[blue, domain=0:5.3, samples=100, smooth] plot ({\x},{exp(\x/5) + sin(deg(5*\x))/2}) node[right] {f};

    \draw[dashed] (1.72,1.77) -- (1.75,0) node[below] {$\bar{n}$};
  \end{tikzpicture}
  \caption{Esempio di funzione \(f \in O(g)\)}
\end{figure}

\[
f \in \Omega(g)
\] 
\[
  \exists c > 0\; \exists \bar{n}\;\; \forall n \ge \bar{n}\;\; f(n) \ge cg(n)
\] 

\[
f \in \Theta(g)
\] 
\[
  f \in O(g) \land f \in \Omega(g)
\]

\vspace{1em}
\noindent
Per gli algoritmi:
\begin{figure}[H]
  \begin{definition}
    \[
      A \in O(f)
    \] 
    So che l'algoritmo \( A \) termina entro il tempo definito dalla funzione \( f \).
    Di conseguenza se un algoritmo termina entro un tempo \( f \) allora sicuramente
    termina entro un tempo \( g \) più grande. Ad esempio:
    \[
      A \in O(n) \Rightarrow A \in O(n^2)
    \] 
    Questa affermazione è \textbf{corretta}, ma \textbf{non accurata}.

    \vspace{1em}
    \[
      A \in \Omega(f)
    \] 
    Significa che esiste uno schema di input tale che se \( g(n) \) è il numero di
    passi necessari per risolvere l'istanza \( n \) allora:
    \[
      g \in \Omega(f)
    \] 
    Quindi l'algoritmo non termina in un tempo minore di \( f \).

    \vspace{1em}
    \noindent
    Calcolando la complessità si troverà lo schema di input tale che:
    \[
      g \in O(f)
    \]
    cioè il limite superiore di esecuzione dell'algoritmo.

    \noindent
    Successivamente ci si chiede se esistono algoritmi migliori e si 
    troverà lo schema di input tale che:
    \[
      g \in \Omega(f)
    \]
    cioè il limite inferiore di esecuzione dell'algoritmo.

    \noindent
    Se i due limiti coincidono allora:
    \[
      g \in \Theta(f)
    \]
    abbiamo trovato il tempo di esecuzione dell'algoritmo.
  \end{definition}
\end{figure}

\begin{figure}[H]
  \begin{theorem}[Teorema di Skolem]
    Se c'è una formula che vale coi quantificatori esistenziali, allora nel linguaggio
    si possono aggiungere delle costanti al posto delle costanti quantificate e assumere
    che la formula sia valida con quelle costanti.
  \end{theorem}
\end{figure}

\subsubsection{Esempi di dimostrazioni}
\begin{figure}[H]
  \begin{example}
    È vero che \( n \in O(2n) \)?

    \noindent
    Se prendiamo \( c = 1 \) e \( \bar{n} = 1 \) allora:
    \[
    n \le c2n
    \] 
    Quindi è vero
  \end{example}

\end{figure}
\begin{figure}[H]
\begin{example}
  È vero che \( 2n \in O(n) \)?

  \noindent
  Se prendiamo \( c = 2 \) e \( \bar{n} = 1 \) allora:
  \[
    2n \le 2n
  \] 
  Quindi è vero
\end{example}
\end{figure}
\begin{figure}[H]
  \begin{example}
    È vero che \( f \in O(g) \iff g \in \Omega(f) \)?

    \noindent
    Dimostro l'implicazione da entrambe le parti:
    \begin{itemize}
      \item \( \to \): Usando il teorema di Skolem:
        \[
          \forall n \ge \bar{n}\;\; f(n) \le cg(n)
        \] 
        Trasformo la disequazione:
        \[
          \forall n \ge \bar{n}\;\; \frac{f(n)}{c} \le g(n)
        \] 
        \[
          \forall n \ge \bar{n}\;\; g(n) \ge \frac{f(n)}{c}
        \] 
        \[
          \forall n \ge \bar{n}\;\; g(n) \ge \frac{1}{c} f(n) \quad \square
        \] 
        Se la definizione di \( \Omega(g) \) è:
        \[
          \exists c' > 0\; \exists \bar{n}'\;\; \forall n \ge \bar{n}'\;\; f(n) \ge c'g(n)
        \]
        sappiamo che:
        \[
          c' = \frac{1}{c}
        \] 
      \item \( \leftarrow \): Usando il teorema di Skolem:
        \[
          \forall n \ge \bar{n}'\;\; g(n) \ge c'f(n)
        \] 
        Trasformo la disequazione:
        \[
          \forall n \ge \bar{n}'\;\; \frac{g(n)}{c'}\ge f(n)
        \] 
        \[
          \forall n \ge \bar{n}'\;\; f(n) \le \frac{1}{c'} g(n) \quad \square
        \] 
    \end{itemize}
  \end{example}
\end{figure}

\begin{figure}[H]
  \begin{example}
    \[
    f_1 \in O(g) \; f_2 \in O(g) \Rightarrow f_1 + f_2 \in O(g)
    \] 
    Dimostrazione:

    Ipotesi
    \[
      \bar{n}_1 c_1\; \forall n > n_1 \quad f_1(n) \le c_1 g(n)
    \] 
    \[
      \bar{n}_1 c_2\; \forall n > n_2 \quad f_2(n) \le c_2 g(n)
    \] 

    \[
    f_1(n) + f_2(n) \le (c_1 + c_2)g(n) \quad \square
    \] 
    Quindi:
    \[
    c = (c_1 + c_2)
    \] 
    \[
      \bar{n} = \max(\bar{n}_1, \bar{n}_2)
    \] 
  \end{example}
\end{figure}

\begin{figure}[H]
  \begin{example}
    Se
    \[
    f_1 \in O(g_1) \; f_2 \in O(g_2)
    \] 
    è vero che:
    \[
    f_1 \cdot f_2 \in O(g_1 \cdot g_2)
    \] 
    Dimostrazione:

    Ipotesi
    \[
      \bar{n}_1 c_1\; \forall n > \bar{n}_1 \quad f_1(n) \le c_1 g_1(n)
    \] 
    \[
      \bar{n}_2 c_2\; \forall n > \bar{n}_2 \quad f_2(n) \le c_2 g_2(n)
    \]

    \[
      f_1(n) \cdot f_2(n) \le (c_1 \cdot c_2) (g_1(n) \cdot g_2(n)) \quad \square
    \] 
    Quindi:
    \[
      c = c_1 \cdot c_2
    \]
    \[
      \bar{n} = \max(\bar{n}_1, \bar{n}_2)
    \]
  \end{example}
\end{figure}

\subsection{Studio degli algoritmi}
Il problema dell'ordinamento si definisce stabilendo la relazione che deve esistere tra
\textbf{input} e \textbf{output} del sistema.
\begin{itemize}
  \item \textbf{Input}: Sequenza \( (a_1,\ldots,a_n) \) di oggetti su cui è definita una
    relazione di ordinamento, cioè l'unico modo per capire la differenza tra due oggetti
    è confrontarli.

  \item \textbf{Output}: Permutazione \( (a'_1,\ldots,a'_n) \) di \( (a_1,\ldots,a_n) \) 
    tale che:
    \[
    \forall i < j \;\; a'_i \le a'_j
    \] 
\end{itemize}
L'obiettivo è trovare un algoritmo che segua la relazione di ordinamento definita e risolva
il problema nel minor tempo possibile.

\subsubsection{Insertion sort}
Divide la sequenza in due parti:
\begin{itemize}
  \item \textbf{Parte ordinata}: Sequenza di elementi ordinati
  \item \textbf{Parte non ordinata}: Sequenza di elementi non ordinati
\end{itemize}
\begin{figure}[H]
  \centering
  \begin{tikzpicture}
    \draw[fill, fill opacity=0.2] (0,0) rectangle (3,1) node[midway, black, opacity=1]
      {Ordinato};
    \draw (3,1) rectangle (8,0) node[midway] {Non ordinato};

    \draw[<-] (3.5,0) -- ++(0,-0.5) node[below] {j};
    \draw[<-] (3,0) -- ++(0,-0.5) node[below] {i};
  \end{tikzpicture}
  \caption{Parte ordinata e non ordinata}
\end{figure}

\noindent
Pseudocodice:
\begin{lstlisting}[language=Scala]
insertion_sort(A)
  for j <- 2 to length[A] // A sinistra di j e tutto ordinato-
    key <- a[j]                //                            |
    i <- j - 1                 //                            | O(n)
    while i > 0 and a[i] > key // --                         |
      a[i + 1] <- a[i]         //  | O(n)                    |
      i--                      // --                     -----
    a[i + 1] <- key            
\end{lstlisting}

\noindent
La complessità di questo algoritmo è:
\[
  O(n^2)
\] 
Per capirlo è sufficiente guardare il numero di cicli nidificati e quante volte eseguono
il codice all'interno.

\vspace{1em}
\noindent
Se l'array è già ordinato la complessità è:
\[
\Omega(n)
\] 
Con l'input peggiore possibile la complessità è:
\[
\Omega(n^2)
\]
di conseguenza, visto che vale \( O(n^2) \) e \( \Omega(n^2) \) vale:
\[
\Theta(n^2)
\] 

Quanto spazio in memoria utilizza questo algoritmo?
\begin{itemize}
  \item Variabile j
  \item Variabile i
  \item Variabile key
\end{itemize}
A prescindere da quanto è grande l'array utilizzato, di conseguenza la memoria utilizzata
è costante.

\begin{itemize}
  \item \textbf{Ordinamento in loco}: se la quantità di memoria extra che deve usare 
    non dipende dalla dimensione del problema allora si dice che l'algoritmo è in loco.

  \item \textbf{Ordinamento non in loco}: se la quantità di memoria extra che deve usare
    dipende dalla dimensione del problema allora si dice che l'algoritmo è non in loco.

  \item \textbf{Stabilità}: La posizione relativa di elementi uguali non viene modificata
\end{itemize}
L'insertion sort ordina in loco ed è stabile.

\subsubsection{Fattoriale}
\begin{lstlisting}[language=Scala]
Fatt(n)
  if n = 0
    ret 1
  else 
    ret n * Fatt(n - 1)
\end{lstlisting}
L'argomento della funzione ci fa capire la complessità dell'algoritmo:
\[
  T(n) = \begin{cases}
    1 & \text{se } n = 0 \\
    T(n - 1) + 1 & \text{se } n > 0
  \end{cases}
\] 
Con problemi ricorsivi si avrà una complessità con funzioni definite ricorsivamente.
Questo si risolve induttivamente:
\[
  \begin{aligned}
    T(n) & = 1 + T(n-1)\\
         & = 1 + 1 + T(n-2)\\
         & = 1 + 1 + 1 + T(n-3)\\
         & = \underbrace{1 + 1 + \ldots + 1}_{i} + T(n-i)\\
  \end{aligned}
\] 
La condizione di uscita è: \( n-i = 0 \quad n = i \) 
\[
\begin{aligned}
         & = \underbrace{1 + 1 + \ldots + 1}_{n} + T(n-n)\\
         & = n + 1 = \Theta(n)
\end{aligned}
\] 
Questo si chiama passaggio iterativo.

\begin{example}
  \[
    T(n) = 2T\left(\floor*{\frac{n}{2}}\right) + n
  \] 
  Questa funzione si può riscrivere come:
  \[
  T(n) = \begin{cases}
    \text{Costante} & \text{se } n < a \\
    2T\left(\floor*{\frac{n}{2}}\right) + n & \text{se } n \ge a
  \end{cases}
  \] 

  \vspace{1em}
  \noindent
  Se la complessità fosse già data bisognerebbe soltanto verificare se è corretta.
  Usando il metodo di sostituzione:
  \[
    T(n) = cn \log n
  \] 
  sostituiamo nella funzione di partenza:
  \[
    \begin{aligned}
      T(n)  & = 2T\left(\floor*{\frac{n}{2}}\right) + n\\
            & \le 2c\left(\floor*{\frac{n}{2}}\right) \log \floor*{\frac{n}{2}} + n\\
            & \le \cancel{2} c \frac{n}{\cancel{2}} \log \frac{n}{2} + n\\
            & = cn \log n - cn \log 2 + n\\
            & \stackrel{?}{\le} cn \log n \quad \text{se } n- cn \log 2 \le 0\\
    \end{aligned}
  \] 
  \[
    c \ge \frac{n}{n \log 2} = \frac{1}{\log 2}
  \] 
  Il metodo di sostituzione dice che quando si arriva ad avere una disequazione
  corrispondente all'ipotesi, allora la soluzione è corretta se soddisfa una certa ipotesi.
\end{example}

\begin{example}
  \[
    T(n) = T\left(\floor*{\frac{n}{2}}\right) + T\left(\ceil*{\frac{n}{2}}\right) + 1 \quad \in O(n)
  \] 
  \[
  T(n) \le cn
  \] 
  \[
  \begin{aligned}
    T(n) & = T\left(\floor*{\frac{n}{2}}\right) + T\left(\ceil*{\frac{n}{2}}\right) + 1\\
         & \le c\left(\floor*{\frac{n}{2}}\right) + c\left(\ceil*{\frac{n}{2}}\right) + 1\\
         & = c \left( \left\lfloor \frac{n}{2} \right\rfloor + \left\lceil \frac{n}{2} \right\rceil  \right) + 1\\
         & = cn + 1 \stackrel{?}{\le} cn
  \end{aligned}
  \] 
  Il metodo utilizzato non funziona perchè rimane l'1 e non si può togliere in alcun modo.
  Per risolvere questo problema bisogna risolverne uno più forte:
  \[
  T(n) \le cn - b
  \] 
  \[
  \begin{aligned}
    T(n) & = T\left(\floor*{\frac{n}{2}}\right) + T\left(\ceil*{\frac{n}{2}}\right) + 1\\
         & \le c\left(\floor*{\frac{n}{2}}\right) -b + c\left(\ceil*{\frac{n}{2}}\right) -b + 1\\
         & = c \left( \left\lfloor \frac{n}{2} \right\rfloor + \left\lceil \frac{n}{2} \right\rceil  \right) - 2b + 1\\
         & = cn - 2b + 1 \stackrel{?}{\le} cn - b\\
         & = \underbrace{cn - b} + \underbrace{1 - b}_{\le 0} \le cn - b \quad \text{se } b \ge 1\\
  \end{aligned}
  \] 
  Se la proprietà vale per questo problema allora vale anche per il problema iniziale
  perchè è meno forte.
\end{example}

\begin{example}
  \[
    \begin{aligned}
      T(n) & = 3T \left( \left\lfloor \frac{n}{4} \right\rfloor \right) + n\\
           & = n + 3T \left( \left\lfloor \frac{n}{4} \right\rfloor \right)\\
           & = n + 3 \left( \left\lfloor \frac{n}{4} \right\rfloor + 3T 
           \left( \left\lfloor \frac{\left\lfloor \frac{n}{4} \right\rfloor}{4} \right\rfloor
           \right)  \right)\\
           & = n + 3 \left\lfloor \frac{n}{4} \right\rfloor + 3^2 T 
           \left( \left\lfloor \frac{n}{4^2} \right\rfloor \right)\\
           & \le n + 3 \left\lfloor \frac{n}{4} \right\rfloor + 3^2 
           \left( \left\lfloor \frac{n}{4^2} \right\rfloor + 3T \left( 
           \left\lfloor \frac{\left\lfloor \frac{n}{4^2} \right\rfloor}{4} \right\rfloor
           \right)  \right) \\
           & = n + 3 \left\lfloor \frac{n}{4} \right\rfloor + 3^2
           \left\lfloor \frac{n}{4^2} \right\rfloor + 3^3 T
           \left( \left\lfloor \frac{n}{4^3} \right\rfloor \right) \\
           & = n + 3 \left\lfloor \frac{n}{4} \right\rfloor + \ldots + 3^{i-1}
           \left\lfloor \frac{n}{4^{i-1}} \right\rfloor + 3^i T
           \left( \left\lfloor \frac{n}{4^i} \right\rfloor \right) 
    \end{aligned}
  \] 
  Per trovare il caso base poniamo l'argomento di T molto piccolo:
  \[
    \begin{aligned}
      \frac{n}{4^i} & < 1\\
      4^i & > n\\
      i & > \log_4 n
    \end{aligned}
  \] 
  L'equazione diventa:
  \[
    \begin{aligned}
      & \le n + 3 \left\lfloor \frac{n}{4} \right\rfloor + \ldots + 3^{\log_4 n - 1}
      \left\lfloor \frac{n}{4^{\log_4 n - 1}} \right\rfloor + 3^{\log_4 n} c\\
    \end{aligned}
  \] 
  Si può togliere l'approssimazione per difetto per ottenere un maggiorante:
  \[
  \begin{aligned}
    & \le n \left( 1 + \frac{3}{4} + \left( \frac{3}{4} \right)^2 + \ldots +
    \left( \frac{3}{4} \right)^{\log_4 n-1} \right) + 3^{\log_4 n} c\\
    & \le n \left( \sum_{i=0}^{\infty} \left( \frac{3}{4} \right)^i \right) + c 3^{\log_4 n}\\
  \end{aligned}
  \] 
  Per capire l'ordine di grandezza di \( 3^{\log_4 n} \) si può scrivere come:
  \[
    3^{\log_4 n} = n^{\left( \log_n 3^{\log_4 n} \right) } = n^{\log_4 n \cdot \log_n 3}
    = n^{\log_4 3}
  \] 
  Quindi la complessità è:
  \[
  \begin{aligned}
    & = O(n) + O(n^{\log_4 3})\\
  \end{aligned}
  \] 
  Si ha che una funzione è uguale al termine noto della funzione originale e l'altra
  che è uguale al logaritmo dei termini noti. Se usassimo delle variabili uscirebbe:
  \[
    \begin{aligned}
      T(n) & = a T \left( \left\lfloor \frac{n}{b} \right\rfloor \right) + f(n)\\
           & = O(f(n)) + O(n^{\log_b a})
    \end{aligned}
  \] 
\end{example}

\end{document}
