\documentclass[a4paper]{article}
\usepackage{import}
\input{../../../preamble.sty}

\begin{document}

\input{title.tex}

\tableofcontents
\pagebreak

\section{Introduzione}
Un'algoritmo è una sequenza \textbf{finita} di \textbf{istruzioni} volta a risolvere un problema.
Per implementarlo nel pratico si scrive un \textbf{programma}, cioè l'applicazione di
un linguaggio di programmazione, oppure si può descrivere in modo informale
attraverso del \textbf{pseudocodice} che non lo implementa in modo preciso,
ma spiega i passi per farlo.
\\
Ogni algoritmo può essere implementato in modi diversi, sta al programmatore
capire qual'è l'opzione migliore e scegliere in base alle proprie necessità.

\subsection{Confronto tra algoritmi}
Ogni algoritmo si può confrontare con gli altri in base a tanti fattori, come:
\begin{itemize}
  \item \textbf{Complessità}: quanto ci vuole ad eseguire l'algoritmo
  \item \textbf{Memoria}: quanto spazio in memoria occupa l'algoritmo
\end{itemize}

\subsection{Rappresentazione dei dati}
Per implementare un algoritmo bisogna riuscire a strutturare i dati in maniera tale
da riuscire a manipolarli in modo efficiente.

\section{Calcolo della complessità}
La complessità di un algoritmo mette in relazione il numero di istruzioni da eseguire
con la dimensione del problema, e quindi è una funzione che dipende dalla dimensione
del problema.

\vspace{1em}
\noindent
La \textbf{dimensione del problema} è un insieme di oggetti adeguato a dare un idea
chiara di quanto è grande il problema da risolvere, ma sta a noi decidere come
misurare il problema.

\noindent
Ad esempio una matrice è più comoda da misurare come il numero di righe e il numero
di colonne, al posto di misurarla come il numero di elementi totali.

\vspace{1em}
\noindent
La complessità di solito si calcola come il \textbf{caso peggiore}, cioè il
limite superiore di esecuzione dell'algoritmo.

\subsection{Linguaggi di programmazione}
Ogni linguaggio di programmazione è formato da diversi blocchi:
\begin{enumerate}
  \item \textbf{Blocco iterativo}: un tipico blocco di codice eseguito sequenzialmente
    e tipicamente finisce con un punto e virgola.
  \item \textbf{Blocco condizionale}: un blocco di codice che viene eseguito solo
    se una condizione è vera.
  \item \textbf{Blocco iterativo}: un blocco di codice che viene eseguito
    ripetutamente finché una condizione è vera.
\end{enumerate}

\noindent
Questi sono i blocchi base della programmazione e se riusciamo a calcolare
la complessità di ognuno di questi blocchi possiamo calcolare più facilmente
la complessità di un intero algoritmo.

\subsubsection{Blocchi iterativi}
\[
  I_1 \;\; c_1(n)
\] 
\[
  I_2 \;\; c_2(n)
\] 
\[
  \vdots \;\;\;\;\;\; \vdots
\] 
\[
  I_l \;\; c_l(n)
\]
Se ogni blocco ha complessità \( c_1(n) \), allora la complessità totale è data
da:
\[
\sum_{i=1}^{l} c_i(n)
\] 

\subsubsection{Blocchi condizionali}
\[
  \text{IF cond} \;\; c_{cond}(n)
\] 
\[
  I_1 \quad \quad c_1(n)
\] 
\[
  \hspace{-1.75cm} \text{ELSE}
\] 
\[
  I_2 \quad \quad c_2(n)
\] 
La complessità totale è data da:
\[
  c(n) = c_{cond}(n) + \max(c_1(n), c_2(n))
\] 
A volte la condizione è un test sulla dimensione del problema e in quel caso si
può scrivere una complessità più precisa.

\subsubsection{Blocchi iterativi}
\[
  \text{WHILE cond} \;\; c_{cond}(n)
\] 
\[
  I \hspace{1.6cm} c_0(n)
\] 
Si cerca di trovare un limite superiore \( m \) al limite di iterazioni.

\vspace{1em}
\noindent
Di conseguenza la complessità totale è data da:
\[
  c_{cond}(n) + m(c_{cond}(n) + c_0(n))
\]

\subsection{Esempio}
\begin{figure}[H]
  \begin{example}
    Calcoliamo la complessità della moltiplicazione tra 2 matrici:
    \[
      A_{n \times m} \cdot B_{m \times l} = C_{n \times l}
    \] 
    L'algoritmo è il seguente:
    \begin{lstlisting}[language=Scala]

  for i <- 1 to n // n ( 5 ml + 4l + 2) + n + 1
    for j <- 1 to l // l (5m + 2 + 1) + 1 + l 
      c[i][j] <- 0
      for k <- 1 to m // (m + 1 + m(4))
        // 3 (moltiplicazione, somma e assegnamento)
        // 1 (incremento for) 
        c[i][j] += a[i][k] * b[k][j]
    \end{lstlisting}

    \noindent
    Partiamo calcolando la complessità del ciclo for più interno. Non ha
    senso tenere in considerazione tutti i dati, ma solo quelli rilevanti. In
    questo caso avremo:
    \[
      (m + 1 + m(4)) = 5m + 1
    \] 
    Questa complessità contiene informazioni poco rilevanti perchè possono far
    riferimento alla velocità della cpu e un millisecondo in più o in meno non cambia
    nulla se teniamo in considerazione solo l'incognita abbiamo:
    \[
      m
    \]
    Questo semplifica molto i calcoli, rendendo meno probabili gli errori. Siccome
    la complessità si calcola su numeri molto grandi, le costanti piccole prima o poi
    verranno tolte perchè poco influenti.

    \vspace{1em}
    \noindent
    La complessità totale alla fine sarebbe stata:
    \[
      5nml+4ml+2n+n+1
    \] 
    Ma ciò che ci interessa veramente è:
    \[
      5\color{red}nml\color{black}+4ml+2n+n+1
    \] 
    Se non consideriamo le costanti inutili, la complessità finale è:
    \[
      nml
    \]
    Nella maggior perte dei casi ci si concentra soltanto sull'ordine di grandezza
    della complessità, e non sulle costanti.
  \end{example}
\end{figure}

\subsection{Ordine di grandezza}
L'ordine di grandezza è una funzione che approssima la complessità di un algoritmo:
\[
f \in O(g)
\] 
\[
  \exists c > 0\; \exists \bar{n}\;\; \forall n \ge \bar{n}\;\; f(n) \le c g(n)
\] 
\begin{figure}[H]
  \centering
  \begin{tikzpicture}
    % axis
    \draw[->] (-0.2,0) -- (6,0) node[right] {$t$};
    \draw[->] (0,-0.2) -- (0,5) node[above] {$y$};

    \draw[red, domain=0:5, samples=100, smooth] plot ({\x},{exp(\x/3)}) node[right] {$cg$};
    \draw[blue, domain=0:5.3, samples=100, smooth] plot ({\x},{exp(\x/5) + sin(deg(5*\x))/2}) node[right] {f};

    \draw[dashed] (1.72,1.77) -- (1.75,0) node[below] {$\bar{n}$};
  \end{tikzpicture}
  \caption{Esempio di funzione \(f \in O(g)\)}
\end{figure}

\[
f \in \Omega(g)
\] 
\[
  \exists c > 0\; \exists \bar{n}\;\; \forall n \ge \bar{n}\;\; f(n) \ge cg(n)
\] 

\[
f \in \Theta(g)
\] 
\[
  f \in O(g) \land f \in \Omega(g)
\]

\vspace{1em}
\noindent
Per gli algoritmi:
\begin{figure}[H]
  \begin{definition}
    \[
      A \in O(f)
    \] 
    So che l'algoritmo \( A \) termina entro il tempo definito dalla funzione \( f \).
    Di conseguenza se un algoritmo termina entro un tempo \( f \) allora sicuramente
    termina entro un tempo \( g \) più grande. Ad esempio:
    \[
      A \in O(n) \Rightarrow A \in O(n^2)
    \] 
    Questa affermazione è \textbf{corretta}, ma \textbf{non accurata}.

    \vspace{1em}
    \[
      A \in \Omega(f)
    \] 
    Significa che esiste uno schema di input tale che se \( g(n) \) è il numero di
    passi necessari per risolvere l'istanza \( n \) allora:
    \[
      g \in \Omega(f)
    \] 
    Quindi l'algoritmo non termina in un tempo minore di \( f \).

    \vspace{1em}
    \noindent
    Calcolando la complessità si troverà lo schema di input tale che:
    \[
      g \in O(f)
    \]
    cioè il limite superiore di esecuzione dell'algoritmo.

    \noindent
    Successivamente ci si chiede se esistono algoritmi migliori e si 
    troverà lo schema di input tale che:
    \[
      g \in \Omega(f)
    \]
    cioè il limite inferiore di esecuzione dell'algoritmo.

    \noindent
    Se i due limiti coincidono allora:
    \[
      g \in \Theta(f)
    \]
    abbiamo trovato il tempo di esecuzione dell'algoritmo.
  \end{definition}
\end{figure}

\begin{figure}[H]
  \begin{theorem}[Teorema di Skolem]
    Se c'è una formula che vale coi quantificatori esistenziali, allora nel linguaggio
    si possono aggiungere delle costanti al posto delle costanti quantificate e assumere
    che la formula sia valida con quelle costanti.
  \end{theorem}
\end{figure}

\subsubsection{Esempi di dimostrazioni}
\begin{figure}[H]
  \begin{example}
    È vero che \( n \in O(2n) \)?

    \noindent
    Se prendiamo \( c = 1 \) e \( \bar{n} = 1 \) allora:
    \[
    n \le c2n
    \] 
    Quindi è vero
  \end{example}

\end{figure}
\begin{figure}[H]
\begin{example}
  È vero che \( 2n \in O(n) \)?

  \noindent
  Se prendiamo \( c = 2 \) e \( \bar{n} = 1 \) allora:
  \[
    2n \le 2n
  \] 
  Quindi è vero
\end{example}
\end{figure}
\begin{figure}[H]
  \begin{example}
    È vero che \( f \in O(g) \iff g \in \Omega(f) \)?

    \noindent
    Dimostro l'implicazione da entrambe le parti:
    \begin{itemize}
      \item \( \to \): Usando il teorema di Skolem:
        \[
          \forall n \ge \bar{n}\;\; f(n) \le cg(n)
        \] 
        Trasformo la disequazione:
        \[
          \forall n \ge \bar{n}\;\; \frac{f(n)}{c} \le g(n)
        \] 
        \[
          \forall n \ge \bar{n}\;\; g(n) \ge \frac{f(n)}{c}
        \] 
        \[
          \forall n \ge \bar{n}\;\; g(n) \ge \frac{1}{c} f(n) \quad \square
        \] 
        Se la definizione di \( \Omega(g) \) è:
        \[
          \exists c' > 0\; \exists \bar{n}'\;\; \forall n \ge \bar{n}'\;\; f(n) \ge c'g(n)
        \]
        sappiamo che:
        \[
          c' = \frac{1}{c}
        \] 
      \item \( \leftarrow \): Usando il teorema di Skolem:
        \[
          \forall n \ge \bar{n}'\;\; g(n) \ge c'f(n)
        \] 
        Trasformo la disequazione:
        \[
          \forall n \ge \bar{n}'\;\; \frac{g(n)}{c'}\ge f(n)
        \] 
        \[
          \forall n \ge \bar{n}'\;\; f(n) \le \frac{1}{c'} g(n) \quad \square
        \] 
    \end{itemize}
  \end{example}
\end{figure}

\begin{figure}[H]
  \begin{example}
    \[
    f_1 \in O(g) \; f_2 \in O(g) \Rightarrow f_1 + f_2 \in O(g)
    \] 
    Dimostrazione:

    Ipotesi
    \[
      \bar{n}_1 c_1\; \forall n > n_1 \quad f_1(n) \le c_1 g(n)
    \] 
    \[
      \bar{n}_1 c_2\; \forall n > n_2 \quad f_2(n) \le c_2 g(n)
    \] 

    \[
    f_1(n) + f_2(n) \le (c_1 + c_2)g(n) \quad \square
    \] 
    Quindi:
    \[
    c = (c_1 + c_2)
    \] 
    \[
      \bar{n} = \max(\bar{n}_1, \bar{n}_2)
    \] 
  \end{example}
\end{figure}

\begin{figure}[H]
  \begin{example}
    Se
    \[
    f_1 \in O(g_1) \; f_2 \in O(g_2)
    \] 
    è vero che:
    \[
    f_1 \cdot f_2 \in O(g_1 \cdot g_2)
    \] 
    Dimostrazione:

    Ipotesi
    \[
      \bar{n}_1 c_1\; \forall n > \bar{n}_1 \quad f_1(n) \le c_1 g_1(n)
    \] 
    \[
      \bar{n}_2 c_2\; \forall n > \bar{n}_2 \quad f_2(n) \le c_2 g_2(n)
    \]

    \[
      f_1(n) \cdot f_2(n) \le (c_1 \cdot c_2) (g_1(n) \cdot g_2(n)) \quad \square
    \] 
    Quindi:
    \[
      c = c_1 \cdot c_2
    \]
    \[
      \bar{n} = \max(\bar{n}_1, \bar{n}_2)
    \]
  \end{example}
\end{figure}

\section{Studio degli algoritmi}
Il problema dell'ordinamento si definisce stabilendo la relazione che deve esistere tra
\textbf{input} e \textbf{output} del sistema.
\begin{itemize}
  \item \textbf{Input}: Sequenza \( (a_1,\ldots,a_n) \) di oggetti su cui è definita una
    relazione di ordinamento, cioè l'unico modo per capire la differenza tra due oggetti
    è confrontarli.

  \item \textbf{Output}: Permutazione \( (a'_1,\ldots,a'_n) \) di \( (a_1,\ldots,a_n) \) 
    tale che:
    \[
    \forall i < j \;\; a'_i \le a'_j
    \] 
\end{itemize}
L'obiettivo è trovare un algoritmo che segua la relazione di ordinamento definita e risolva
il problema nel minor tempo possibile.

\subsection{Insertion sort}
Divide la sequenza in due parti:
\begin{itemize}
  \item \textbf{Parte ordinata}: Sequenza di elementi ordinati
  \item \textbf{Parte non ordinata}: Sequenza di elementi non ordinati
\end{itemize}
\begin{figure}[H]
  \centering
  \begin{tikzpicture}
    \draw[fill, fill opacity=0.2] (0,0) rectangle (3,1) node[midway, black, opacity=1]
      {Ordinato};
    \draw (3,1) rectangle (8,0) node[midway] {Non ordinato};

    \draw[<-] (3.5,0) -- ++(0,-0.5) node[below] {j};
    \draw[<-] (3,0) -- ++(0,-0.5) node[below] {i};
  \end{tikzpicture}
  \caption{Parte ordinata e non ordinata}
\end{figure}

\noindent
Pseudocodice:
\begin{lstlisting}[language=Scala]
insertion_sort(A)
  for j <- 2 to length[A] // A sinistra di j e tutto ordinato-
    key <- A[j]                //                            |
    i <- j - 1                 //                            | O(n)
    while i > 0 and A[i] > key // --                         |
      A[i + 1] <- A[i]         //  | O(n)                    |
      i--                      // --                     -----
    A[i + 1] <- key            
\end{lstlisting}

\noindent
La complessità di questo algoritmo è:
\[
  O(n^2)
\] 
Per capirlo è sufficiente guardare il numero di cicli nidificati e quante volte eseguono
il codice all'interno.

\vspace{1em}
\noindent
Se l'array è già ordinato la complessità è:
\[
\Omega(n)
\] 
Con l'input peggiore possibile la complessità è:
\[
\Omega(n^2)
\]
di conseguenza, visto che vale \( O(n^2) \) e \( \Omega(n^2) \) vale:
\[
\Theta(n^2)
\] 

Quanto spazio in memoria utilizza questo algoritmo?
\begin{itemize}
  \item Variabile j
  \item Variabile i
  \item Variabile key
\end{itemize}
A prescindere da quanto è grande l'array utilizzato, di conseguenza la memoria utilizzata
è costante.

\begin{itemize}
  \item \textbf{Ordinamento in loco}: se la quantità di memoria extra che deve usare 
    non dipende dalla dimensione del problema allora si dice che l'algoritmo è in loco.

  \item \textbf{Ordinamento non in loco}: se la quantità di memoria extra che deve usare
    dipende dalla dimensione del problema allora si dice che l'algoritmo è non in loco.

  \item \textbf{Stabilità}: La posizione relativa di elementi uguali non viene modificata
\end{itemize}
L'insertion sort ordina in loco ed è stabile.

\subsection{Fattoriale}
\begin{lstlisting}[language=Scala]
Fatt(n)
  if n = 0
    ret 1
  else 
    ret n * Fatt(n - 1)
\end{lstlisting}
L'argomento della funzione ci fa capire la complessità dell'algoritmo:
\[
  T(n) = \begin{cases}
    1 & \text{se } n = 0 \\
    T(n - 1) + 1 & \text{se } n > 0
  \end{cases}
\] 
Con problemi ricorsivi si avrà una complessità con funzioni definite ricorsivamente.
Questo si risolve induttivamente:
\[
  \begin{aligned}
    T(n) & = 1 + T(n-1)\\
         & = 1 + 1 + T(n-2)\\
         & = 1 + 1 + 1 + T(n-3)\\
         & = \underbrace{1 + 1 + \ldots + 1}_{i} + T(n-i)\\
  \end{aligned}
\] 
La condizione di uscita è: \( n-i = 0 \quad n = i \) 
\[
\begin{aligned}
         & = \underbrace{1 + 1 + \ldots + 1}_{n} + T(n-n)\\
         & = n + 1 = \Theta(n)
\end{aligned}
\] 
Questo si chiama passaggio iterativo.

\begin{example}
  \[
    T(n) = 2T\left(\floor*{\frac{n}{2}}\right) + n
  \] 
  Questa funzione si può riscrivere come:
  \[
  T(n) = \begin{cases}
    \text{Costante} & \text{se } n < a \\
    2T\left(\floor*{\frac{n}{2}}\right) + n & \text{se } n \ge a
  \end{cases}
  \] 

  \vspace{1em}
  \noindent
  Se la complessità fosse già data bisognerebbe soltanto verificare se è corretta.
  Usando il metodo di sostituzione:
  \[
    T(n) = cn \log n
  \] 
  sostituiamo nella funzione di partenza:
  \[
    \begin{aligned}
      T(n)  & = 2T\left(\floor*{\frac{n}{2}}\right) + n\\
            & \le 2c\left(\floor*{\frac{n}{2}}\right) \log \floor*{\frac{n}{2}} + n\\
            & \le \cancel{2} c \frac{n}{\cancel{2}} \log \frac{n}{2} + n\\
            & = cn \log n - cn \log 2 + n\\
            & \stackrel{?}{\le} cn \log n \quad \text{se } n- cn \log 2 \le 0\\
    \end{aligned}
  \] 
  \[
    c \ge \frac{n}{n \log 2} = \frac{1}{\log 2}
  \] 
  Il metodo di sostituzione dice che quando si arriva ad avere una disequazione
  corrispondente all'ipotesi, allora la soluzione è corretta se soddisfa una certa ipotesi.
\end{example}

\begin{example}
  \[
    T(n) = T\left(\floor*{\frac{n}{2}}\right) + T\left(\ceil*{\frac{n}{2}}\right) + 1 \quad \in O(n)
  \] 
  \[
  T(n) \le cn
  \] 
  \[
  \begin{aligned}
    T(n) & = T\left(\floor*{\frac{n}{2}}\right) + T\left(\ceil*{\frac{n}{2}}\right) + 1\\
         & \le c\left(\floor*{\frac{n}{2}}\right) + c\left(\ceil*{\frac{n}{2}}\right) + 1\\
         & = c \left( \left\lfloor \frac{n}{2} \right\rfloor + \left\lceil \frac{n}{2} \right\rceil  \right) + 1\\
         & = cn + 1 \stackrel{?}{\le} cn
  \end{aligned}
  \] 
  Il metodo utilizzato non funziona perchè rimane l'1 e non si può togliere in alcun modo.
  Per risolvere questo problema bisogna risolverne uno più forte:
  \[
  T(n) \le cn - b
  \] 
  \[
  \begin{aligned}
    T(n) & = T\left(\floor*{\frac{n}{2}}\right) + T\left(\ceil*{\frac{n}{2}}\right) + 1\\
         & \le c\left(\floor*{\frac{n}{2}}\right) -b + c\left(\ceil*{\frac{n}{2}}\right) -b + 1\\
         & = c \left( \left\lfloor \frac{n}{2} \right\rfloor + \left\lceil \frac{n}{2} \right\rceil  \right) - 2b + 1\\
         & = cn - 2b + 1 \stackrel{?}{\le} cn - b\\
         & = \underbrace{cn - b} + \underbrace{1 - b}_{\le 0} \le cn - b \quad \text{se } b \ge 1\\
  \end{aligned}
  \] 
  Se la proprietà vale per questo problema allora vale anche per il problema iniziale
  perchè è meno forte.
\end{example}

\begin{example}
  \[
    \begin{aligned}
      T(n) & = 3T \left( \left\lfloor \frac{n}{4} \right\rfloor \right) + n\\
           & = n + 3T \left( \left\lfloor \frac{n}{4} \right\rfloor \right)\\
           & = n + 3 \left( \left\lfloor \frac{n}{4} \right\rfloor + 3T 
           \left( \left\lfloor \frac{\left\lfloor \frac{n}{4} \right\rfloor}{4} \right\rfloor
           \right)  \right)\\
           & = n + 3 \left\lfloor \frac{n}{4} \right\rfloor + 3^2 T 
           \left( \left\lfloor \frac{n}{4^2} \right\rfloor \right)\\
           & \le n + 3 \left\lfloor \frac{n}{4} \right\rfloor + 3^2 
           \left( \left\lfloor \frac{n}{4^2} \right\rfloor + 3T \left( 
           \left\lfloor \frac{\left\lfloor \frac{n}{4^2} \right\rfloor}{4} \right\rfloor
           \right)  \right) \\
           & = n + 3 \left\lfloor \frac{n}{4} \right\rfloor + 3^2
           \left\lfloor \frac{n}{4^2} \right\rfloor + 3^3 T
           \left( \left\lfloor \frac{n}{4^3} \right\rfloor \right) \\
           & = n + 3 \left\lfloor \frac{n}{4} \right\rfloor + \ldots + 3^{i-1}
           \left\lfloor \frac{n}{4^{i-1}} \right\rfloor + 3^i T
           \left( \left\lfloor \frac{n}{4^i} \right\rfloor \right) 
    \end{aligned}
  \] 
  Per trovare il caso base poniamo l'argomento di T molto piccolo:
  \[
    \begin{aligned}
      \frac{n}{4^i} & < 1\\
      4^i & > n\\
      i & > \log_4 n
    \end{aligned}
  \] 
  L'equazione diventa:
  \[
    \begin{aligned}
      & \le n + 3 \left\lfloor \frac{n}{4} \right\rfloor + \ldots + 3^{\log_4 n - 1}
      \left\lfloor \frac{n}{4^{\log_4 n - 1}} \right\rfloor + 3^{\log_4 n} c\\
    \end{aligned}
  \] 
  Si può togliere l'approssimazione per difetto per ottenere un maggiorante:
  \[
  \begin{aligned}
    & \le n \left( 1 + \frac{3}{4} + \left( \frac{3}{4} \right)^2 + \ldots +
    \left( \frac{3}{4} \right)^{\log_4 n-1} \right) + 3^{\log_4 n} c\\
    & \le n \left( \sum_{i=0}^{\infty} \left( \frac{3}{4} \right)^i \right) + c 3^{\log_4 n}\\
  \end{aligned}
  \] 
  Per capire l'ordine di grandezza di \( 3^{\log_4 n} \) si può scrivere come:
  \[
    3^{\log_4 n} = n^{\left( \log_n 3^{\log_4 n} \right) } = n^{\log_4 n \cdot \log_n 3}
    = n^{\log_4 3}
  \] 
  Quindi la complessità è:
  \[
  \begin{aligned}
    & = O(n) + O(n^{\log_4 3})\\
  \end{aligned}
  \] 
  Si ha che una funzione è uguale al termine noto della funzione originale e l'altra
  che è uguale al logaritmo dei termini noti. Se usassimo delle variabili uscirebbe:
  \[
    \begin{aligned}
      T(n) & = a T \left(  \frac{n}{b}  \right) + f(n)\\
           & = O(f(n)) + O(n^{\log_b a})
    \end{aligned}
  \] 
\end{example}

\subsection{Teorema dell'esperto}
\begin{theorem}[Teorema dell'esperto o Master theorem]
  Per un'equazione di ricorrenza del tipo:
  \[
    T(n) = a T \left(  \frac{n}{b}  \right) + f(n)\\
  \] 
  Si distinguono 3 casi:
  \begin{itemize}
    \item \( f(n) \in O(n^{\log_b a - \varepsilon}) \) allora \( T(n); \in \Theta(n^{\log_b a}) \)  
    \item \( f(n) \in \Theta(n^{\log_b a}) \) allora \( T(n) \in \Theta(f(n) \log n) \) 
    \item \( f(n) \in \Omega(n^{\log_b a + \varepsilon}) \) allora \( T(n) \in \Theta(f(n)) \) 
  \end{itemize}
\end{theorem}

\begin{example}
  \[
  T(n) = 9 T\left(\frac{n}{3}\right) + n
  \] 
  Applico il teorema dell'esperto:
  \[
  \begin{aligned}
    a & = 9\\
    b & = 3\\
    f(n) & = n\\
  \end{aligned}
  \] 
  \[
    n^{\log_b a} = n^{\log_3 9} = n^2
  \] 
  Verifico se esiste un \( \varepsilon \) tale che:
  \[
    n \in O(n^{2 - \varepsilon})
  \]
  prendo \( \varepsilon = -\frac{1}{2} \) e verifico:
  \[
    n \in O(n^2 \cdot n^{-\frac{1}{2}})
  \] 
  Quindi ho trovato il caso 1 del teorema dell'esperto.
  \[
    T(n) \in \Theta(n^2)
  \] 
\end{example}

\begin{example}
  \[
  T(n) = T \left( \frac{2n}{3} \right) + 1
  \] 
  Applico il teorema dell'esperto:
  \[
    \begin{aligned}
      a & = 1\\
      b & = \frac{3}{2}\\
      f(n) & = n^0\\
    \end{aligned}
  \]

  \[
    n^{\log_b a} = n^{\log_{\frac{3}{2}} 1} = n^0
  \] 
  Si nota che le due funzioni hanno lo stesso ordine di grandezza, quindi siamo nel secondo
  caso del teorema dell'esperto.
  \[
    T(n) \in \Theta(\log n)
  \] 
\end{example}

\begin{example}
  \[
    T(n) = 3T \left( \frac{n}{4} \right) + n \log n
  \] 
  Applico il teorema dell'esperto:
  \[
    \begin{aligned}
      a & = 3\\
      b & = 4\\
      f(n) & = n \log n\\
    \end{aligned}
  \]
  \[
    n^{\log_b a} = n^{\log_4 3}
  \]
  \[
    n \log n \in \Omega(n^{\log_4 3})
  \]
  Esiste un \( \varepsilon \) tale che:
  \[
    n \log n \in \Omega(n^{\log_4 3 + \varepsilon})
  \]
  perchè basta che sia compreso tra \( \log_4 3 \) e \( 1 \).
  
  \vspace{1em}
  \noindent
  Quindi siamo nel terzo caso del teorema dell'esperto.
  \[
    T(n) \in \Theta(n \log n)
  \]
\end{example}

\begin{example}
  \[
  T(n) = 2T \left( \frac{n}{2} \right) + n \log n
  \] 
  Applico il teorema dell'esperto:
  \[
    \begin{aligned}
      a & = 2\\
      b & = 2\\
      f(n) & = n \log n\\
    \end{aligned}
  \]
  \[
    n^{\log_b a} = n^{\log_2 2} = n
  \]
  \[
    n \log n \in \Omega(n)
  \]
  Verifico se esiiste un \( \varepsilon \), quindi divido per \( n \):
  \[
    \log n \in \Omega(n^{\varepsilon})
  \] 
  Quindi si nota che questa proprietà non è verificata, quindi non si può applicare il
  teorema dell'esperto.
\end{example}

\subsection{Merge sort}
Questo algoritmo di ordinamento è basato sulla tecnica divide et impera:
\begin{itemize}
  \item \textbf{Divide}: Dividi il problema in sottoproblemi più piccoli
  \item \textbf{Impera}: Risolvi i sottoproblemi in modo ricorsivo
  \item \textbf{Combina}: Unisci le soluzioni dei sottoproblemi per risolvere il problema
    originale
\end{itemize}
Questo algoritmo divide la sequenza in due parti uguali e le ordina separatamente, successivamente
le unisce in modo ordinato. La complessità, comsiderando il merge con complessità lineare,
risulta:
\[
  T(n) = 2T\left(\frac{n}{2}\right) + n
\] 
Applicando il teorema dell'esperto si ottiene:
\[
\begin{aligned}
  a & = 2\\
  b & = 2\\
  f(n) & = n\\
\end{aligned}
\] 
\[
  n^{\log_b a} = n
\] 
\[
  n \in \Theta(n)
\] 
Quindi siamo nel secondo caso del teorema dell'esperto:
\[
  T(n) \in \Theta(n \log n)
\]

\vspace{1em}
\noindent
Definizione del merge sort:
\begin{lstlisting}[language=Scala]
// A: Array da ordinare
// P: Indice di partenza
// r: Indice di arrivo
merge_sort(A, p, r)         // --
  if p < r                  //  |
    q <- floor((p + r) / 2) //  | 
    merge_sort(A, p, q)     //  | O(n log n)
    merge_sort(A, q + 1, r) //  |
    merge(A, p, q, r)       // --
\end{lstlisting}
\begin{lstlisting}[language=Scala]
// A: Array da ordinare
// P: Indice di partenza
// q: Indice di mezzo
// r: Indice di arrivo
merge(A, p, q, r)
  i <- 1
  j <- p
  k <- q + 1
  // Ordina gli elementi di A in B
  while(j <= q and k <= r)                // --
    if j <= q and (k > r or A[j] <= A[k]) //  |
      B[i] <- A[j]                        //  |
      j++                                 //  |
    else                                  //  | O(n)
      B[i] <- A[k]                        //  |
      k++                                 //  |
    i++                                   // --

  // Copia gli elementi di B in A
  for i <- 1 to r - p + 1                 // -|
    A[p + i - 1] <- B[i]                  // -| O(n)
\end{lstlisting}

\noindent
L'algoritmo è stablie perchè non vengono scambiati elementi uguali e non è in loco perchè
utilizza un array di appoggio.

\subsection{Heap}
È un albero semicompleto (ogni nodo ha 2 figli ad ogni livello tranne l'ultimo che è
completo solo fino ad un certo punto) in cui i nodi contengono oggetti con relazioni di
ordinamento.
\begin{figure}[H]
  \centering
  \begin{forest}
    for tree={
    draw,
    circle,
    minimum size=2em,
    inner sep=1pt,
    s sep=1cm,
  }
    [1
      [2
        [4
        [8]
        [9]
        ]
        [5
        [10]
        ]
      ]
      [3
        [6]
        [7]
      ]
    ]
  \end{forest}
  \caption{Heap con l'indice di un array associato ai nodi}
\end{figure}

\subsubsection{Proprietà}
\( \forall \) nodo il contenuto del nodo è \( \ge \) del contenuto dei figli.
Per calcolare il numero di nodi di un albero binario si usa la formula:
\[
  N = 2^0 + 2^1 + 2^2 + \ldots + 2^{h-1} = \frac{1-2^h}{1-2} = 2^h - 1
\] 
dove \( h \) è l'altezza dell'albero.
Il numero di foglie di un albero sono la metà dei nodi.

\vspace{1em}
\noindent
Definiamo una funzione che "aggiusta" i figli di un nodo per mantenere la proprietà di heap:
\begin{lstlisting}[language=Scala]
  heapify(A, i) // O(n)
    l <- left[i] // Indice del figlio sinistro (2i)
    r <- right[i] // Indice del figlio destro (2i+1)
    if l < H.heap_size and H[l] > H[i]
      largest <- l
    else
      largest <- i

    if r < H.heap_size and H[r] > H[largest]
      largest <- r
    if largest != i
      swap(H[i], H[largest])
      heapify(H, largest)
\end{lstlisting}
Ora si vuole definire una funzione che costruisce un heap da un array:
\begin{lstlisting}[language=Scala]
  build_heap(A) // O(n)
    heapsize(a) <- length[A]
    for i <- floor(length[A]/2) downto 1
      heapify(A, i)
\end{lstlisting}
Una volta definito un heap si possono fare diverse operazioni, come ad esempio estrarre
il nodo massimo:
\begin{lstlisting}[language=Scala]
  extract_max(A)
    H[1] <- H[H.heap_size]
    H.heap_size <- H.heap_size - 1
    heapify(H,1)
\end{lstlisting}
\subsubsection{Heap sort}
Heap sort è un algoritmo di ordinamento basato su heap.
\begin{lstlisting}[language=Scala]
  heap_sort(A) // O(n log n)
    build_heap(A) // n
    for i <- length[A] downto 2
      scambia(A[1], A[i])
      heapsize(A)--
      heapify(A, 1) // log i
\end{lstlisting}


La complessità dell'algoritmo è precisamente:
\[
\sum_{i=1}^{n} \log i = \log \prod_{i=1}^{n} i = \log n! = \Theta(\log n^n) = \Theta(n \log n)
\] 
Per la formula di Stirling \( n! \) ha ordine di grandezza \( n^n \). Questo algoritmo
è in loco e instabile.

\vspace{1em}
\noindent
Il caso pessimo è un array ordinato al contrario (\( O(n \log n)\)) e il caso migliore
è un array già ordinato (\( \Omega(n \log n)\)), quindi la complessità è:
\[
\Theta(n \log n)
\]

\subsection{Quick sort}
Il concetto di questo algoritmo è quello di mettere prima in disordine l'algoritmo e poi
ordinarlo. L'algoritmo divide l'array in 2 parti e ordina ricorsivamente le due parti; a
quel punto l'array è ordinato.

\begin{lstlisting}[language=Scala]
  // A: Array da ordinare 
  // p: Indice di partenza
  // r: Indice di arrivo
  quick_sort(A, p, r)
    if p < r // Ordina solo se l'array ha piu' di un elemento
      q <- partition(A, p, r) // Dividi l'array in due parti
      quick_sort(A, p, q) // Ordina sinistra
      quick_sort(A, q + 1, r) // Ordina destra
\end{lstlisting}
\begin{lstlisting}[language=Scala]
  partition(A, p, r)
    x <- A[p] // Elemento perno (o pivot)
    i <- p - 1
    j <- r + 1
    while true
      repeat // Ripete finche' la condizione non e' soddisfatta
        j--  // n/2
      until A[j] <= x // Trova un elemento che non puo' stare a destra
      repeat
        i++  // n/2
      until A[i] >= x // Trova un elemento che non puo' stare a sinistra
      if i < j
        swap(A[i], A[j]) // n/2
      else
        return j // alla fine j puntera' all'ultimo elemento di sinistra
\end{lstlisting}

\noindent
Questo algoritmo è in loco e non è stabile. La sua complessità nel caso peggiore è:
\[
  \begin{aligned}
    T(n) &= T(\text{partition}) + T(q) + T(n-q)\\
         &= n + T(q) + T(n-q)\\
         &= n + \cancel{T(1)} + T(n-1)\\
         &= n + T(n-1)\\
         &= \Theta(n^2)
  \end{aligned}
\] 
Il caso peggiore è un array già ordinato.

\vspace{1em}
\noindent
Mediamente ci si aspetta che l'array venga diviso in 2 parti molto simili, quindi
la complessità è \( O(n \log n) \) perchè:
\[
0 < c < 1
\] 
\[
  T(n) = n + T(cn) + T((1-c)n)
\] 

\begin{lstlisting}[language=Scala]
  rand_partition(A, p, r)
    i <- random(p, r)
    swap(A[i], A[p])
    return partition(A, p, r)
\end{lstlisting}
La complessità è la media di tutte le complessità con probabilità \( \frac{1}{n} \) 
\[
  \begin{aligned}
    T(n) = n + \frac{1}{n}\left( T(1) + T(n-1) \right) + \frac{n-1}{n}\left( T(2) + T(n-2) \right)\\
    + \ldots + \frac{1}{n}\left( T(n-1) + T(1) \right)
  \end{aligned}
\] 
\[
  \begin{aligned}
    T(n) &= n + \frac{1}{n} \sum_{i} \left( T(i) + T(n-i) \right) \\
         &= n + \frac{2}{n} \sum_{i} T(i)\\
  \end{aligned}
\] 

\subsection{Algoritmi di ordinamento non per confronti} 
Si possono avere algoritmi di ordinamento con complessità \( < n \log n \)?

\vspace{1em}
\noindent
Qualsiasi algoritmo che ordina per confronti deve fare almeno \( n \log n \) confronti
nel caso pessimo
\begin{figure}[H]
  \centering
  \begin{forest}
    for tree={
    minimum size=2em,
    inner sep=1pt,
    s sep=1cm,
  }
    [\( x \to n! \) 
      [\( x_1 \) 
        [\( x_{1,1} \)]
        [\( x_{1,2} \) ]
      ]
      [\( x_2 \) 
        [\( x_{2,1} \) ]
        [\( x_{2,2} \) ]
      ]
    ]
  \end{forest}
  \caption{Heap con l'indice di un array associato ai nodi}
\end{figure}
\noindent
Le foglie rappresentano ogni singola combinazione possibile. Il numero di foglie è
\( n! \) e l'altezza sarà sempre
\[
h \ge \log_2 n! = n \log n
\] 

\subsubsection{Counting sort}
Si vogliono ordinare \( n \) numeri con valori da \( 1 \) a \( k \). L'idea di questo
algoritmo è quella di creare un'array che contiene il numero di occorrenze di un
certo valore (rappresentato dall'indice).

\begin{lstlisting}[language=Scala]
counting_sort(A, k) 
  for i <- 1 to k // k
    C[i] <- 0 // Inizializzazione di un array a 0

  for j <- 1 to length[A] // n
    C[A[j]]++ // Conteggio delle occorrenze

  // k
  for i <- 2 to k         // In ogni indice metto il numero di
    C[i] <- C[i] + C[i-1] // elementi minori o uguali
                          // al numero dell'indice
  // Alla fine l'array C conterra' l'ultima posizione di occorrenza per ogni elemento

  for j <- length[A] downto 1 // n
    B[C[A[j]]] <- A[j] // Inserimento dell'elemento in posizione
    C[A[j]]--          // Decremento della posizione di occorrenza
\end{lstlisting}
La complessità di questo algoritmo è \( O(n + k) \) e siccome sappiamo che \( k \) è
una costante fissata a priori la complessità è \( O(n) \). Non è in loco, ma è stabile

\subsubsection{Radix sort}
Il radix sort è un ordinamento lessico grafico, cioè si ordinano le cifre partendo da
quella meno significativa e se sono uguali si passa a quella più significativa.

La complessità dell'algoritmo è:
\[
 \Theta(l(n + k))
\] 
dove:
\[
\begin{aligned}
  l & = \text{numero di cifre} = \log_k n\\
  n & = \text{numero di elementi}\\
  k & = \text{numero di valori possibili}
\end{aligned}
\] 
Se rappresentiamo i numeri in base \( n \), allora si avrà la seguente rappresentazione:
\[
   \ldots \;\; n^2 \;\; n^1 \;\; n^0
\] 
e ad esempio per rappresentare \( n^2 - 1 \) valori possibili serviranno \( 2 \) cifre.
cifre.

\subsubsection{Bucket sort}
Dato un array di numeri con \textbf{supporto infinito} e \textbf{distribuzione nota}, si può dividere
l'array in \( k \) parti (bucket) uguali (equiprobabili) e ordinare ricorsivamente. Ogni coppia
di gruppi deve essere totalmente ordinata, cioè ogni elemento del primo gruppo deve essere
minore di ogni elemento del secondo gruppo. Una volta ordinati i gruppi (con un algoritmo
di ordinamento a scelta) si concatenano in modo ordinato.

\vspace{1em}
\noindent
Il caso peggiore è quello in cui tutti gli elementi finiscono in un singolo bucket,
la probabilità che questo accada è molto bassa:
\[
  \underbrace{\frac{1}{n} \cdot \frac{1}{n} \cdot \dots \cdot \frac{1}{n}}_{n-1}
  = \frac{1}{n^{n-1}}
\] 
e la sua complessità diventa:
\[
  O(n^2)
\] 

\vspace{1em}
\noindent
Nel caso medio si ha che per creare i bucket si ha una complessità \( O(n) \) e per
assegnare gli elementi ai bucket si ha una complessità \( O(n) \). Per ogni bucket
ci si aspetta che il numero di elementi al suo interno sia una \textbf{costante},
quindi \textbf{indipendente dal valore di \( n \)}. Per ordinare un bucket si ha una
complessità \( O(1) \) siccome il numero di elementi è costante. La complessità totale
è quindi:
\[
  \Theta(n)
\]

\vspace{1em}
\noindent
Formalizzando si ha:

\noindent
Sia \( X_{ij} \) la variabile casuale che vale: 
$\begin{cases}
  1 & \text{se l'elemento } i \text{ va nel bucket } j\\
  0 & \text{altrimenti}
\end{cases}$

\noindent
Per esprimere il numero di elementi nel bucket \( j \) si può scrivere:
\[
  N_j = \sum_{i} X_{ij}
\] 
La complessità di questo algoritmo sarà quindi:
\[
  C = \sum_{j} (N_j)^2
\] 
Per ottenere il valore medio della complessità:
\[
  \begin{aligned}
    E[C] &= E\left[\sum_{j} (N_j)^2\right] \\
         &= \sum_{j} E[(N_j)^2] \\
         &= \sum_{j} \left( Var[N_j] + E[N_j]^2 \right) \\
  \end{aligned}
\] 
sappiamo che \( N_j = \sum_{i} X_{ij} \), quindi la media è:
\[
  E[N_j] = \sum_{j}^{n} E[X_{ij}] = \sum_{j}^{n} \frac{1}{n} = 1
\] 
e la varianza è:
\[
  Var[N_j] = \sum_{j}^{n} Var[X_{ij}] = \sum_{j}^{n} \frac{1}{n} \left( 1 - \frac{1}{n} \right) = 1 - \frac{1}{n}
\] 
La complessità diventa:
\[
  \begin{aligned}
    E[C] &= \sum_{j} \left( \left( 1 - \frac{1}{n} \right) - 1 \right) \\
         &= \sum_{j} 2 - \frac{1}{n} \\
         &= 2n - 1
  \end{aligned}
\] 

\section{Algoritmi di selezione}
Dato in input un array \( A \) di oggetti su cui è definita una relazione di ordinamento
e un indice \( i \) compreso tra \( 1 \) e \( n \) (\( n \) è il numero di oggetti
nell'array), l'output dell'algoritmo è l'oggetto che si trova in posizione \( i \)
nell'array ordinato.
\begin{lstlisting}[language=Scala]
selezione(A, i)
  ordina(A) // O(n log n)
  return A[i]
\end{lstlisting}
Quindi la complessità di questo algoritmo nel caso peggiore è \( O(n \log n) \)
(limite superiore). È possibile selezionare un elemento in tempo lineare? Analizziamo
un caso particolare dell'algoritmo di selezione, ovvero la ricerca del minimo (o del massimo).


\subsection{Ricerca del minimo o del massimo}
\vspace{1em}
\noindent
In tempo lineare si può trovare il minimo e il massimo
di un array:
\begin{lstlisting}[language=Scala]
minimo(A)
  min <- A[1]
  for i <- 2 to length[A]
    if A[i] < min
      min <- A[i]
  return min
\end{lstlisting}
trovare il minimo equivale a trovare \texttt{selezione(A, 1)} e trovare il massimo
equivale a trovare \texttt{selezione(A, n)}. Si può però andare sotto la complessità
lineare?

\vspace{1em}
\noindent
Per trovare il massimo (o il minimo) elemento \( n \) di un array bisogna fare
\textbf{almeno} \( n-1 \) confronti perchè bisogna confrontare ogni elemento con
l'elemento massimo (o minimo) trovato per poter dire se è il massimo (o minimo).
Di conseguenza, non è possibile avere un algoritmo per la ricerca del massimo (o minimo)
in cui c'è un elemento che non "perde" mai ai confronti (cioè risulta sempre il più 
grande) e non viene dichiarato essere il più grande (o più piccolo).

\vspace{1em}
\noindent
\textbf{Dimostrazione}:
Per dimostrarlo si può prendere un array in cui l'elemento \( a \) non perde mai ai
confronti, ma l'algoritmo dichiara che il massimo è l'elemento \( b \). Allora si rilancia
l'algoritmo sostituendo l'elemento \( a \) con \( a = \text{\texttt{max(b+1,a)}} \) e si
ripete l'algoritmo con questo secondo array in cui \( a \) è l'elemento più grande. Si ha
quindi che i confronti in cui \( a \) non è coinvolto rimangono gli stessi e i confronti
in cui \( a \) è coinvolto non cambiano perchè anche prima \( a \) non perdeva mai ai
confronti, di conseguenza l'algoritmo dichiarerà che il massimo è \( b \) e quindi
l'algoritmo non è corretto, dimostrando che non esiste un algoritmo che trova il massimo
in meno di \( n-1 \) confronti.

\vspace{1em}
\noindent
Abbiamo quindi trovato che la complessità del massimo (o minimo) nel caso migliore è 
\( \Omega(n) \) (limite inferiore) e nel caso peggiore è \( O(n) \) (limite superiore).
Di conseguenza la complessità è \( \Theta(n) \).

\subsubsection{Ricerca del minimo e del massimo contemporaneamente}
Si potrebbe implementare unendo i 2 algoritmi precedenti:
\begin{lstlisting}[language=Scala]
min_max(A)
  min <- A[1]
  max <- A[1]
  for i <- 2 to length[A]
    if A[i] < min
      min <- A[i]
    if A[i] > max
      max <- A[i]
  return (min, max)
\end{lstlisting}
Questo algoritmo esegue \( n-1 + n-1 = 2n-2 \) confronti.

\begin{itemize}
  \item \textbf{Limite inferiore}: Potenzialmente ogni oggetto potrebbe essere il minimo
    o il massimo. Sia \( m \) il numero di oggetti potenzialmente minimi e \( M \) il
    numero di oggetti potenzialmente massimi. Sia \( n \) il numero di oggetti nell'array.
    \begin{itemize}
      \item All'inizio \( m+M = 2n \) perchè ogni oggetto può essere sia minimo che 
        massimo.
      \item Alla fine \( m+M = 2 \) perchè alla fine ci sarà un solo minimo e un solo 
        massimo.
    \end{itemize}
    Quando viene fatto un confronto \( m+M \) può diminuire.
    \begin{itemize}
      \item Se si confrontano due oggetti che sono potenzialmente sia minimi che massimi,
        allora \( m+M \) diminuisce di \( 2 \) perchè:
        \[
          a < b
        \] 
        \( b \) non può essere il minimo e \( a \) non può essere il massimo e si perdono
        2 potenzialità.

      \item Se si confrontano due potenziali minimi (o massimi), allora \( m+M \) 
        diminuisce di \( 1 \) perchè:
        \[
          a < b
        \]
        \( b \) non può essere il minimo e si perde 1 potenzialità.
    \end{itemize}
    Un buon algoritmo dovrebbe scegliere di confrontare sempre 2 oggetti che sono
    entrambi potenziali minimi o potenziali massimi.

    \vspace{1em}
    \noindent
    Due oggetti che sono potenzialmente sia minimi che massimi esistono
    se \( m+M > n+1 \) perchè se bisogna distribuire n potenzialità ne avanzano
    due che devono essere assegnate a due oggetti che hanno già una potenzialità.
    Quindi fino a quando \( m+M \) continua ad essere almeno \( n+2 \) si riesce a
    far diminuire \( m+M \) di 2 ad ogni confronto.

    Questa diminuzione si può fare \( \left\lfloor \frac{n}{2} \right\rfloor \) volte,
    successivamente \( m+M \) potrà calare solo di 1 ad ogni confronto.
    
    \vspace{1em}
    \noindent
    Successivamente il numero di oggetti rimane:
    \[
      \begin{cases}
        n+1 & \text{se } n \text{ è dispari}\\
        n & \text{se } n \text{ è pari}
      \end{cases}
    \] 
    \begin{itemize}
      \item \( n \) dispari:
        \[
          \begin{aligned}
            &n+1 - 2 + \left\lfloor \frac{n}{2} \right\rfloor\\
            &= n-1 + \left\lfloor \frac{n}{2} \right\rfloor\\
            &= \left\lfloor \frac{3}{2}n \right\rfloor - 1\\
            &= \left\lceil \frac{3}{2}n \right\rceil - 2\\
          \end{aligned}
        \] 

      \item \( n \) pari:
        \[
          \begin{aligned}
            &n - 2 + \left\lfloor \frac{n}{2} \right\rfloor \\
            &= n-2 + \frac{n}{2}\\
            &= \frac{3}{2}n - 2\\
            &= \left\lceil \frac{3}{2}n \right\rceil -2
          \end{aligned}
        \]
    \end{itemize}
    Quindi la complessità è \( \Omega(\left\lceil \frac{3}{2}n \right\rceil -2) = \Omega(n)
    \) (limite inferiore). Meglio di così non si può fare, ma non è detto che esista
    un algoritmo che raggiunga questo limite inferiore.
\end{itemize}
Un algoritmo che raggiunge il limite inferiore è il seguente:
\begin{enumerate}
  \item Dividi gli oggetti in 2 gruppi:
    \[
      \underbrace{
        \underbrace{
          \begin{aligned}
          &a_1\\
          &a_2\\
          &\vdots\\
          &a_{\left\lfloor \frac{n}{2} \right\rfloor}
          \end{aligned}
        }_{\text{Potenziali minimi}}
        \quad
        \underbrace{
          \begin{aligned}
        &b_1\\
        &b_2\\
        &\vdots\\
        &b_{\left\lceil \frac{n}{2} \right\rceil}
          \end{aligned}
        }_{\text{Potenziali massimi}}
      }_{\text{Potenziali sia minimi che massimi}}
    \] 

  \item Confronta \( a_i \) con \( b_i \), supponendo \( a_i < b_i \) (mette a sinistra
    i più piccoli e a destra i più grandi)

  \item Cerca il minimo degli \( a_i \) e cerca il massimo dei \( b_i \):

  \item Sistema l'eventuale elemento in più se l'array è dispari
\end{enumerate}

\subsection{Randomized select}
Si può implementare un algoritmo che divide l'array in 2 parti allo stesso modo
in cui viene effettuata la \texttt{partition} di quick sort:
\begin{lstlisting}[language=Scala]
// A: Array
// p: Indice di partenza
// r: Indice di arrivo
// i: Indice che stiamo cercando (compreso tra 1 e r-p+1)
randomized_select(A, p, r, i)
  if p = r
    return A[p]
  q <- randomized_partition(A, p, r)
  k <- q - p + 1 // Numero di elementi a sinistra 
  // Controlla se l'elemento cercato e' a sinistra o a destra
  if i <= k
    return randomized_select(A, p, q, i) // Cerca a sinistra
  else
    return randomized_select(A, q+1, r, i-k) // Cerca a destra
\end{lstlisting}
\begin{itemize}
  \item 
    Se dividessimo sempre a metà si avrebbe:
    \[
      T(n) = n + T\left(\frac{n}{2}\right) = \Theta(n) \text{ (terzo caso del teorema dell'esperto)}
    \] 

  \item Mediamente:
    \[
      \begin{aligned}
        T(n) &= n + \frac{1}{n} T \left( max(1,n-1) \right) + \frac{1}{n} T \left( max(2,n-2) \right)
        + \dots\\
             &= n + \frac{2}{n} \sum_{i=\frac{n}{2}}^{n-1} T \left( i \right)\\
      \end{aligned}
    \] 
    La complessita media è lineare.

    Si esegue un solo ramo, che nel caso pessimo è quello con più elementi. La risoluzione
    è la stessa del quick sort.
\end{itemize}
Esiste un algoritmo che esegue la ricerca in tempo lineare anche nel caso peggiore?

Si potrebbe cercare un elemento perno più ottimale, cioè che divida l'array in
\textbf{parti proporzionali}:
\begin{enumerate}
  \item Dividi gli oggetti in \( \left\lfloor \frac{n}{5} \right\rfloor \) gruppi di
    5 elementi più un eventuale gruppo con meno di 5 elementi.

  \item Calcola il mediano di ogni gruppo di 5 elementi (si ordina e si prende l'elemento
    centrale). \( \Theta(n) \)

  \item Calcola ricorsivamente il mediano \( x \) dei mediani
    \[
      T\left(\left\lceil \frac{n}{5} \right\rceil\right)
    \] 

  \item Partiziona con perno \( x \) e calcola \( k \) (numero di elementi a sinistra).
    \( \Theta (n) \) 

  \item Se \( i<k \) cerca a sinistra l'elemento \( i \), altrimenti cerca a destra
    l'elemento \( i-k \). La chiamata ricorsiva va fatta su un numero di elementi
    sufficientemente piccolo, e deve risultare un proporzione di \( n \), quindi
    ad esempio dividere in gruppi da 3 elementi non funzionerebbe.
    \[
    T(?)
    \] 
    \[
      \begin{aligned}
        m_1 \;\;&\to\;\; m_2 \;\;&\to\;\; m_3 \;\;&\to\;\; \color{red}\underset{x}{m_4} \;\;&\to\;\; \color{green!50!black}m_5
        \;\;&\to\;\; \color{green!50!black}m_6 \;\;&\to\;\; \color{blue}m_7\\
             &&&& \downarrow \quad&\qquad \downarrow &\downarrow \;\;\\
             &&&& \color{green!50!black}m_{5,4} &\qquad \color{green!50!black}m_{6,4} & \color{blue}m_{7,4} \\
             &&&& \downarrow \quad&\qquad \downarrow &\\
             &&&& \color{green!50!black}m_{5,5} &\qquad \color{green!50!black}m_{6,5} & \\
      \end{aligned}
    \] 
    Gli elementi verdi sono maggiori dell'elemento \( x \) e ogni elemento verde avrà
    2 elementi maggiori di esso (tranne nel caso del gruppo con meno di 5 elementi
    rappresentato in blu).
    \[
      \text{\#left} \le 3 \cdot  \left(\underbrace{\left\lceil \frac{1}{2} \left\lceil \frac{n}{5} \right\rceil  \right\rceil}_{\text{
          verdi + blu + rosso
      }} - \underbrace{2}_{\text{rosso + blu}} \right) 
      = \frac{7}{10} n + 6
    \] 
    \[
      \text{\#right} \ge 3 \cdot  \left(\underbrace{\left\lceil \frac{1}{2} \left\lceil \frac{n}{5} \right\rceil  \right\rceil}_{\text{
          verdi + blu + rosso
      }} - \underbrace{2}_{\text{rosso + blu}} \right) 
      = \frac{7}{10} n + 6
    \] 
    Da ogni parte si hanno almeno \( \frac{7}{10} n + 6 \) elementi.

    \vspace{1em}
    \noindent
    Quindi abbiamo trovato \( T(?) \):
    \[
      T(n) = \Theta (n) + T\left(\left\lceil \frac{n}{5} \right\rceil\right) + T\left(\frac{7}{10} n + 6\right)
    \] 
    Dimostriamo con il metodo di sostituzione, supponendo \( T(n) \le cn \), che la
    disequazione sia vera:
    \[
    \begin{aligned}
      T(n) &\le n + c \left\lceil \frac{n}{5} \right\rceil + c(\frac{7}{10}n + 6) \
    \end{aligned}
    \] 
    Non sappiamo se \( \frac{7}{10}n + 6 \) è minore di \( n \), quindi lo calcoliamo:
    \[
    \begin{aligned}
      \frac{7}{10}n + 6 &\le n\\
      7n + 60 &\le 10n\\
      3n &\ge 60\\
      n &\ge 20
    \end{aligned}
    \] 
    Quindi per valori di \( n \le 20 \) la disequazione non è vera. Consideriamo quindi
    \( \bar{n} > 20 \) e \( n > \bar{n} \). Togliendo l'approssimazione per eccesso si ha:
    \[
    \begin{aligned}
      T(n) &\le n + c + \frac{c}{5}n + \frac{7}{10}n + 6c \\
           &\le \frac{9}{10}cn + 7c + n \\
           &\stackrel{?}{\le} cn \\
           &= cn + \left(-\frac{1}{10}cn + 7c + n\right) \le cn \text{ quando } \\
           & \left( n + 7c - \frac{1}{10}cn \right) \le cn
    \end{aligned}
    \] 
    Quindi \( T(n) \le cn \) e quindi \( T(n) = O(n) \). Abbiamo trovato un limite superiore
    e un limite inferiore, quindi la complessità è \( T(n) = \Theta(n) \) . Il problema è
    che le costanti sono così alte che nella pratica è meglio il
    \texttt{randomized\_select}.
\end{enumerate}

\vspace{1em}
\noindent
Esiste un modo per strutturare meglio le informazioni nel calcolatore per trovare
l'elemento cercato in tempo \( \log n \)?
Si possono implementare delle \textbf{Strutture dati} che permettono di fare
ricerche in tempo logaritmico.

\section{Strutture dati}
Una struttura dati è un modo per organizzare i dati in modo da poterli manipolare
in modo efficiente. Bisogna avere un modo per comunicare con le strutture dati, senza
dover sapere come sono implementate.
\subsection{Stack}
Ad esempio se consideriamo uno stack, si possono individuare le seguenti operazioni:
\begin{itemize}
  \item \texttt{new()}: Crea uno stack vuoto
  \item \texttt{push(S,x)}: Inserisce un elemento \( x \) nello stack \( S \) 
  \item \texttt{top(S)}: Restituisce l'elemento in cima allo stack \( S \) 
  \item \texttt{pop(S)}: Rimuove l'elemento in cima allo stack \( S \) 
  \item \texttt{is\_empty()}: Restituisce vero se lo stack è vuoto
\end{itemize}
Da queste operazioni si possono definire certe proprietà dello stack:
\begin{itemize}
  \item \texttt{top(push(S,x)) = x}
  \item \texttt{pop(push(S,x)) = S}
\end{itemize}
Questo ci dice che lo stack è LIFO (Last In First Out). 

\vspace{1em}
\noindent
Abbiamo quindi definito
un'algebra dei termini da cui possono definire tutte le operazioni possibili,
ad esempio uno stack è definito come una sequenza di push:
\[
  \text{\texttt{push(push(push(empty(),1),2),3)}}
\] 

\subsection{Queue}
Una coda è una struttura dati in cui si possono inserire elementi in fondo e
rimuoverli dall'inizio. Le operazioni possibili sono:
\begin{itemize}
  \item \texttt{new()}: Crea una coda vuota
  \item \texttt{enqueue(Q,x)}: Inserisce un elemento \( x \) in fondo alla coda \( Q \) 
  \item \texttt{dequeue(Q)}: Rimuove l'elemento in cima alla coda \( Q \) 
  \item \texttt{front(Q)}: Restituisce l'elemento in cima alla coda \( Q \) 
  \item \texttt{is\_empty()}: Restituisce vero se la coda è vuota
\end{itemize}

\subsection{Lista doppiamente puntata}
Una lista doppiamente puntata è una struttura dati in cui ogni nodo ha un puntatore
al nodo precedente e al nodo successivo.

\vspace{1em}
\noindent
Per rimuovere un elemento \( x \) dalla lista bisogna:
\begin{enumerate}
  \item Trovare il nodo \( x \)
  \item Collegare il nodo precedente a \( x \) con il nodo successivo a \( x \)
\end{enumerate}
Il problema di questa soluzione è che bisogna anche gestire i casi degli estremi
separatamente. Per evitare di gestire i casi specifici si possono aggiungere dei
nodi sentinella all'inizio e alla fine della lista.

\subsection{Albero binario}
Un albero binario è una struttura dati in cui ogni nodo ha al massimo 2 figli.
\begin{figure}[H]
  \centering
  \begin{forest}
    for tree={
    circle,
    draw,
    minimum size=2em,
    inner sep=1pt,
    s sep=1cm,
  }
    [\( x \)
      [\( y \)
        [,no edge, draw=none]
        [\( z \)]
      ]
      [\( v \)
        [\( u \)]
        [\( t \)]
      ]
    ]
  \end{forest}
  \caption{Albero binario}
\end{figure}
\noindent
Le operazioni possibili su un albero binario sono:
\begin{itemize}
  \item \texttt{new()}: Crea un albero vuoto
  \item \texttt{insert(T, x)}: Crea un nuovo nodo con valore \( x \) e lo aggiunge
    all'albero \( T \)
  \item \texttt{extract(T, x)}: Rimuove il nodo con valore \( x \) dall'albero \( T \)
  \item \texttt{is\_empty()}: Restituisce vero se l'albero è vuoto
  \item \texttt{left(T)}: Restituisce il figlio sinistro dell'albero \( T \) 
  \item \texttt{right(T)}: Restituisce il figlio destro dell'albero \( T \) 
  \item \texttt{value(T)}: Restituisce il valore del nodo dell'albero \( T \)
\end{itemize}
Un albero è \textbf{bilanciato} quando la differenza tra l'altezza del sottoalbero sinistro
e di quello destro è al massimo 1. Un albero è \textbf{completo} quando tutti i livelli
sono completi, cioè tutti i nodi sono presenti, tranne l'ultimo, che può essere incompleto.

\vspace{1em}
\noindent
La profondità di un albero di \( n \) nodi è:
\[
P(n) = 1 + P\left( \left\lceil \frac{n-1}{2} \right\rceil \right) = \Theta(\log n)
\] 

\subsubsection{Albero binario di ricerca}
Un albero binario di ricerca è un albero binario in cui per ogni nodo \( x \) valgono
le seguenti proprietà:
\begin{itemize}
  \item Tutti i nodi nel sottoalbero sinistro di \( x \) hanno valore minore di \( x \) 
  \item Tutti i nodi nel sottoalbero destro di \( x \) hanno valore maggiore di \( x \)
\end{itemize}

\begin{figure}[H]
  \centering
  \begin{forest}
    for tree={
    circle,
    draw,
    minimum size=2em,
    inner sep=1pt,
    s sep=1cm,
  }
    [\( 25 \)
      [\( 17 \)
        [\( 2 \)]
        [\( 20 \)]
      ]
      [\( 30 \)
        [\( 27 \)]
        [\( 40 \)]
      ]
    ]
  \end{forest}
  \caption{Albero binario di ricerca}
\end{figure}

\noindent
Le operazioni possibili su un albero binario di ricerca sono:
\begin{itemize}
  \item \texttt{insert(T, x)}: Inserisce un nodo con valore \( x \) nell'albero \( T \)
  \item \texttt{extract(T, x)}: Rimuove il nodo con valore \( x \) dall'albero \( T \)
  \item \texttt{search(T, x)}: Restituisce vero se il valore \( x \) è 
    presente nell'albero \( T \)
  \item \texttt{min(T)}: Restituisce il valore minimo dell'albero \( T \)
  \item \texttt{max(T)}: Restituisce il valore massimo dell'albero \( T \)
\end{itemize}

\vspace{1em}
\noindent
Non è possibile inserire un nodo in un albero binario di ricerca in tempo logaritmico
perchè potrebbe dover essere inserito in modo da sbilanciare l'albero, quindi per
riottenere un albero bilanciato non rimane che spostare tutti gli elementi in una
posizione diversa.

Per ottenere la complessità logaritmica, bisogna utilizzare un albero che si può
sbilanciare, ma non troppo.

\begin{itemize}
  \item \textbf{Inserimento}
    Per inserire un nodo in un albero binario di ricerca si può procedere nel seguente modo:
    \begin{enumerate}
      \item Si parte dalla radice e si scende fino a trovare il nodo in cui inserire il
        nuovo nodo rispettando le proprietà dell'albero binario di ricerca
      \item Si inserisce il nodo in quel punto
    \end{enumerate}

  \item \textbf{Rimozione}
    Per rimuovere un elemento da un albero binario di ricerca si possono avere 3 casi:
    \begin{enumerate}
      \item Il nodo da rimuovere è una foglia: si può rimuovere direttamente
      \item Il nodo da rimuovere ha un solo figlio: si può sostituire il nodo con il figlio
      \item Il nodo da rimuovere ha due figli: si può sostituire il nodo con il minimo
        del sottoalbero destro o con il massimo del sottoalbero sinistro
    \end{enumerate}
    
\end{itemize}
Una volta che un nodo viene aggiunto o rimosso non si può più essere certi che 
l'albero sia ancora bilanciato, quindi bisogna fare in modo che l'albero venga
bilanciato ogni volta che viene modificato e la complessità non sarà più logaritmica.

\subsubsection{RB-Tree (Red-Black Tree)}
Gli alberi rosso-neri sono alberi binari di ricerca in cui ogni nodo può essere
rosso o nero.
\begin{itemize}
  \item \textbf{Nero}: Il nodo nero indica che il nodo è a posto
  \item \textbf{Rosso}: Il nodo rosso è un nodo ausiliario
\end{itemize}
Questo tipo di albero ha le seguenti proprietà:
\begin{enumerate}
  \item Ogni nodo è rosso o nero
  \item Ogni foglia è nera
  \item I figli di un nodo rosso sono neri
  \item Ogni cammino dalla radice ad una foglia contiene lo stesso numero di nodi neri
\end{enumerate}
\begin{example}
  Abbiamo un albero con \( l \) nodi neri nel sotto albero destro, quindi per la proprietà
  4, il sotto albero sinistro deve avere \( l \) nodi neri:
  \begin{figure}[H]
    \centering
    \begin{forest}
      for tree={
      circle,
      draw,
      minimum size=2em,
      inner sep=1pt,
      s sep=1cm,
    }
    [
      [,red
        [
        [,red
        []
        [,no edge, draw=none]
        ]
        [,no edge, draw=none]
        ]
        [,no edge, draw=none]
      ]
      [
        [,no edge, opacity=0]
        []
      ]
    ]
    \end{forest}
    \caption{Albero rosso-nero}
  \end{figure}

  \noindent Il sotto albero sinistro avrà \( 2l \) nodi totali.
\end{example}

\begin{example}
  \label{ex:rb-tree}
  Prendiamo ad esempio il seguente albero RB:
  \begin{figure}[H]
    \centering
    \begin{forest}
      for tree={
      circle,
      draw,
      minimum size=2em,
      inner sep=1pt,
      s sep=0.2cm,
      scale=0.8
    }
    [26
      [17,red
        [14
          [10,red
            [7
              [3,red
                [nil]
                [nil]
              ]
            ]
            [12
              [nil]
              [nil]
            ]
          ]
          [16
            [15,red
              [nil]
              [nil]
            ]
          ]
        ]
        [21
          [19
            [20,red
              [nil]
              [nil]
            ]
          ]
          [23
            [nil]
            [nil]
          ]
        ]
      ]
      [41
        [30,red
          [28
            [nil]
            [nil]
          ]
          [38
            [33,red
              [nil]
              [nil]
            ]
            [39,red
              [nil]
              [nil]
            ]
          ]
        ]
        [47
          [nil]
          [nil]
        ]
      ]
    ]
    \end{forest}
    \caption{Albero rosso-nero}
  \end{figure}

  \noindent
  Tutte le proprietà sono verificate, però tutte le foglie sono dei \texttt{nil} (nero)
  e questo è uno spreco di memoria, perchè metà dei nodi di un albero sono foglie e quindi
  metà dei nodi sono utilizzati per la sentinella.
  
  Questo si può risolvere facendo puntare tutte le sentinelle allo stesso valore \texttt{nil},
  però facendo così non si riesce più a risalire all'elemento padre di una foglia.
\end{example}

\noindent
Il concetto principale di questo tipo di alberi è quello della \textbf{black height},
cioè il numero di nodi neri che si incontrano lungo un cammino dalla radice ad una foglia.
Prendendo in considerazione l'esempio \ref{ex:rb-tree} i nodi:
\begin{itemize}
  \item \textbf{nil}: Hanno black height 0
  \item \textbf{3}: Ha black height 1
  \item \textbf{19}: Ha black height 1
  \item \textbf{14}: Ha black height 2
  \item \textbf{26}: Ha black height 3
  \item ecc...
\end{itemize}

\vspace{1em}
\noindent
\textbf{Lemma:}

\noindent
Per ogni nodo \( x \) dell'albero, il sottoalbero radicato in \( x \) contiene almeno
\( 2^{bh(x)} - 1 \) nodi interni (nodi che non sono una foglia).

\vspace{1em}
\noindent
\textbf{Dimostrazione:}

\noindent
Dimostriamo per induzione su \( bh(x) \):
\begin{itemize}
  \item \textbf{Caso base}: Se \( x \) è una foglia, allora \( bh(x) = 0 \) e il sottoalbero
    radicato in \( x \) contiene 0 nodi interni.
    \[
      2^{bh(x)} - 1 = 2^0 - 1 = 0
    \] 

  \item Se \( x \) è un nodo con un figlio destro \( b \) e un figlio sinistro
    \( a \) allora:
    \begin{figure}[H]
      \centering
      \begin{forest}
        for tree={
        circle,
        draw,
        minimum size=2em,
        inner sep=1pt,
        s sep=1cm,
      }
        [x
          [a]
          [b]
        ]
      \end{forest}
    \end{figure}
    \[
      bh(a) \ge bh(x) -1 \quad \text{e} \quad bh(b) \ge bh(x) -1
    \]
    \[
      \#nodi(x) \ge \#nodi(a) + \#nodi(b) + 1
    \] 
    \[
    \Downarrow
    \] 
    \[
      \begin{aligned}
        \#nodi(x) &\ge 2^{bh(a)} - 1 + 2^{bh(b)} - 1 + 1\\
                  &\ge 2^{bh(x) - 1} - 1 + 2^{bh(x) - 1} \cancel{- 1} \cancel{+ 1}\\
                  &= 2 \cdot 2^{bh(x) - 1} - 1\\
                  &= 2^{bh(x)} - 1
      \end{aligned}
    \] 
    Quindi
    \[
      \#nodi(x) \ge 2^{bh(x)} - 1
    \] 
\end{itemize}

\vspace{1em}
\noindent
\textbf{Complessità}:
Consideriamo un albero RB di altezza \( h \) (distanza tra la radice e la foglia più
lontana, escludendo la radice) e radice \( x \), si ha che \( bh(x) \) vale:
\[
  bh(x) \ge \frac{h}{2}
\] 
Il numero di nodi \( n \) (per il lemma) vale \( 2^{\frac{h}{2}} \), si ha quindi che
l'altezza di un RB albero è \textbf{almeno} il doppio dell'altezza di un albero binario:
\[
  \begin{aligned}
    n &\ge 2^{\frac{h}{2}-1}\\
    2^{\frac{h}{2}} &\le n+1\\
    \frac{h}{2} &\le \log_2(n+1)\\
    h &\le 2 \log_2(n+1)
  \end{aligned}
\] 

\begin{itemize}
  \item \textbf{Inserimento}: L'inserimento di un nodo in un albero RB si fa allo stesso
    modo di un albero binario di ricerca. Per non violare la proprietà 4, si fa
    di colore rosso e si salva un puntatore al nuovo oggetto, questo perchè il nuovo albero
    è un RB albero, \textbf{tranne} per l'oggetto puntato dal puntatore perchè potrebbe
    avere un padre rosso. Si propaga l'anomalia verso la radice dell'albero per poter
    cambiare il colore della radice mantenendo tutte le proprietà. Per risolvere il problema
    si può fare una serie di \textbf{rotazioni} a destra o a sinistra.

    \vspace{1em}
    \noindent
    Prendiamo in considerazione il seguente albero:
    \begin{figure}[H]
      \centering
      \begin{forest}
        for tree={
        circle,
        draw,
        minimum size=2em,
        inner sep=1pt,
        s sep=1cm,
      }
      [y
        [x
          [\( \alpha \)]
          [\( \beta \)]
        ]
        [\( \gamma  \)]
      ]
      \end{forest}
    \end{figure}

    \noindent
    La rotazione a destra della coppia \( x \) e \( y \) è la seguente:
    \begin{figure}[H]
      \centering
      \begin{forest}
        for tree={
        circle,
        draw,
        minimum size=2em,
        inner sep=1pt,
        s sep=1cm,
      }
      [x
        [\( \alpha \)]
        [y
          [\( \beta \)]
          [\( \gamma \)]
        ]
      ]
      \end{forest}
    \end{figure}
    Questa operazione mantiene tutte le proprietà dell'albero binario di ricerca.
    L'operazione opposta è la rotazione a sinistra. Il tempo di esecuzione di una
    rotazione è costante.

    \vspace{1em}
    \noindent
    Lo pseudocodice dell'algoritmo per risistemare l'albero è il seguente:
\begin{lstlisting}[language=Scala]
// Albero con radice sicuramente nera
// x: Nodo da cui si propaga l'anomalia

while x != root and color(parent(x)) == red
  // Se il padre e' figlio sinistro del nonno
  if parent(x) == left(parent(parent(x)))
    y <- right(parent(parent(x))
    if color(y) == red
      color(parent(parent(x))) <- red
      color(parent(x)) <- black
      color(y) <- black
      x <- parent(parent(x))
    else
      if x == right(parent(x))
        x <- parent(x)
        left_rotate(x)

      color(parent(x)) <- black
      color(parent(parent(x))) <- red
      right_rotate(parent(parent(x)))
      x <- root

  // Se il padre e' figlio sinistro del nonno
  // Si fanno le stesse operazioni con le direzioni invertite
\end{lstlisting}

\vspace{1em}
\noindent
La complessità di questo algoritmo è \( O(\log n) \) perchè è un inserimento in un albero
binario di ricerca con delle rotazioni per sistemare l'anomalia, ma il numero di rotazioni
è costante.

\begin{example}
  Un esempio del predcedente algoritmo è il seguente:
  \begin{figure}[H]
    \centering
    \begin{forest}
      for tree={
        circle,
        draw,
        minimum size=2em,
        inner sep=1pt,
        s sep=1cm,
      }
      [11
        [2,red
          [1]
          [7
            [5,red
            [\( \underset{x}{4} \),red]
              [,no edge, draw=none]
            ]
            [\( \underset{y}{8} \),red]
          ]
        ]
        [14]
      ]
    \end{forest}
  \end{figure}
  \noindent
  Si cambiano i colori dei nodi \( parent(x) \) e \( y \) e si sposta l'anomalia verso
  l'alto
  \begin{figure}[H]
    \centering
    \begin{forest}
      for tree={
        circle,
        draw,
        minimum size=2em,
        inner sep=1pt,
        s sep=1cm,
      }
      [11
        [2,red
          [1]
          [\( \underset{x}{7} \),red
          [5
              [4,red]
              [,no edge, draw=none]
            ]
            [8 ]
          ]
        ]
        [\( \underset{y}{14} \)]
      ]
    \end{forest}
  \end{figure}

  \noindent
  Prendiamo la coppia 2 e 7 e ruotiamo a sinistra
  \begin{figure}[H]
    \centering
    \begin{forest}
      for tree={
        circle,
        draw,
        minimum size=2em,
        inner sep=1pt,
        s sep=1cm,
      }
      [11
        [7,red
        [\( \underset{x}{2} \),red
          [1]
          [5
            [4,red]
            [,no edge, draw=none]
          ]
        ]
          [8
          ]
        ]
        [\( \underset{y}{14} \)]
      ]
    \end{forest}
  \end{figure}
  \noindent
  Si cambiano i colori e si effettua una rotazione a destra
  \begin{figure}[H]
    \centering
    \begin{forest}
      for tree={
        circle,
        draw,
        minimum size=2em,
        inner sep=1pt,
        s sep=1cm,
      }
      [7
        [2,red
          [1]
          [5
            [4,red]
            [,no edge, draw=none]
          ]
        ]
        [11,red
          [8]
          [14]
        ]
      ]
    \end{forest}
  \end{figure}
  \noindent
  L'albero ora non ha più l'anomalia.
\end{example}

\item \textbf{Rimozione}: La rimozione di un nodo da un albero RB si fa allo stesso modo
  di un albero binario di ricerca. Bisogna quindi distinguere i 3 casi:
  \begin{enumerate}
    \item \textbf{Il nodo da rimuovere è una foglia}: si può rimuovere direttamente e
      salviamo in un puntatore il nodo sentinella che sostituirà il nodo rimosso
    \item \textbf{Il nodo da rimuovere ha un solo figlio}: si può sostituire il nodo con
      il figlio e si salva in un puntatore il figlio
    \item \textbf{Il nodo da rimuovere ha due figli}: si può sostituire il nodo con il 
      minimo del sottoalbero destro o con il massimo del sottoalbero sinistro
  \end{enumerate}
  L'anomalia sarà un nodo rosso che dovrà essere
  nero per mantenere la proprietà 4, oppure un nodo nero che dovrà valere come due neri
  per mantenere la proprietà 4. Questa è un anomalia locale che si può risolvere 
  propagandola verso la radice dell'albero lavorando su un sottoinsieme di nodi.

  \vspace{1em}
  \noindent
  Visto che ci sono i nodi sentinella i primi due casi sono risolti dallo stesso codice.
  L'algoritmo è il seguente:

  \vspace{1em}
  \begin{figure}[H]
    \centering
    \begin{forest}
      for tree={
        circle,
        draw,
        minimum size=2em,
        inner sep=1pt,
        s sep=1cm,
      }
      [B
        [\( \underset{x}{A} \),double
          % [...,draw=none]
          % [...,draw=none]
        ]
        [\( \underset{w}{D} \) 
          [C]
          [E]
        ]
      ]
    \end{forest}
  \end{figure}
  \noindent
  Consideriamo un nodo \( x \) non radice e \textbf{doppiamente nero} (anomalia), prendiamo il 
  fratello \( w \) di \( x \) e distinguiamo i seguenti casi:
  \begin{itemize}
    \item \textbf{Se \( w \) è rosso}:
      \begin{figure}[H]
        \centering
        \begin{forest}
          for tree={
            circle,
            draw,
            minimum size=2em,
            inner sep=1pt,
            s sep=1cm,
          }
          [B
            [\( \underset{x}{A} \),double
            % [...,draw=none]
            % [...,draw=none]
            ]
            [\( \underset{w}{D} \),red
              [C]
              [E]
            ]
          ]
        \end{forest}
      \end{figure}
      \noindent
      Per forza il padre di \( x \) deve essere nero (per mantenere la proprietà 3), di
      conseguenza lo sono anche i figli di \( w \). Si scambia il colore tra
      il padre di \( x \) e \( w \) e si fa una rotazione a sinistra sul padre di \( x \).
      \begin{figure}[H]
        \centering
        \begin{forest}
          for tree={
            circle,
            draw,
            minimum size=2em,
            inner sep=1pt,
            s sep=1cm,
          }
          [D
            [B, red
              [\( \underset{x}{A} \),double]
              [C]
            ]
            [E]
          ]
        \end{forest}
      \end{figure}
      \noindent
      Questa trasformazione non viola nessuna proprietà dell'RB-albero, tranne per 
      l'anomalia di \( x \).

      Non siamo più nel caso in cui il fratello di \( x \) è rosso.

    \item \textbf{Se \( w \) è nero}:
      \begin{figure}[H]
        \centering
        \begin{forest}
          for tree={
            circle,
            draw,
            minimum size=2em,
            inner sep=1pt,
            s sep=1cm,
          }
          [\( \stackrel{?B}{B} \),green!50!black
            [\( \underset{x}{A} \),double
            % [...,draw=none]
            % [...,draw=none]
            ]
            [\( \underset{w}{D} \)
              [C]
              [E]
            ]
          ]
        \end{forest}
      \end{figure}
      \noindent
      Se \( w \) è nero non si può dire nulla sul colore del padre di \( x \). Anche
      adesso si distinguono due casi:
      \begin{itemize}
        \item \textbf{Se i figli di \( w \) sono neri}: Si toglie 1 nero dal livello di
          \( x \) e \( w \) e si aggiunge 1 nero al padre di \( x \). Si propaga l'anomalia
          verso l'alto:
          \begin{figure}[H]
            \centering
            \begin{forest}
              for tree={
                circle,
                draw,
                minimum size=2em,
                inner sep=1pt,
                s sep=1cm,
              }
              [\( \underset{x}{\stackrel{?B}{B}} \),green!50!black,double
                [A
                % [...,draw=none]
                % [...,draw=none]
                ]
                [D,red
                  [C]
                  [E]
                ]
              ]
            \end{forest}
          \end{figure}
          \noindent
          Se il nuovo \( x \) è rosso l'anomalia è risolta, altrimenti si applica
          la stessa procedura al nuovo \( x \).

          Questa trasformazione non viola nessuna
          proprietà dell'RB-albero; anche se \( x \) non ha un colore
          specifico (e se fosse rosso avrebbe un figlio rosso, di conseguenza violerebbe
          una proprietà dell'RB-albero), l'anomalia si è spostata sul nuovo \( x \),
          di conseguenza se fosse rosso diventerebbe nero (anomalia risolta)
          e se fosse nero diventerebbe doppiamente nero.

        \item \textbf{Se i figli di \( w \) non sono entrambi neri}:
          \begin{itemize}
            \item \textbf{Se il figlio destro di \( w \) è nero}: Visto che entrambi i
              figli di \( w \) non sono neri e il destro è nero, allora il sinistro
              è per forza rosso.
              (le lettere greche sono i sottoalberi delle foglie che non vengono considerati)
            \begin{figure}[H]
              \centering
              \begin{forest}
                for tree={
                  circle,
                  draw,
                  minimum size=2em,
                  inner sep=1pt,
                  s sep=1cm,
                }
                [\( \stackrel{?B}{B} \),green!50!black
                  [\( \underset{x}{A} \),double
                    [\( \alpha \),draw=none]
                    [\( \beta \),draw=none]
                  ]
                  [\( \underset{w}{D} \)
                    [C,red
                      [\( \gamma \),draw=none]
                      [\( \delta \),draw=none]
                    ]
                    [E
                      [\( \varepsilon \),draw=none]
                      [\( \zeta \),draw=none]
                    ]
                  ]
                ]
              \end{forest}
            \end{figure}
            \noindent
            Si scambia il colore tra \( w \) e il figlio sinistro di \( w \) e si fa una
            rotazione a destra su \( w \):
            \begin{figure}[H]
              \centering
              \begin{forest}
                for tree={
                  circle,
                  draw,
                  minimum size=2em,
                  inner sep=1pt,
                  s sep=1cm,
                }
                [\( \stackrel{?B}{B} \),green!50!black
                  [\( \underset{x}{A} \),double
                    [\( \alpha \),draw=none]
                    [\( \beta \),draw=none]
                  ]
                  [C
                    [\( \gamma  \),draw=none]
                    [D,red
                      [\( \delta \),draw=none]
                      [E
                        [\( \varepsilon \),draw=none]
                        [\( \zeta \),draw=none]
                      ]
                    ]
                  ]
                ]
              \end{forest}
            \end{figure}
            \noindent
            Questa trasformazione preserva l'RB-albero. Ora il figlio destro del
            fratello di \( x \) è rosso e si applica il caso successivo.

          \item \textbf{Se il figlio destro di \( w \) è rosso}: Visto che entrambi i
            figli di \( w \) non sono neri e il destro è rosso, allora il sinistro
            potrebbe essere sia rosso che nero.
            \begin{figure}[H]
              \centering
              \begin{forest}
                for tree={
                  circle,
                  draw,
                  minimum size=2em,
                  inner sep=1pt,
                  s sep=1cm,
                }
                [\( \stackrel{?B}{B} \),green!50!black
                  [\( \underset{x}{A} \),double]
                  [\( \underset{w}{D} \)
                  [\( \stackrel{?C}{C} \),green!50!black]
                    [E,red]
                  ]
                ]
              \end{forest}
            \end{figure}
            \noindent
            Si assegna a \( w \) il colore del padre di \( x \), al padre di \( x \) 
            e al figlio destro di \( w \) il colore nero. Infine si fa una rotazione
            a sinistra sul padre di \( x \) e si porta l'anomalia alla radice.
            \begin{figure}[H]
              \centering
              \begin{forest}
                for tree={
                  circle,
                  draw,
                  minimum size=2em,
                  inner sep=1pt,
                  s sep=1cm,
                }
                [\( \stackrel{?B}{D} \),green!50!black
                  [B
                    [A]
                    [\( \stackrel{?C}{C} \),green!50!black]
                  ]
                  [E]
                ]
              \end{forest}
            \end{figure}
            \noindent
            Questa trasformazione preserva l'RB-albero. 

            L'anomalia punta alla radice dell'albero, in questo caso non punta a niente
            perchè non sappiamo se il nodo più in alto è la radice, quindi l'anomalia
            è risolta.
          \end{itemize}
      \end{itemize}
  \end{itemize}
  L'algoritmo è finito quando l'anomalia punta alla radice dell'albero. Lo pseudocodice
  dell'algoritmo è il seguente:
\begin{lstlisting}[language=Scala]
// x: Nodo da cui si propaga l'anomalia
while x != root and color(x) == black
  w <- right(parent(x))
  // Se x e' figlio sinistro del padre
  if x == left(parent(x))
    // Se w e' rosso (caso 1)
    if color(w) == red
      color(w) <- black
      color(parent(x)) <- red
      left_rotate(parent(x))
    else // Se w e' nero
      // Se i figli del w sono neri (caso 2, propaga x in alto)
      if color(left(w)) == black and color(right(w)) == black
        color(w) <- red
        x <- parent(x)
      else
        // Se il figlio destro di w e' nero (caso 3, termina)
        if color(right(w)) == black
          color(left(w)) <- black
          color(w) <- red
          right_rotate(w))

        // Se il figlio destro di w e' rosso (caso 4, termina)
        color(w) <- color(parent(x))
        color(parent(x)) <- black
        color(right(w)) <- black
        left_rotate(parent(x))
        x <- root
  // Se x e' figlio destro del padre
  // Si fanno le stesse operazioni con le direzioni invertite
\end{lstlisting}

\noindent
La complessità di questo algoritmo è \( \Theta(\log n) \) si fa un massimo di 3 rotazioni.

\item \textbf{Selezione}: La selezione di un nodo in un albero RB si fa allo stesso modo
  di un albero binario di ricerca. Si parte dalla radice e si scende fino a trovare il
  nodo desiderato. La complessità di questo algoritmo è \( O(\log n) \). Se riesco
  a trovare un modo per decidere se andare a sinistra o a destra in tempo costante
  allora è sufficiente a far diventare la complessità logaritmica.

  \begin{itemize}
    \item \textbf{Dato l'indice da trovare}:
      \vspace{1em}
      \noindent
      Per sapere la posizione dell'elemento della radice si può salvare la grandezza del
      sottoalbero radicato nel nodo \( x \) e usare la seguente formula:
      \[
        size(left(x)) + 1
      \] 
      Usando questa formula si può decidere se andare a sinistra o a destra in tempo costante
      e lo pseudocodice dell'algoritmo è il seguente:
\begin{lstlisting}[language=Scala]
// x: Nodo da cui si parte la selezione
// i: Indice dell'elemento da selezionare
select(x,i)
  r <- size(left(x)) + 1
  if i == r
    return x
  else if i < r
    return select(left(x),i)
  else
    return select(right(x),i-r)
    \end{lstlisting}
    La complessità di questo algoritmo è \( O(\log n) \) perchè la decisione di andare
    a sinistra o a destra è in tempo costante.

    \vspace{1em}
    \noindent
    Questo crea il problema di dover aggiornare la grandezza del sottoalbero ad ogni
    inserimento e rimozione di un nodo.
    \begin{itemize}
      \item In un inserimento, il valore size del nodo aggiunto vlae 1, mentre il campo size
        di tutti i nodi del percorso seguito per inserire il nodo aumenta di 1.
      \item In un estrazione, il campo size di tutti i nodi del percorso seguito per rimuovere
        il nodo diminuisce di 1.
      \item Nella sistemazione dell'albero, i campi del colore non hanno alcuna influenza
        sul campo size, mentre se si fa una rotazione il valore di size deve cambiare:
\begin{lstlisting}[language=Scala]
size(x) <- size(left(x)) + size(right(x)) + 1
      \end{lstlisting}
      Questo funziona perchè il campo size è la somma dei campi size dei figli più 1,
      cioè viene ricalcolato da capo utilizzando il valore salvato nei figli.

      In ogni caso dopo una rotazione gli unici nodi che cambiano il campo size sono
      i nodi alla radice che si scambiano il valore.
  \end{itemize}
  Abbiamo dimostrato che si può mantenere il campo size mantenendo la complessità
  logaritmica.

  \item \textbf{Dato un nodo da trovare}:
    \vspace{1em}
    \noindent
    Lo pseudocodice dell'algoritmo è il seguente:
\begin{lstlisting}[language=Scala]
// x: Nodo da cui si parte la selezione
// y: Nodo da trovare
rank(x)
  count <- size(left(x)) + 1
  while x != root
    // Se x e' figlio destro del padre
    if x == right(parent(x))
      // Si aggiunge la grandezza del sottoalbero sinistro del padre
      count <- count + size(left(parent(x)) + 1

    x <- parent(x)

  return count
\end{lstlisting}
    La complessità di questo algoritmo è \( O(\log n) \) perchè la decisione di andare
    a sinistra o a destra è in tempo costante.
  \end{itemize}

\end{itemize}

\vspace{1em}
\noindent
Se si volessero inserire tutti gli elementi di un RB-albero in un array ordinato, si
potrebbe usare un algoritmo di visita in-order dell'albero. Lo pseudocodice dell'algoritmo
è il seguente:
\begin{lstlisting}[language=Scala]
// x: Nodo da cui si parte la visita
// A: Array in cui inserire x
// i: Indice dell'array in cui inserire x (passato per riferimento)
in_visit(x,A,i)
  if x != nil
    in_visit(left(x),A,i)
    visit(x,A,i) // Inserisce x nell'array
    in_visit(right(x),A,i)


// x: Nodo da inserire nell'array
// A: Array in cui inserire x
// i: Indice dell'array in cui inserire x (passato per riferimento)
visit(x,A,i)
  A[i] <- value(x)
  i++
\end{lstlisting}
\noindent
Questo algoritmo ha complessità \( \Theta(n) \) perchè visita tutti i nodi dell'albero.
\[
  T(n) = T(n_{\text{left}}) + 1 + T(n_{\text{right}}) = n_{\text{left}} + 1 + n_{\text{right}} = n
\] 

\vspace{1em}
\noindent
Ci sono anche le seguenti alternative:
\begin{lstlisting}[language=Scala]
pre_visit(x,A,i)
  if x != nil
    visit(x,A,i) // Inserisce x nell'array
    pre_visit(left(x),A,i)
    pre_visit(right(x),A,i)

post_visit(x,A,i)
  if x != nil
    post_visit(left(x),A,i)
    post_visit(right(x),A,i)
    visit(x,A,i) // Inserisce x nell'array
\end{lstlisting}

\vspace{2em}
\noindent
Se si volesse creare un RB-albero partendo da un array (non necessariamente ordinato)
si potrebbero fare \( n \) inserimenti in un RB-albero vuoto. La complessità di questo
algoritmo è \( \Theta(n \log n) \).

\vspace{1em}
\noindent
Oppure si potrebbe prima ordinare l'array e poi
usare l'algoritmo di visita in-order per inserire tutti gli elementi in un RB-albero,
per fare ciò però bisogna creare un "impalcatura", cioè un RB-albero con tutti i nodi
nil, e poi inserire i nodi ordinati. Si fanno tutte le foglie rosse e tutti gli altri
nodi neri. In pseudocodice l'algoritmo è il seguente:
\begin{lstlisting}[language=Scala]
// Crea un albero bilanciato con n nodi nil
build_empty_tree(n)
  if n == 0
    return nil
  else
    x <- new_node(nil)
    left(x) <- build_empty_tree(floor((n-1)/2))
    right(x) <- build_empty_tree(ceil((n-1)/2))
    return x
\end{lstlisting}
\noindent
Questo algoritmo non si può fare con complessità minore di \( n \log n \).

\vspace{1em}
\noindent
\textbf{Dimostrazione}:

\noindent
Supponiamo che \texttt{array\_to\_rb} abbia complessità \( f < n \log n \) e consideriamo:
\begin{lstlisting}[language=Scala]
sort(A)
  T <- array_to_rb(A)
  A <- rb_to_array(T)
\end{lstlisting}
\noindent
La complessità di questo algoritmo sarebbe:
\[
f+n < n \log n
\] 
e ciò vorrebbe dire che è stato creato un algoritmo di ordinamento più veloce di \( n \log n \),
cosa impossibile, quindi l'algoritmo \texttt{array\_to\_rb} non può avere complessità
\( f < n \log n \).

\vspace{1em}
\noindent
Se visitiamo un albero con gli algoritmi in-visit e pre-visit ottieniamo una stringa di
nodi, è possibile ricostruire l'albero originale partendo dalla radice?
\begin{itemize}
  \item \textbf{Pre}: 
    $\overbrace{\color{green!50!black}A}^{\text{Radice}}
      \quad \overbrace{\color{red}B \quad D \quad E \quad F}^{\text{Sinistra}
      } \quad \overbrace{\color{blue}C \quad G \quad H \quad I}^{\text{Destra}}$
    \item \textbf{In}: \hspace{0.3cm} 
    $
    \underbrace{\color{red}E\quad D \quad B \quad F}_{\text{Sinistra}}
    \quad \underbrace{\color{green!50!black}A}_{\text{Radice}} \quad
    \underbrace{\color{blue}G \quad C \quad H \quad I}_{\text{Destra}}$
\end{itemize}
Dalle stringhe si possono individuare quali sono i sottoalberi sinistro e destro
e qual'è la radice. L'algoritmo per ricostruire l'albero è il seguente:
\begin{lstlisting}[language=Scala]
// P: Stringa di visita pre-ordine
// I: Stringa di visita in-ordine
// Supponiamo che le stringhe abbiano la stessa lunghezza
str_to_tree(P,I)
  if length(I) == 0
    return nil
  else
    x <- new_node(P[1])
    i <- find(I,P[1])
    left(x) <- str_to_tree(P[2..i],I[1..i-1])
    right(x) <- str_to_tree(P[i+1..length(P)],I[i+1..length(I)])
    return x
\end{lstlisting}

\vspace{1em}
\noindent
L'albero sarà quindi:
\begin{figure}[H]
  \centering
  \begin{forest}
    for tree={
      circle,
      draw,
      minimum size=2em,
      inner sep=1pt,
      s sep=1cm,
    }
    [A,green
      [B,red
        [D,red
          [E,red]
          [,no edge, draw=none]
        ]
        [F,red]
      ]
      [C,blue
      [G,blue]
        [H,blue
          [,no edge, draw=none]
          [I,blue]
        ]
      ]
    ]
  \end{forest}
  \caption{Albero ricostruito}
\end{figure}

\vspace{2em}
\noindent
È possibile implementare un algoritmo che unisce 2 RB-alberi in un unico RB-albero
in tempo \( \log n \)?
\begin{lstlisting}[language=Scala]
A1 <- rb_to_array(T1)
A2 <- rb_to_array(T2)
A <- merge(A1,A2)
T <- array_to_rb(A)
\end{lstlisting}
\noindent
Questo ha complessità \( O(n) \) perchè è l'unione di più algoritmi con complessità
lineare.

Questo problema è simile al concetto del merge sort, cioè prendere 2 metà, risolverle
ed unire il risultato.
\begin{lstlisting}[language=Scala]
array_to_rb(A,p,q)
  if p > q
    return nil
  else
    r <- (p+q)/2
    T1 <- array_to_rb(A,p,r)
    T2 <- array_to_rb(A,r+1,q)
    return union(T1,T2)
\end{lstlisting}
\noindent
Questo algoritmo ha complessità:
\[
  T(n) = 2T(n/2) + C(\texttt{union})
\] 
Supponiamo che \( C(\texttt{union}) = \log n \), quindi \( T(n) = \Theta(n) \), ma
sappiamo che in tempo minore di \( n \log n \) non si può costruire un RB-albero,
di conseguenza non è possibile che la union abbia complessità \( \log n \).

\subsection{Campi aggiuntivi}
Si possono aggiungere dei campi aggiuntivi alle strutture dati per velocizzare
alcuni algoritmi, ma bisogna stare attenti a mantenere la complessità delle operazioni
inalterata.

\vspace{1em}
\noindent
\begin{theorem}
  Sia \( F \) un campo aggiuntivo, se
  \[
  \exists f . \forall x F(x) = f(key(x), F(left(x)), F(right(x)), key(left(x)), key(right(x)))
  \] 
  allora \( F \) è mantenibile in \( \log n \).

  \vspace{1em}
  \noindent
  Cioè se il campo aggiuntivo è calcolabile utilizzando le operazioni definite 
  in \( f \) allora la complessità di \( F \) è mantenibile.
\end{theorem}

\subsection{Albero Binomiale}
Gli alberi binomiali sono definiti ricorsivamente rispetto ad un valore chiamato
\textbf{valore binomiale}. Un albero di dimensione \( 0 \) sarà da un solo nodo:
\begin{figure}[H]
  \centering
  \begin{forest}
    for tree={
      circle,
      draw,
      minimum size=2em,
      inner sep=1pt,
      s sep=1cm,
    }
    [\( B_0 \)]
  \end{forest}
\end{figure}
Per costruire un albero binomiale di dimensione \( i+1 \) si fa diventare il secondo albero
un sottoalbero del primo albero:
\begin{figure}[H]
  \centering
  \begin{forest}
    for tree={
      circle,
      draw,
      minimum size=2em,
      inner sep=1pt,
      s sep=1cm,
    }
    [\( B_i \)
      [\( B_i \)
      ]
    ]
  \end{forest}
\end{figure}

Quindi un albero di dimensione:
\begin{itemize}
  \item \( B_0 \):
    \begin{figure}[H]
      \centering
      \begin{forest}
        for tree={
          circle,
          draw,
          minimum size=2em,
          inner sep=1pt,
          s sep=1cm,
        }
        [\( B_0 \)]
      \end{forest}
    \end{figure}
  \item \( B_1 \):
    \begin{figure}[H]
      \centering
      \begin{forest}
        for tree={
          circle,
          draw,
          minimum size=2em,
          inner sep=1pt,
          s sep=1cm,
        }
        [\( B_1 \)
          [\( B_0 \) ]
        ]
      \end{forest}
    \end{figure}

  \item \( B_2 \):
    \begin{figure}[H]
      \centering
      \begin{forest}
        for tree={
          circle,
          draw,
          minimum size=2em,
          inner sep=1pt,
          s sep=1cm,
        }
        [\( B_1 \)
          [\( B_1 \)
            [\( B_0 \) ]
          ]
          [\( B_0 \) ]
        ]
      \end{forest}
    \end{figure}
    
  \item \( B_3 \):
    \begin{figure}[H]
      \centering
      \begin{forest}
        for tree={
          circle,
          draw,
          minimum size=2em,
          inner sep=1pt,
          s sep=1cm,
        }
        [\( B_1 \)
          [\( B_1 \)
            [\( B_1 \)
              [\( B_0 \) ]
            ]
            [\( B_0 \) ]
          ]
          [\( B_1 \)
            [\( B_0 \) ]
          ]
          [\( B_0 \) ]
        ]
      \end{forest}
    \end{figure}
\end{itemize}

\begin{theorem}
  \begin{itemize}
    \item Il numero di nodi è \( 2^k \)
      \begin{itemize}
        \item Dimostrazione:

          \vspace{1em}
          \noindent
          Faccio vedere che la proprietà vale per 0 (caso base). Il numero totale di
          nodi in un albero binomiale di dimensione 0 è 1, cioè \( 2^0 \).

          Supponiamo per induzione che in \( B_k \) ci siano \( 2^k \) nodi.
          Per dimostrare che in \( B_{k+1} \) ci sono \( 2^{k+1} \) nodi, utilizziamo
          la costruzione dell'albero binomiale \( B_k \) e aggiungiamo un altro albero
          binomiale \( B_k \) come figlio sinistro della radice di \( B_k \). Quindi
          il numero di nodi in \( B_{k+1} \) è \( 2^k + 2^k = 2^{k+1} \).
      \end{itemize}
    \item L'altezza dell'albero è \( k \)
      \begin{itemize}
        \item Dimostrazione:
          
          \vspace{1em}
          \noindent
          L'altezza di un albero binomiale di dimensione 0 è 0. 

          Supponiamo per induzione che la dimensione di un albero binomiale \( B_k \)
          sia \( k \). Per dimostrare che la dimensione di un albero binomiale \( B_{k+1} \)
          sia \( k+1 \), sappiamo che l'altezza di un albero è il cammino più lungo
          radice-foglia, quindi il cammino più lungo dalla radice al figlio sinistro
          è \( k \) più un arco, quindi l'altezza è \( k+1 \).
      \end{itemize}
    \item A profondità \( i \) ci sono \( \binom{k}{i} \) nodi: \( (1+i)^k \)
      \begin{itemize}
        \item Dimostrazione:

          \vspace{1em}
          \noindent
          Se consideriamo il triangolo di targaglia, abbiamo che l'elemento alla riga
          \( k \) e colonna \( i \) è \( \binom{k}{i} \). Quindi il numero di nodi
          alla profondità \( i \) è:
          \[
            \binom{k}{i} + \binom{k}{i-1} = \binom{k+1}{i}
          \] 

      \end{itemize}
    \item I figli della radice da sinistra a destra sono radici di:
      \[
        B_{k-1}, B_{k-2}, \ldots, B_0
      \] 
      \begin{itemize}
        \item Dimostrazione:

          \vspace{1em}
          \noindent
          Un albero di dimensione 0 non ha figli, quindi la radice di \( B_k \) ha
          come figli le radici di \( B_{k-1}, B_{k-2}, \ldots, B_0 \). Per \( k+1 \) 
          basta aggiungere un sottoalbero \( k \) come figlio sinistro della radice
          \begin{figure}[H]
            \centering
            \begin{forest}
              for tree={
                circle,
                draw,
                minimum size=2em,
                inner sep=1pt,
                s sep=1cm,
              }
              [
                [k,red]
                [k-1]
                [k-2]
                [...]
                [0]
              ]
            \end{forest}
          \end{figure}
      \end{itemize}
  \end{itemize}
\end{theorem}


\subsection{Heap Binomiale}
Un heap binomiale è una lista di alberi binomiali, dove:
\begin{itemize}
  \item I contenuti dei nodi sono oggetti su cui è definita una relazione di ordinamento
  \item Per ogni dimensione c'è al più un albero binomiale
  \item I vari nodi soddisfano la proprietà di uno heap, cioè:
\begin{lstlisting}[language=Scala]
key(x) <= keys(children(x))
\end{lstlisting}
\end{itemize}

\begin{figure}[H]
  \centering
  \begin{tikzpicture}
    [
      every node/.style={draw,circle,minimum size=2em,node distance = 0.7cm},
    ]
    \node[draw=none] (n1) {};
    \node[right=of n1] (10) {10};
    \draw[->] (n1) -- (10);

    \node[right=of 10] (1) {1};
    \node[below=of 1] (25) {25};
    \node[left=of 25] (12) {12};
    \node[below=of 12] (15) {15};
    \draw[->] (10) -- (1);
    \draw[->] (1) -- (25);
    \draw[->] (1) -- (12);
    \draw[->] (12) -- (15);

    \node[right=5cm of 1] (6) {6};
    \node[below=of 6] (29) {29};
    \node[left=of 29] (14) {14};
    \node[left=of 14] (8) {8};
    \node[below=of 14] (38) {38};
    \node[left=of 38] (25) {25};
    \node[left=of 25] (11) {11};
    \node[below=of 11] (27) {27};
    \draw[->] (1) -- (6);
    \draw[->] (6) -- (29);
    \draw[->] (6) -- (14);
    \draw[->] (6) -- (8);
    \draw[->] (14) -- (38);
    \draw[->] (8) -- (25);
    \draw[->] (8) -- (11);
    \draw[->] (11) -- (27);
  \end{tikzpicture}
  \caption{Esempio di heap binomiale}
\end{figure}
\noindent
La complessità per cercare il minimo è data dal numero di radici.

In un heap di \( n \) nodi il numero di alberi binomiali è \( \log n \).
\[
  2^k > 2^{\log_2 n} = n
\] 
Di conseguenza la complessità per trovare il minimo è \( O(\log n) \).

\vspace{1em}
\noindent
Per questa struttura non esiste un algoritmo di visita in ordine, perchè se ci fosse
vorrebbe dire che esisterebbe un algoritmo di ordinamento in tempo logaritmico e
sappiamo che non esiste.

\subsubsection{Creazione di un heap binomiale}
Per costruire un heap binomiale di dimensione \( n \) si può trasformare \( n \) in
binario e considerare la notazione polinomiale:
\[
  n = \stackrel{2^5}{1} \stackrel{2^4}{0} \stackrel{2^3}{0} \stackrel{2^2}{1} \stackrel{2^1}{0} \stackrel{2^0}{1}
\] 
Si può esprimere un qualsiasi numero \( n \) come somma di potenze di 2. Se la cifra
è 1 allora si crea un albero binomiale di quella dimensione, altrimenti si passa alla
cifra successiva.

\subsubsection{Unione di due heap binomiali}
Si vogliono unire i seguenti heap binomiali:
\begin{figure}[H]
  \centering
  \begin{tikzpicture}
    [
      every node/.style={draw,circle,minimum size=2em,node distance = 0.7cm},
    ]
    \node[draw=none] (n1) {};
    \node[right=of n1] (5) {5};
    \node[right=of 5] (9) {9};
    \node[below=of 9] (11) {11};
    \node[right=of 9] (12) {12};
    \node[draw=none,below=of 12] (b2) {\( B_2 \) };

    \draw[->] (n1) -- (5);
    \draw[->] (5) -- (9);
    \draw[->] (9) -- (11);
    \draw[->] (9) -- (12);
    \draw[->] (12) -- (b2);
  \end{tikzpicture}
  \begin{tikzpicture}
    [
      every node/.style={draw,circle,minimum size=2em,node distance = 0.7cm},
    ]
    \node[draw=none] (n1) {};
    \node[right=of n1] (7) {7};
    \node[right=of 7] (4) {4};
    \node[below=of 4] (10) {10};
    \node[right=of 4] (27) {27};
    \node[draw=none,below=of 27] (b4) {\( B_4 \) };
    \node[right=of 27] (25) {25};
    \node[draw=none,below=of 25] (b5) {\( B_5 \) };

    \draw[->] (n1) -- (7);
    \draw[->] (7) -- (4);
    \draw[->] (4) -- (10);
    \draw[->] (4) -- (27);
    \draw[->] (27) -- (b4);
    \draw[->] (27) -- (25);
    \draw[->] (25) -- (b5);
  \end{tikzpicture}
\end{figure}
\noindent
Si scrive un unica lista che è il merge delle due liste:

\begin{figure}[H]
  \centering
  \begin{tikzpicture}
    [
      every node/.style={draw,circle,minimum size=2em,node distance = 0.7cm},
    ]
    \node[draw=none] (n1) {};
    \node[right=of n1] (5) {5} node[draw=none,above,red] at (n1) {x};
    \node[right=of 5] (7) {7} node[draw=none,above,red] at (7) {next(x)};
    \node[right=of 7] (9) {9} node[draw=none,above,red] at (9) {sibling(x)};
    \node[right=of 9] (4) {4};
    \node[right=of 4] (12) {12};
    \node[right=of 12] (27) {27};
    \node[right=of 27] (25) {25};
    \node[draw=none,below=of 25] (b5) {\( B_5 \) };
    \node[draw=none,below=of 27] (b4) {\( B_4 \) };
    \node[draw=none,below=of 12] (b2) {\( B_2 \) };
    \node[below=of 4] (10) {10};
    \node[below=of 9] (11) {11};

    \draw[->] (n1) -- (5);
    \draw[->] (5) -- (7);
    \draw[->] (7) -- (9);
    \draw[->] (9) -- (4);
    \draw[->] (4) -- (12);
    \draw[->] (12) -- (27);
    \draw[->] (27) -- (25);
    \draw[->] (25) -- (b5);
    \draw[->] (27) -- (b4);
    \draw[->] (12) -- (b2);
    \draw[->] (4) -- (10);
    \draw[->] (9) -- (11);
  \end{tikzpicture}
\end{figure}
\noindent
Le due liste rappresentate in binario sono:
\[
  L1 = 111 \quad L2 = 110011
\] 
L'unione delle due liste si fa sommando i numeri binari:
La nuova lista diventa:
\begin{enumerate}
  \item Passo 1:
    \begin{figure}[H]
      \centering
      \begin{tikzpicture}
        [
        every node/.style={draw,circle,minimum size=2em,node distance = 0.7cm},
        ]
        \node[draw=none] (n1) {};
        \node[right=of n1] (5) {5};
        \node[below=of 5] (7) {7};
        \node[right=of 5] (9) {9} node[draw=none,above=0.5cm,red] at (9) {x};
        \node[right=of 9] (4) {4} node[draw=none,above,red] at (4) {next(x)};
        \node[right=of 4] (12) {12} node[draw=none,above,red] at (12) {sibling(x)};
        \node[right=of 12] (27) {27};
        \node[right=of 27] (25) {25};
        \node[draw=none,below=of 25] (b5) {\( B_5 \) };
        \node[draw=none,below=of 27] (b4) {\( B_4 \) };
        \node[draw=none,below=of 12] (b2) {\( B_2 \) };
        \node[below=of 4] (10) {10};
        \node[below=of 9] (11) {11};

        \draw[->] (n1) -- (5);
        \draw[->] (5) -- (7);
        \draw[->] (5) -- (9);
        \draw[->] (9) -- (4);
        \draw[->] (4) -- (12);
        \draw[->] (12) -- (27);
        \draw[->] (27) -- (25);
        \draw[->] (25) -- (b5);
        \draw[->] (27) -- (b4);
        \draw[->] (12) -- (b2);
        \draw[->] (4) -- (10);
        \draw[->] (9) -- (11);
      \end{tikzpicture}
    \end{figure}
    \begin{table}[H]
      \centering
      \begin{tabular}{cccccc}
     & & & 1 & 1 & 1 \\
        1 & 1 & 0 & 0 & 1 & 1 \\
        \hline
          &   &   &   &   & 0 \\
      \end{tabular}
    \end{table}

  \item  Passo 2:
    \begin{figure}[H]
      \centering
      \begin{tikzpicture}
        [
        every node/.style={draw,circle,minimum size=2em,node distance = 0.7cm},
        ]
        \node[draw=none] (n1) {};
        \node[right=of n1] (5) {5};
        \node[below=of 5] (7) {7};
        \node[right=2cm of 5] (4) {4} node[draw=none,above=0.5cm,red] at (4) {x};
        \node[below=of 4] (10) {10}; 
        \node[left=of 10] (9) {9};
        \node[below=of 9] (11) {11};
        \node[right=of 4] (12) {12} node[draw=none,above,red] at (12) {next(x)};
        \node[right=of 12] (27) {27} node[draw=none,above,red] at (27) {sibling(x)};
        \node[right=of 27] (25) {25};
        \node[draw=none,below=of 25] (b5) {\( B_5 \) };
        \node[draw=none,below=of 27] (b4) {\( B_4 \) };
        \node[draw=none,below=of 12] (b2) {\( B_2 \) };

        \draw[->] (n1) -- (5);
        \draw[->] (5) -- (7);
        \draw[->] (5) -- (4);
        \draw[->] (4) -- (9);
        \draw[->] (4) -- (12);
        \draw[->] (12) -- (27);
        \draw[->] (27) -- (25);
        \draw[->] (25) -- (b5);
        \draw[->] (27) -- (b4);
        \draw[->] (12) -- (b2);
        \draw[->] (4) -- (10);
        \draw[->] (9) -- (11);
      \end{tikzpicture}
    \end{figure}
    \begin{table}[H]
      \centering
      \begin{tabular}{cccccc}
              & & & 1 & $1^1$ & 1 \\
        1 & 1 & 0 & 0 & 1 & 1 \\
        \hline
          &   &   &   & 1 & 0 \\
      \end{tabular}
    \end{table}

  \item Passo 3:
    \begin{figure}[H]
      \centering
      \begin{tikzpicture}
        [
        every node/.style={draw,circle,minimum size=2em,node distance = 0.7cm},
        ]
        \node[draw=none] (n1) {};
        \node[right=of n1] (5) {5};
        \node[below=of 5] (7) {7};
        \node[right=3.5cm of 5] (4) {4} node[draw=none,above=0.5cm,red] at (4) {x};
        \node[below=of 4] (10) {10};
        \node[left=of 10] (9) {9};
        \node[below=of 9] (11) {11};
        \node[left=of 9] (12) {12};
        \node[right=of 4] (27) {27} node[draw=none,above,red] at (27) {next(x)};
        \node[right=of 27] (25) {25} node[draw=none,above,red] at (25) {sibling(x)};
        \node[draw=none,below=of 25] (b5) {\( B_5 \) };
        \node[draw=none,below=of 27] (b4) {\( B_4 \) };
        \node[draw=none,below=of 12] (b2) {\( B_2 \) };

        \draw[->] (n1) -- (5);
        \draw[->] (5) -- (7);
        \draw[->] (5) -- (4);
        \draw[->] (4) -- (9);
        \draw[->] (4) -- (12);
        \draw[->] (4) -- (27);
        \draw[->] (27) -- (25);
        \draw[->] (25) -- (b5);
        \draw[->] (27) -- (b4);
        \draw[->] (12) -- (b2);
        \draw[->] (4) -- (10);
        \draw[->] (9) -- (11);
      \end{tikzpicture}
    \end{figure}
    \begin{table}[H]
      \centering
      \begin{tabular}{cccccc}
              & & $^1$ & $1^1$ & $1^1$ & 1 \\
        1 & 1 & 0 & 0 & 1 & 1 \\
        \hline
        1 & 1 & 1 & 0 & 1 & 0 \\
      \end{tabular}
    \end{table}
\end{enumerate}
L'unione di due heap binomiali ha complessità \( O(\log n) \).

\subsubsection{Inserimento}
L'inserimento si può vedere come l'unione tra un heap binomiale e un heap binomiale
con un solo nodo.
\begin{table}[H]
  \centering
  \begin{tabular}{cccccc}
    1 & 1 & 1 & 1 & 1 & 1 \\
      &   &   &   &   & 1 \\
    \hline
    1 & 0 & 0 & 0 & 0 & 0 \\
  \end{tabular}
\end{table}
Anche l'inserimento ha complessità \( O(\log n) \).

\subsubsection{Rimozione}
\begin{itemize}
  \item Se si vuole rimuovere il minimo elemento di un heap binomiale bisogna rimuovere
    una radice. Sappiamo che i figli di una radice sono tutti heap binomiali di dimensione
    minore.
    \begin{figure}[H]
      \centering
      \begin{tikzpicture}
        [
        every node/.style={draw,circle,minimum size=2em,node distance = 0.7cm},
        ]
        \node (n1) {};
        \node[right=of n1] (n2) {};
        \node[right=of n2] (k) {};
        \node[below=of k] (bk-1) {\( B_{k-1} \)};
        \node[left=of bk-1] (bk-2) {\( B_{k-2} \)};
        \node[right=of bk-1] (bk) {\( B_0 \)};
        \node[right=of k] (n3) {};
        \node[right=of n3] (n4) {};

        \draw[->] (n1) -- (n2);
        \draw[->] (n2) -- (k);
        \draw[->] (k) -- (bk-1);
        \draw[->] (k) -- (bk-2);
        \draw[->] (k) -- (bk);
\draw[->] (k) -- (n3);
        \draw[->] (n3) -- (n4);
      \end{tikzpicture}
    \end{figure}

    \noindent
    La complessità per rimuovere il minimo elemento è \( O(\log n) \).
\end{itemize}

\vspace{1em}
\noindent
Per rimuovere un nodo arbitrario si usa la \texttt{reduce} diminuendo il valore della chiave
del nodo e facendolo salire fino a che non arriva alla radice eliminando il nodo.

\subsubsection{Riduzione}
La riduzione prende un nodo, lo abbassa (diminuendo il valore della chiave) e gli assegna
la chiave \( a \), mantenendo le proprietà di un heap.

La proprietà dello heap viene mantenuta rispetto ai figli, ma non rispetto al padre. Se
la chiave del padre è maggiore di quella del figlio, si scambiano le chiavi e si fa
salire il nodo fino a che non si rispettano le proprietà dello heap.

\begin{figure}[H]
  \centering
  \begin{tikzpicture}
    [
    every node/.style={draw,circle,minimum size=2em,node distance = 0.7cm},
    ]
    \node (n1) {};
    \node[below=of n1] (n2) {};
    \node[below=of n2] (n3) {};
    \node[below=of n3] (n4) {};
    \node[below=of n4] (x) {a} node[draw=none,left=0.3cm] at (x) {x};

    \draw[->] (n1) -- (n2);
    \draw[->] (n2) -- (n3);
    \draw[->] (n3) -- (n4);
    \draw[->] (n4) -- (x);

    \path[->] (x) edge [bend right=45] (n4);
    \path[->] (n4) edge [bend right=45] (n3);
    \path[->] (n3) edge [bend right=45] (n2);
    \path[->] (n2) edge [bend right=45] (n1);
  \end{tikzpicture}
  \caption{Riduzione della chiave di un nodo}
\end{figure}

\noindent
La complessità della riduzione è \( O(\log n) \).

\subsubsection{Divisione}
Per dividere un heap binomiale mantenendo la differenza di nodi tra le due parti al più
uno bisogna mantenere da parte il nodo di dimensione 0 e dividere il resto. Il nodo
da parte verrà poi riaggiunto ad una delle due lista utilizzando la union.

\subsection{Struttura dati facilmente partizionabile}
Farebbe comodo avere una struttura dati che permetta di partizionare una collezione
di oggetti in complessità costante. Si vogliono implementare i seguenti algoritmi:
\begin{itemize}
  \item \texttt{make\_set(x)} crea un insieme
    di un solo elemento

  \item \texttt{union(x,y)} unisce due insiemi a cui appertengono due oggetti

  \item \texttt{find\_set(x)} restituisce il \textbf{rappresentante} dell'insieme a cui 
    appartiene \( x \).
\end{itemize}
Sappiamo soltanto quello che vogliamo fare, non sappiamo come implementarlo.

\subsubsection{Complessità}
Un modo per rappresentare un insieme è utilizzare una lista concatenata.
\begin{figure}[H]
  \centering
  \begin{tikzpicture}
    [
    every node/.style={draw,minimum size=0.7cm,node distance = 0cm},
    ]
    \node (a) {a};
    \node[above=of a] (at) {};
    \node[below=of a] (ab) {};

    \node[right=1cm of a] (b) {b};
    \node[above=of b] (bt) {};
    \node[below=of b] (bb) {/};
    
    \draw[->] (at.center) -- (a.north);
    \draw[->] (ab.east) -- ++(0.5,0) |- (b.west);
    \draw[->] (bt) -- ++(0,0.8) -| (at);
  \end{tikzpicture}
  \caption{Esempio di una lista concatenata}
\end{figure}
\noindent
Si potrebbe dire che il rappresentante dell'insieme è il primo elemento della lista.
Con queste informazioni possiamo dire che le complessità degli algoritmi saranno:
\begin{itemize}
  \item \texttt{make\_set(x)}: \( O(1) \) perchè basta creare un nodo che punta a se stesso
    perchè è il rappresentante e non ha altri elementi
  \item \texttt{find\_set(x)}: \( O(1) \) perchè basta restituire il valore del puntatore
    che punta al rappresentante
  \item \texttt{union(x,y)}: \( O(n) \) perchè bisogna aggiornare tutti i rappresentanti
    degli insiemi. Se viene aggiunto un puntatore all'elemento in fondo alla lista,
    la complessità diventa \( O(1) \).
\end{itemize}
Se vengono effettuate \( m \) operazioni di cui \( n \) sono \texttt{make\_set} (
cioè il numero di oggetti, di conseguenza il numero di union possibili),
la complessità nel caso pessimo è \( O(m*n) \). Di tutte queste operazioni, le
\texttt{union} saranno al massimo \( n-1 \) perchè non si possono unire più di
\( n \) insiemi. Tutte le rimanenti saranno operazioni di tempo costante, di
conseguenza un altro limite superiore corretto sarebbe \( O(m+n^2) \). Un limite
superiore migliore si può ottenere calcolando la complessità media di tutte le operazioni
e quindi la complessità finale diventa: \( O(\frac{m+n^2}{m}) = O(1 + \frac{n^2}{m}) \).

\vspace{1em}
\noindent
Per capire se questa complessità è accurata bisogna trovare un insieme di operazioni
da fare che portino ad avere quella complessità.

\begin{enumerate}
  \item Si fanno \( n \) \texttt{make\_set} creando \( n \) insiemi
  \item Si uniscono le coppie di insiemi. Per la prima coppia il costo è \( 1 \),
    per la seconda \( 2 \) e così via. Il costo totale è:
    \[
      1 + 2 + 3 + \ldots + n = \frac{n(n+1)}{2} = \Theta(n^2)
    \] 
\end{enumerate}
Per fare meglio di \( \Theta (n ^2) \) bisogna ottimizzare la union, cioè l'aggiornamento
di tutti i puntatori. Si potrebbe unire l'insieme più piccolo a quello più grande,
al posto di unire quello più grande a quello più piccolo e a questo punto bisogna cambiare
soltanto un puntatore, nel caso in cui l'elemento più piccolo abbia un elemento.
Questo metodo è chiamato \textbf{unione per rango}.
Il numero massimo di volte che si può modificare il puntatore al rappresentante dopo
questa tecnica diventa \( \log_2 n \) perchè:
\begin{enumerate}
  \item Se ad un insieme di un elemento viene cambiato il puntatore al rappresentante
    allora sappiamo che l'insieme risultante avrà minimo 2 elementi:
    \[
    \ge 2
    \] 
  \item Se ad un insieme di due elementi viene cambiato il puntatore al rappresentante
    allora sappiamo che l'insieme risultante avrà al minimo 4 elementi (perchè sappiamo
    che bisogna unire l'insieme più piccolo ad un altro insieme, di conseguenza se
    sappiamo che l'insieme di due elementi è il più piccolo, l'altro insieme avrà
    al massimo 2 elementi):
    \[
    \ge 4 = 2^2
    \] 

  \item ...

  \item Se ad un insieme di \( i \) elementi viene cambiato il puntatore al rappresentante
    allora sappiamo che l'insieme risultante avrà al minimo \( 2^i \) elementi:
    \[
    \ge 2^i
    \]
\end{enumerate}
Questa costruzione si chiama \textbf{iterative squaring}.
\noindent
La complessità con l'unione per rango diventa:
\[
\frac{m + n \log n}{m} = 1 + \frac{n \log n}{m} \le  1 + \log n
\] 

\vspace{1em}
\noindent
Per fare meglio si può rappresentare in un modo alternativo gli insiemi, quindi con
degli alberi.
\begin{figure}[H]
  \centering
  \begin{forest}
    for tree={
      circle,
      draw,
      minimum size=2em,
      inner sep=1pt,
      s sep=1cm,
    }
    [a
     [b
       [d]
       [,draw=none,no edge]
     ]
     [c
       [,draw=none,no edge]
       [e]
     ]
    ]
  \end{forest}
  \hspace{1cm}
  \begin{forest}
    for tree={
      circle,
      draw,
      minimum size=2em,
      inner sep=1pt,
      s sep=1cm,
    }
    [f
     [g
      [h]
     ]
    ]
  \end{forest}
  \caption{Esempio di alberi come insiemi}
\end{figure}
\begin{itemize}
  \item \texttt{make\_set(x)}: si crea un albero con un solo nodo
    \[
      O(1)
    \] 

  \item \texttt{find\_set(x)}: si risale l'albero fino alla radice
    \[
      O(n)
    \]

  \item \texttt{union(x,y)}: si fa diventare la radice di un albero il figlio dell'altra
    \[
      O(n)
    \]
\end{itemize}

\vspace{1em}
\noindent
Come prima si può usare l'unione per rango per migliorare la complessità. Ogni volta
che un nodo aumenta di profondità (grazie all'unione ad un altro albero), il numero
di nodi raddoppia. Facendo l'unione per rango sappiamo che la complessità della ricerca
del rappresentante è \( O(\log n) \) e la complessità dell'unione è \( O(\log n) \).

\vspace{1em}
\noindent
Siccome la \texttt{find\_set} è \( O(\log n) \) siamo comunque peggio di prima, quindi
si può far puntare ogni nodo direttamente al rappresentante, ma questo viene fatto
soltanto quando viene richiamata la \texttt{find\_set}, im modo da non farlo più alle
chiamate successive. L'algoritmo è il seguente:
\begin{lstlisting}[language=Scala]
find_set(x):
  if parent(x) == x
    return x
  else
    parent(x) = find_set(parent(x))
    return parent(x)
\end{lstlisting}
oppure più compatto:
\begin{lstlisting}[language=Scala]
find_set(x):
  if parent(x) != x
    x <- find_set(parent(x))
  return parent(x)
\end{lstlisting}
Man mano che vengono eseguite le \texttt{find\_set} la struttura si comprime 
(\textbf{tecnica di compressione dei cammini}):
\begin{figure}[H]
  \centering
  \begin{forest}
    for tree={
      circle,
      draw,
      minimum size=2em,
      inner sep=1pt,
      s sep=1cm,
    }
    [f
      [a]
      [b]
      [c]
      [...]
      [g]
      [h]
    ]
  \end{forest}
\end{figure}
\noindent
Di conseguenza la complessità della \texttt{find\_set} diventa costante.

\vspace{1em}
\noindent
La nuova complessità sapendo di avere \( m \) operazioni di cui \( n \) \texttt{make\_set}
diventa:
\[
O(m \alpha(m,n))
\] 
dove \( \alpha \) è l'inversa della funzione di Ackermann:
\[
  \alpha(m,n) = m,n \left\{ i \ge 1 , \left| A \left( i, \left\lfloor \frac{m}{n} \right\rfloor > \log n \right)  \right.  \right\}
\] 
\[
  A(i,1) = 2^{2^{2^{2^{{\vdots i}^{2}}}}}
\] 
Siccome la funzione di Ackermann cresce molto velocemente, la sua inversa
cresce molto lentamente. Di conseguenza non si riuscirà mai a superare una certa
costante, quindi la complessità diventa costante.

\begin{exercise}
  Si vuole gestire un'agenda che contiene appuntamenti con un orario di inizio e un orario
  di fine, si vuole:
  \begin{itemize}
    \item Inserire un appuntamento (un intervallo di tempo \( \left[ min_t, max_t \right] \))
    \item Rimuovere un appuntamento
    \item Dato un intervallo di tempo trovare tutti gli appuntamenti in conflitto con
      questo intervallo di tempo 
  \end{itemize}
  Questo può essere rappresentato come un RB-albero in cui i nodi rappresentano gli
  appuntamenti.

  \vspace{1em}
  \noindent
  Il tempo di fine è un dato importante per capire se è presente un'intersezione,
  se ordinassimo per il tempo di inizio, ciò che ci dice se c'è stata un intersezione
  con la radice è se il tempo di fine dei nodi a sinistra è maggiore del tempo di 
  inizio della radice.

  \vspace{1em}
  \noindent
  Si può aggiungere un campo aggiuntivo che memorizza il massimo tempo di fine del
  sottoalbero di un nodo
  e questo si può mantenere in tempo logaritmico perchè dipende solo dal nodo stesso
  e dai propri figli

  \vspace{1em}
  \noindent
  L'algoritmo che cerca l'eventuale intersezione è il seguente
\begin{lstlisting}[language=Scala]
// x e' il nodo radice
// I e' l'intervallo di tempo
search(x,I)
  if x == nil && I.intersects(x.interval)
    return x
  if left(x) != nil && left(x).max_end > I.start
    return search(left(x),I)
  else
    return search(right(x),I)
\end{lstlisting}
  Quando sappiamo che non ci sono intervalli che intersecano a sinistra sappiamo
  anche che non ce ne sono a destra perchè se l'intervallo da controllare fosse
  all'interno dell'intervallo alla radice, allora sarebbe stato a sinistra.
  Ciò vuol dire che questo intervallo si trova almeno alla fine dell'intervallo alla
  radice e di conseguenza tutti gli altri intervalli si troveranno a destra della fine
  della radice e quidi non ci sono intersezioni.
\end{exercise}

% \section{Tecniche di programmazione}
% Fin'ora abbiamo utilizzato la tecnica del \textbf{divide et impera}, cioè dividere
% il problema in parti più piccole e risolverle con lo stesso algoritmo per poi unirle
% per ottenere il risultato. Questa non è l'unica tecnica di programmazione esistente,
% ce ne sono molte altre, ad esempio la \textbf{programmazione greedy}, cioè prendere
% le decisioni il prima possibile e sperare che siano le migliori, oppure la
% \textbf{programmazione dinamica}, cioè creare un'infrastruttura prima di poter 
% prendere una decisione.
%
% \subsection{Programmazione dinamica}
% Prendiamo ad esempio il problema della \textbf{moltiplicazione di matrici}.
% Sappiamo che se abbiamo due matrici \( A \) di dimensione \( n \times m \) e \( B \)
% di dimensione \( m \times l \), il prodotto tra le due matrici avrà complessità
% \( \Theta(nml) \).
%
% Se volessimo moltiplicare 3 matrici: \( A_1 \cdot A_2 \cdot A_3 \) di dimensione:
% \[
% \begin{aligned}
%   A_1 & : 10 \times 100\\
%   A_2 & : 100 \times 5\\
%   A_3 & : 5 \times 50
% \end{aligned}
% \] 
% possiamo sfruttare la proprietà associativa:
% \[
%   (A_1 \cdot A_2) \cdot A_3 = A_1 \cdot (A_2 \cdot A_3)
% \] 
% Solo che il numero di operazioni nei due casi è diverso, quindi per rendere minimo
% il numero di operazioni è più conveniente fare: \( (A_1 \cdot A_2) A_3 \) perchè:
% \[
% \begin{aligned}
%   (A_1 \cdot A_2) A_3 &\quad 10 \times 100 \times 5 + 10 \times 5 \times 50 = 5000 + 2500 = 7500\\
%   A_1 (A_2 \cdot A_3) &\quad 100 \times 5 \times 50 + 10 \times 100 \times 50 = 25000 + 50000 = 75000
% \end{aligned}
% \] 
%
% \vspace{1em}
% \noindent
% Supponiamo di avere un insieme di matrici \( A_1, A_2, \ldots, A_n \), vogliamo trovare
% la \textbf{parentesizzazione} ottimale per minimizzare il numero di operazioni.
%
% \vspace{1em}
% \noindent
% Un modo per affrontare il problema è quello di provare tutte le combinazioni
% di parentesi e vedere quale è la migliore.
%
% C'è per forza una moltiplicazione che dovrà essere eseguita per ultima, quindi
% distinguiamo \( k \) come il punto in cui verrà fatta l'ultima moltiplicazione.
% \[
%   (A_1 \cdots A_k) \cdot (A_{k+1} \cdots A_n)
% \] 
% Il numero modi per moltiplicare le matrici è dato dal numero di modi per moltiplicare
% le matrici \( A_1 \dots A_k \) e le matrici \( A_{k+1} \dots A_n \).
% \[
%   C(n) = \sum_{k=1}^{n-1} C(k) + C(n-k) \quad \in \Omega \left( \frac{4^n}{n^{\frac{3}{2}}} \right) 
% \] 
%
% \vspace{1em}
% \noindent
% Supponiamo che qualcuno ci dica il valore ottimo di \( k \), allora è sicuro che
% il modo in cui vengono moltiplicate le matrici \( (A_1 \cdots A_k) \) e 
% \( (A_{k+1} \cdots A_n) \) è ottimale:
% \[
%   \underbrace{\left( A_1 \cdots A_k \right)}_{\text{Ottimo}} \cdot 
%   \underbrace{\left( A_{k+1} \cdots A_n \right)}_{\text{Ottimo}}
% \] 
% Il numero di moltiplicazioni effettuate è dato da:
% \[
%   N\left( A_1 \cdots A_n \right) =
% \] 
% \[
%   \begin{aligned}
%          &= N\left( A_1 \cdots A_k \right) +
%          N\left( A_{k+1} \cdots A_n \right) +
%          \text{Costo ultima moltiplcazione}
%          \\
%          &= N\left( A_1 \cdots A_k \right) +
%          N\left( A_{k+1} \cdots A_n \right) +
%          rows(A_1) \cdot cols(A_k) \cdot cols(A_n)
%   \end{aligned}
% \] 
% Se \( k \) è ottimo, allora \( N\left( A_1 \cdots A_k \right) \) e
% \( N\left( A_{k+1} \cdots A_n \right) \) sono ottimi perchè il risultato
% viene sommato e se non fossero ottimi il risultato finale non sarebbe ottimo.
% Il problema è che non sappiamo quale sia il valore di \( k \) ottimo.
%
% \vspace{1em}
% \noindent
% Un possibile algoritmo per trovare \( k \) è il seguente:
% \begin{lstlisting}[language=Scala]
% // P e' un vettore che descrive le matrici
% // P[0] = rows(A1)
% // P[i] = cols(Ai)
% // Quindi Ai ha dimensione P[i-1] x P[i]
% // i e' l'indice di partenza
% // j e' l'indice di fine
% matrix_chain_order(P,i,j)
%   if i == j
%     return 0
%   else
%     m <- +inf
%     for k <- i to j-1
%       m <- min(m,
%         matrix_chain_order(P,i,k) +
%         matrix_chain_order(P,k+1,j) +
%         P[i-1]*P[k]*P[j])
%     return m
% \end{lstlisting}
% Questo algoritmo ha una complessità esponenziale.
%
% \vspace{1em}
% \noindent
% Cerchiamo di migliorare l'algoritmo, implementando l'\textbf{albero di ricorrenza}, cioè
% un alberi i cui nodi rappresentano le chiamate ricorsive dell'algoritmo, identificate
% dai parametri \( i \) e \( j \).
% \begin{figure}[H]
%   \centering
%   \begin{forest}
%     for tree={
%       inner sep=1pt,
%       s sep=0.5cm,
%     }
%     [{(1,n)}
%       [{(1,1)(2,n)}
%         [{(1,1)(1,1)}]
%         [{(2,2)\color{red}(3,n)}
%           [{(2,2)(2,2)}]
%           [{(3,3)\color{blue}(4,n)}]
%         ]
%       ]
%       [{(1,2)\color{red}(3,n)}
%         [{(1,1)(2,2)}]
%         [{(3,3)\color{blue}(4,n)}]
%       ]
%       [{(1,3)\color{blue}(4,n)}]
%       [{...}]
%       [{(1,n-1)(n,n)}]
%     ]
%   \end{forest}
%   \caption{Albero di ricorrenza}
% \end{figure}
% \noindent
% Notiamo che ci sono molte chiamate ricorsive ripetute, ad esempio \( \color{blue}(4,n) \),
% e questo aumenta la complessità dell'algoritmo inserendo istanze del problema che abbiamo
% già affrontato. Al massimo ci saranno \( n^2 \) istanze di un problema.
%
% Anzichè risolvere il problema ogni volta che lo incontriamo, possiamo risolvere ogni
% singolo problema una sola volta:
% \begin{lstlisting}[language=Scala]
% matrix_chain_order(P)
%   n <- length(p) - 1
%   for i <- 1 to n // n
%     // La moltiplicazione di una sola matrice costa 0 perche' non c'e' nulla da moltiplicare
%     M[i,i] <- 0 // Matrice che salva tutte le soluzioni (i,j)
%   
%   for l <- 2 to n //           n --*
%     for i <- 1 to n - l + 1 // n --*
%       j <- i + l - 1 //            | n^3
%       M[i,j] <- +inf //            |
%       for k <- i to j - 1 //   n --*
%         M[i,j] <- min(M[i,j],
%           M[i,k] + M[k+1,j] + P[i-1]*P[k]*P[j])
%
%   return m[1,n]
% \end{lstlisting}
% Questo ci dice quante moltiplicazioni sono necessarie per moltiplicare tutte le matrici,
% però per sapere la posizione dell'ultima moltiplicazione da fare invece si può
% riscrivere l'algoritmo precedente come:
% \begin{lstlisting}[language=Scala]
% matrix_chain_order(P)
%   ...
%       for k <- i to j - 1
%         m <- M[i,k] + M[k+1,j] + P[i-1]*P[k]*P[j]
%         if m < M[i,j]
%           m[i,j] <- m // Numero moltiplicazioni
%           s[i,j] <- k // Indice dell'ultima moltiplicazione
%   ...
% \end{lstlisting}
% La complessità di questo algoritmo è \( O(n^3) \).
%
% \vspace{1em}
% \noindent
% L'algoritmo precedente scritto in modo iterativo si potrebbe anche scrivere ricorsivamente
% e l'idea è che prima di calcolare il risultato si controlla se è già stato calcolato
% e si utilizza quello, altrimenti si calcola e si salva il risultato.
% \begin{lstlisting}[language=Scala]
% matrix_chain_order(P)
%   for i <- 1 to n
%     for j <- 1 to n
%       M[i,j] <- +inf
%
%   matrix_chain_order_aux(P,1,n)
%
%
% matrix_chain_order(P,i,j)
%   if M[i,j] != +inf
%     return M[i,j]
%   else
%     if i == j
%       M[i,j] <- 0
%     else
%       m <- +inf
%       for k <- i to j-1
%         m <- min(m,
%           matrix_chain_order(P,i,k) +
%           matrix_chain_order(P,k+1,j) +
%           P[i-1]*P[k]*P[j])
%       M[i,j] <- m
%
%     return M[i,j]
% \end{lstlisting}
% Per ogni istanza del problema si calcola una sola volta il risultato, quindi la complessità
% è \( O(n^3) \).
%
% \vspace{1em}
% \noindent
% \begin{definition}
%   La tecnica di memorizzare i risultati già calcolati si chiama \textbf{memoizzazione}.
% \end{definition}

\end{document}
